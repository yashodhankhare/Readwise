---
tags:
  - readwise
---

# Artificial Intelligence

![rw-book-cover](https://images-na.ssl-images-amazon.com/images/I/417MT%2B-a7WL._SL200_.jpg)

## Metadata
- Author: [[Melanie Mitchell]]
- Full Title: Artificial Intelligence
- Category: #books

## Highlights
- Hofstadter’s terror was in response to something entirely different. It was not about AI becoming too smart, too invasive, too malicious, or even too useful. Instead, he was terrified that intelligence, creativity, emotions, and maybe even consciousness itself would be too easy to produce—that what he valued most in humanity would end up being nothing more than a “bag of tricks,” that a superficial set of brute-force algorithms could explain the human spirit. ([Location 181](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=181))
    - Tags: [[pink]] 
- “If such minds of infinite subtlety and complexity and emotional depth could be trivialized by a small chip, it would destroy my sense of what humanity is about.” ([Location 188](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=188))
    - Tags: [[pink]] 
- I could go on and on with dueling quotations. In short, what I found is that the field of AI is in turmoil. Either a huge amount of progress has been made, or almost none at all. Either we are within spitting distance of “true” AI, or it is centuries away. AI will solve all our problems, put us all out of a job, destroy the human race, or cheapen our humanity. It’s either a noble quest or “summoning the demon.” ([Location 222](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=222))
    - Tags: [[pink]] 
- This book is not a general survey or history of artificial intelligence. Rather, it is an in-depth exploration of some of the AI methods that probably affect your life, or will soon, as well as the AI efforts that perhaps go furthest in challenging our sense of human uniqueness. My aim is for you to share in my own exploration and, like me, to come away with a clearer sense of what the field has accomplished and how much further there is to go before our machines can argue for their own humanity. ([Location 239](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=239))
    - Tags: [[pink]] 
- Digital computers are essentially symbol manipulators, pushing around combinations of the symbols 0 and 1. To pioneers of computing like Alan Turing and ([Location 253](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=253))
    - Tags: [[pink]] 
- John von Neumann, there were strong analogies between computers and the human brain, and it seemed obvious to them that human intelligence could be replicated in computer programs. Most people in artificial intelligence trace the field’s official founding to a small workshop in 1956 at Dartmouth College organized by a young mathematician named John McCarthy. In 1955, McCarthy, aged twenty-eight, joined the mathematics faculty at Dartmouth. As an undergraduate, he had learned a bit about both psychology and the nascent field of “automata theory” (later to become computer science) and had become intrigued with the idea of creating a thinking machine. ([Location 254](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=254))
    - Tags: [[pink]] 
- The term artificial intelligence was McCarthy’s invention; he wanted to distinguish this field from a related effort called cybernetics.2 McCarthy later admitted that no one really liked the name—after all, the goal was genuine, not “artificial,” intelligence—but “I had to call it something, so I called it ‘Artificial Intelligence.’”3 The four organizers submitted a proposal to the Rockefeller Foundation asking for funding for the summer workshop. The proposed study was, they wrote, based on “the conjecture that every aspect of learning or any other feature of intelligence can be in principle so precisely described that a machine can be made to simulate it.”4 The proposal listed a set of topics to be discussed—natural-language processing, neural networks, machine learning, abstract concepts and reasoning, creativity—that have continued to define the field to the present day. Even though the most advanced computers in 1956 were about a million times slower than today’s smartphones, McCarthy and colleagues were optimistic that AI was in close reach: “We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.”5 ([Location 264](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=264))
    - Tags: [[pink]] 
- None of these predicted events have yet come to pass. So how far do we remain from the goal of building a “fully intelligent machine”? Would such a machine require us to reverse engineer the human brain in all its complexity, or is there a shortcut, a clever set of yet-unknown algorithms, that can produce what we recognize as full intelligence? What does “full intelligence” even mean? “Define your terms … or we shall never understand one another.”10 This admonition from the eighteenth-century philosopher Voltaire is a challenge for anyone talking about artificial intelligence, because its central notion—intelligence—remains so ill-defined. Marvin Minsky himself coined the phrase “suitcase word”11 for terms like intelligence and its many cousins, such as thinking, cognition, consciousness, and emotion. Each is packed like a suitcase with a jumble of different meanings. Artificial intelligence inherits this packing problem, sporting different meanings in different contexts. ([Location 292](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=292))
    - Tags: [[pink]] 
- As for human intelligence, IQ is measured on a single scale, but we also talk about the different dimensions of intelligence: emotional, verbal, spatial, logical, artistic, social, and so forth. Thus, intelligence can be binary (something is or is not intelligent), on a continuum (one thing is more intelligent than another thing), or multidimensional (someone can have high verbal intelligence but low emotional intelligence). Indeed, the word intelligence is an over-packed suitcase, zipper on the verge of breaking. For better or worse, the field of AI has largely ignored these various distinctions. Instead, it has focused on two efforts: one scientific and one practical. On the scientific side, AI researchers are investigating the mechanisms of “natural” (that is, biological) intelligence by trying to embed it in computers. On the practical side, AI proponents simply want to create computer programs that perform tasks as well as or better than humans, without worrying about whether these programs are actually thinking in the way humans think. When asked if their motivations are practical or scientific, many AI people joke that it depends on where their funding currently comes from. ([Location 303](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=303))
    - Tags: [[pink]] 
- “The lack of a precise, universally accepted definition of AI probably has helped the field to grow, blossom, and advance at an ever-accelerating pace.”13 Furthermore, the committee notes, “Practitioners, researchers, and developers of AI are instead guided by a rough sense of direction and an imperative to ‘get on with it.’” ([Location 315](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=315))
    - Tags: [[pink]] 
- But since the 2010s, one family of AI methods—collectively called deep learning (or deep neural networks)—has risen above the anarchy to become the dominant AI paradigm. In fact, in much of the popular media, the term artificial intelligence itself has come to mean “deep learning.” This is an unfortunate inaccuracy, and I need to clarify the distinction. AI is a field that includes a broad set of approaches, with the goal of creating machines with intelligence. Deep learning is only one such approach. Deep learning is itself one method among many in the field of machine learning, a subfield of AI in which machines “learn” from data or from their own “experiences.” To better understand these various distinctions, it’s important to understand a philosophical split that occurred early in the AI research community: ([Location 328](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=328))
    - Tags: [[pink]] 
- the split between so-called symbolic and subsymbolic AI. ([Location 334](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=334))
    - Tags: [[pink]] 
- First let’s look at symbolic AI. A symbolic AI program’s knowledge consists of words or phrases (the “symbols”), typically understandable to a human, along with rules by which the program can combine and process these symbols in order to perform its assigned task. ([Location 336](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=336))
    - Tags: [[pink]] 
- I’ll give you an example. One early AI program was confidently called the General Problem Solver,15 or GPS for short. (Sorry about the confusing acronym; the General Problem Solver predated the Global Positioning System.) ([Location 338](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=338))
    - Tags: [[pink]] 
- This is part of what the term General refers to in General Problem Solver. To the computer, the “meaning” of the symbols derives from the ways in which they can be combined, related to one another, and operated on. Advocates of the symbolic approach to AI argued that to attain intelligence in computers, it would not be necessary to build programs that mimic the brain. Instead, the argument goes, general intelligence can be captured entirely by the right kind of symbol-processing program. Agreed, the workings of such a program would be vastly more complex than the Missionaries and Cannibals example, but it would still consist of symbols, combinations of symbols, and rules and operations on symbols. Symbolic AI of the kind illustrated by GPS ended up dominating the field for its first three decades, most notably in the form of expert systems, in which human experts devised rules for computer programs to use in tasks such as medical diagnosis and legal decision-making. There are several active branches of AI that still employ symbolic AI; I’ll describe examples of it later, particularly in discussions of AI approaches to reasoning and common sense. ([Location 370](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=370))
    - Tags: [[pink]] 
- Symbolic AI was originally inspired by mathematical logic as well as by the way people described their conscious thought processes. In contrast, subsymbolic approaches to AI took inspiration from neuroscience and sought to capture the sometimes-unconscious thought processes underlying what some have called fast perception, such as recognizing faces or identifying spoken words. Subsymbolic AI programs do not contain the kind of human-understandable language we saw in the Missionaries and Cannibals example above. Instead, a subsymbolic program is essentially a stack of equations—a thicket of often hard-to-interpret operations on numbers. As I’ll explain shortly, such systems are designed to learn from data how to perform a task. ([Location 380](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=380))
    - Tags: [[pink]] 
- A neuron is a cell in the brain that receives electrical or chemical input from other neurons that connect to it. Roughly speaking, a neuron sums up all the inputs it receives from other neurons, and if the total sum reaches a certain threshold level, the neuron fires. Importantly, different connections (synapses) from other neurons to a given neuron have different strengths; in calculating the sum of its inputs, the given neuron gives more weight to inputs from stronger connections than inputs from weaker connections. Neuroscientists believe that adjustments to the strength of connections between neurons is a key part of how learning takes place in the brain. ([Location 391](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=391))
    - Tags: [[pink]] 
- To a computer scientist (or, in Rosenblatt’s case, a psychologist), information processing in neurons can be simulated by a computer program—a perceptron—that has multiple numerical inputs and one output. The analogy between a neuron and a perceptron is illustrated in figure 1. Figure 1A shows a neuron, with its branching dendrites (fibers that carry inputs to the cell), cell body, and axon (that is, output channel) labeled. Figure 1B shows a simple perceptron. Analogous to the neuron, the perceptron adds up its inputs, and if the resulting sum is equal to or greater than the perceptron’s threshold, the perceptron outputs the value 1 (it “fires”); otherwise it outputs the value 0 (it “does not fire”). To simulate the different strengths of connections to a neuron, Rosenblatt proposed that a numerical weight be assigned to each of a perceptron’s inputs; each input is ([Location 398](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=398))
    - Tags: [[pink]] 
- multiplied by its weight before being added to the sum. A perceptron’s threshold is simply a number set by the programmer (or, as we’ll see, learned by the perceptron itself). In short, a perceptron is a simple program that makes a yes-or-no (1 or 0) decision based on whether the sum of its weighted inputs meets a threshold value. You probably make some decisions like this in your life. For example, you might get input from several friends on how much they liked a particular movie, but you trust some of those friends’ taste in movies more than others. If the total amount of “friend enthusiasm”—giving more weight to your more trusted friends—is high enough (that is, greater than some unconscious threshold), you decide to go to the movie. This is how a perceptron would decide about movies, if only it had friends. ([Location 405](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=405))
    - Tags: [[pink]] 
- Inspired by networks of neurons in the brain, Rosenblatt proposed that networks of perceptrons could perform visual tasks such as recognizing faces and objects. To get a flavor of how that might work, let’s explore how a perceptron might be used for a particular visual task: recognizing handwritten digits like those in figure 2. ([Location 413](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=413))
    - Tags: [[pink]] 
- Rosenblatt’s idea was that the perceptron should similarly be trained on examples: it should be rewarded when it fires correctly and punished when it errs. This form of conditioning is now known in AI as supervised learning. ([Location 440](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=440))
    - Tags: [[pink]] 
- Perhaps the most important term in computer science is algorithm, which refers to a “recipe” of steps a computer can take in order to solve a particular problem. Frank Rosenblatt’s primary contribution to AI was his design of a specific algorithm, called the perceptron-learning algorithm, by which a perceptron could be trained from examples to determine the weights and threshold that would produce correct answers. Here’s how it works: Initially, the weights and threshold are set to random values between −1 and 1. In our example, the weight on the first input might be set to 0.2, the weight on the second input set to −0.6, and so on, and the threshold set to 0.7. A computer program called a random-number generator can easily generate these initial values. ([Location 450](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=450))
    - Tags: [[pink]] 
- Just as the psychologist B. F. Skinner found when training pigeons, it’s better to learn gradually over ([Location 467](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=467))
    - Tags: [[pink]] 
- many trials; if the weights and threshold are changed too much on any one trial, then the system might end up learning the wrong thing (such as an overgeneralization that “the bottom and top halves of an 8 are always equal in size”). After many repetitions on each training example, the system eventually (we hope) settles on a set of weights and a threshold that result in correct answers for all the training examples. At that point, we can evaluate the perceptron on the test examples to see how it performs on images it hasn’t been trained on. An 8 detector is useful if you care only about 8s. But what about recognizing other digits? It’s fairly straightforward to extend our perceptron to have ten outputs, one for each digit. Given an example handwritten digit, the output corresponding to that digit should be 1, and all the other outputs should be 0. This extended perceptron can learn all of its weights and thresholds using the perceptron-learning algorithm; the system just needs enough examples. Rosenblatt and others showed that networks of perceptrons could learn to perform relatively simple perceptual tasks; moreover, Rosenblatt proved mathematically that for a certain, albeit very limited, class of tasks, perceptrons with sufficient training could, in principle, learn to perform these tasks without error. What wasn’t ([Location 468](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=468))
    - Tags: [[pink]] 
- clear was how well perceptrons could perform on more general AI tasks. This uncertainty didn’t seem to stop Rosenblatt and his funders at the Office of Naval Research from making ridiculously optimistic predictions about their algorithm. ([Location 478](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=478))
    - Tags: [[pink]] 
- Yes, even at its beginning, AI suffered from a hype problem. I’ll talk more about the unhappy results of such hype shortly. But for now, I want to use perceptrons to highlight a major difference between symbolic and subsymbolic approaches to AI. The fact that a perceptron’s “knowledge” consists of a set of numbers—namely, the weights and threshold it has learned—means that it is hard to uncover the rules the perceptron is using in performing its recognition task. The perceptron’s rules are not symbolic; unlike the ([Location 485](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=485))
    - Tags: [[pink]] 
- General Problem Solver’s symbols, such as LEFT-BANK, #MISSIONARIES, and MOVE, a perceptron’s weights and threshold don’t stand for particular concepts. It’s not easy to translate these numbers into rules that are understandable by humans. The situation gets much worse with modern neural networks that have millions of weights. ([Location 489](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=489))
    - Tags: [[pink]] 
- After the 1956 Dartmouth meeting, the symbolic camp dominated the AI landscape. In the early 1960s, while Rosenblatt was working avidly on the perceptron, the big four “founders” of AI, all strong devotees of the symbolic camp, had created influential—and well-funded—AI laboratories: Marvin Minsky at MIT, John McCarthy at Stanford, and Herbert Simon and Allen Newell at Carnegie Mellon. (Remarkably, these three universities remain to this day among the most prestigious places to study AI.) Minsky, in particular, felt that Rosenblatt’s brain-inspired approach to AI was a dead end, and moreover was stealing away research dollars from more worthy symbolic AI efforts.19 In 1969, Minsky and his MIT colleague Seymour Papert published a book, Perceptrons,20 in which they gave a mathematical proof showing that the types of problems a perceptron could solve perfectly were very limited and that the perceptron-learning algorithm would not do well in scaling up to tasks requiring a large number of weights and thresholds. ([Location 501](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=501))
    - Tags: [[pink]] 
- Minsky and Papert pointed out that if a perceptron is augmented by adding a “layer” of simulated neurons, the types of problems that the device can solve is, in principle, much broader.21 A perceptron with such an added layer is called a multilayer neural network. Such networks form the foundations of much of modern AI; I’ll describe them in detail in the next chapter. ([Location 510](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=510))
    - Tags: [[pink]] 
- The limitations Minsky and Papert proved for simple perceptrons were already known to people working in this area.22 Frank Rosenblatt himself had done extensive work on multilayer perceptrons and recognized the difficulty of training them.23 It wasn’t Minsky and Papert’s mathematics that put the final nail in the perceptron’s coffin; rather, it was their speculation on multilayer neural networks: ([Location 515](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=515))
    - Tags: [[pink]] 
- Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile.24 Ouch. In today’s vernacular that final sentence might be termed “passive-aggressive.” Such negative speculations were at least part of the reason that funding for neural network research dried up in the late 1960s, at the same time that symbolic AI was flush with government dollars. In 1971, at the age of forty-three, Frank Rosenblatt died in a boating accident. Without its most prominent proponent, and without much government funding, research on perceptrons and other subsymbolic AI methods largely halted, except in a few isolated academic groups. ([Location 522](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=522))
    - Tags: [[pink]] 
- The two-part cycle goes like this. Phase 1: New ideas create a lot of optimism in the research community. Results of imminent ([Location 542](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=542))
    - Tags: [[pink]] 
- AI breakthroughs are promised, and often hyped in the news media. Money pours in from government funders and venture capitalists for both academic research and commercial start-ups. Phase 2: The promised breakthroughs don’t occur, or are much less impressive than promised. Government funding and venture capital dry up. Start-up companies fold, and AI research slows. This pattern became familiar to the AI community: “AI spring,” followed by overpromising and media hype, followed by “AI winter.” This has happened, to various degrees, in cycles of five to ten years. When I got out of graduate school in 1990, the field was in one of its winters and had garnered such a bad image that I was even advised to leave the term “artificial intelligence” off my job applications. ([Location 543](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=543))
    - Tags: [[pink]] 
- The attempt to create artificial intelligence has, at the very least, helped elucidate how complex and subtle are our own minds. ([Location 558](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=558))
    - Tags: [[pink]] 
- Spoiler alert: Multilayer neural networks—the extension of perceptrons that was dismissed by Minsky and Papert as likely to be “sterile”—have instead turned out to form the foundation of much of modern artificial intelligence. Because they are the basis of several of the methods I’ll describe in later chapters, I’ll take some time here to describe how these networks work. ([Location 562](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=562))
    - Tags: [[pink]] 
- In contrast, as we’ve seen, subsymbolic systems tend to be hard to interpret, and no one knows how to directly program complex human knowledge or logic into these systems. Subsymbolic systems seem much better suited to perceptual or motor tasks for which humans can’t easily define rules. You can’t easily write down rules for identifying handwritten digits, catching a baseball, or recognizing your mother’s voice; you just seem to do it automatically, without conscious thought. As the philosopher Andy Clark put it, the nature of subsymbolic systems is to be “bad at logic, good at Frisbee.”7 So, why not just use symbolic systems for tasks that require high-level language-like descriptions and logical reasoning, and use subsymbolic systems for the low-level perceptual tasks such as recognizing faces and voices? To some extent, this is what has been done in AI, with very little connection between the two areas. Each of these approaches has had important successes in narrow areas but has serious limitations in achieving the original goals of AI. While there ([Location 659](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=659))
    - Tags: [[pink]] 
- have been some attempts to construct hybrid systems that integrate subsymbolic and symbolic methods, none have yet led to any striking success. ([Location 668](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=668))
    - Tags: [[pink]] 
- This self-taught cat-recognition machine was one of a series of impressive AI feats that have captured the public’s attention over the last decade. Most of these achievements rely on a set of neural network algorithms known as deep learning. ([Location 688](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=688))
    - Tags: [[pink]] 
- Our collective human angst over Deep Blue quickly receded. We accepted that chess could yield to brute-force machinery; playing chess well, we allowed, didn’t require general intelligence after all. This seems to be a common response when computers surpass humans on a particular task; we conclude that the task doesn’t actually require intelligence. As John McCarthy lamented, “As soon as it works, no one calls it AI anymore.”4 ([Location 700](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=700))
    - Tags: [[pink]] 
- Like every AI spring before it, our current one features experts predicting that “general AI”—AI that equals or surpasses humans in most ways—will be here soon. “Human level AI will be passed in the mid-2020s,”6 predicted Shane Legg, cofounder of Google DeepMind, in 2008. In 2015, Facebook’s CEO, Mark Zuckerberg, declared, “One of our goals for the next five to 10 years is to basically get better than human level at all of the primary human senses: vision, hearing, language, general cognition.”7 The AI philosophers Vincent Müller and Nick Bostrom published a 2013 poll of AI researchers in which many assigned a 50 percent chance of human-level AI by the year 2040.8 While much of this optimism is based on the recent successes of deep learning, these programs—like all instances of AI to date—are still examples of what is called “narrow” or “weak” AI. These terms are not as derogatory as they sound; they simply refer to a system that can perform only one narrowly defined task (or a small set of related tasks). AlphaGo is possibly the world’s best Go player, but it can’t do anything else; it can’t even play checkers, tic-tac-toe, or Candy Land. Google Translate can render an English movie review into Chinese, but it can’t tell you if the reviewer liked the movie or not, and it certainly can’t watch and review the movie itself. The terms narrow and weak are used to contrast with strong, human-level, general, or full-blown AI (sometimes called AGI, or artificial general intelligence)—that is, the AI that we see in movies, that can do most everything we humans can do, and possibly much more. General AI might have been the original goal of the field, but achieving it has turned out to be much harder than expected. Over time, efforts in AI have become focused on particular well-defined tasks—speech recognition, chess playing, autonomous driving, and so on. Creating machines that perform such functions is useful and often lucrative, and it could be argued that each of these tasks individually requires “intelligence.” But no AI program has been created yet that could be called intelligent in any general sense. A recent appraisal of the field stated this well: “A pile of narrow intelligences ([Location 723](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=723))
    - Tags: [[pink]] 
- will never add up to a general intelligence. General intelligence isn’t about the number of abilities, but about the integration between those abilities.”9 But wait. Given the rapidly increasing pile of narrow intelligences, how long will it be before someone figures out how to integrate them and produce all of the broad, deep, and subtle features of human intelligence? Do we believe the cognitive scientist Steven Pinker, who thinks all this is business as usual? “Human-level AI is still the standard fifteen to twenty-five years away, just as it always has been, and many of its recently touted advances have shallow roots,” Pinker declared.10 Or should we pay more attention to the AI optimists, who are certain that this time around, this AI spring, things will be different? Not surprisingly, in the AI research community there is considerable controversy over what human-level AI would entail. How can we know if we have succeeded in building such a “thinking machine”? Would such a system be required to have consciousness or self-awareness in the way humans do? Would it need to understand things in the same way a human understands them? Given that we’re talking about a machine here, would we be more correct to say it is “simulating thought,” or could we say it is truly thinking? ([Location 743](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=743))
    - Tags: [[pink]] 
- Hofstadter has noted Kurzweil’s clever use of what Hofstadter calls the “Christopher Columbus ploy,”38 referring to the Ira Gershwin song “They All Laughed,” which includes the line “They all laughed at Christopher ([Location 1004](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1004))
    - Tags: [[pink]] 
- Columbus.” Kurzweil cites numerous quotations from prominent people in history who completely underestimated the progress and impact of technology. Here are a few examples. IBM’s chairman, Thomas J. Watson, in 1943: “I think there is a world market for maybe five computers.” Digital Equipment Corporation’s cofounder Ken Olsen in 1977: “There’s no reason for individuals to have a computer in their home.” Bill Gates in 1981: “640,000 bytes of memory ought to be enough for anybody.”39 Hofstadter, having been stung by his own wrong predictions on computer chess, was hesitant to dismiss Kurzweil’s ideas out of hand, as crazy as they sounded. “Like Deep Blue’s defeat of Kasparov, it certainly gives one pause for thought.”40 ([Location 1007](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1007))
    - Tags: [[pink]] 
- I’d have to (cautiously) agree with this assessment from Kurzweil: “In my view, there is no set of tricks or simpler algorithms (i.e., methods simpler than those underlying human intelligence) that would enable a machine to pass a properly designed Turing Test without actually possessing intelligence at a fully human level.”45 ([Location 1048](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1048))
    - Tags: [[pink]] 
- Is the current AI spring, as many have claimed, the first harbinger of a coming explosion? Or is it simply a waypoint on a slow, incremental growth curve that won’t result in human-level AI for at least another century? Or yet another AI bubble, soon to be followed by another AI winter? To help us get some bearing on these questions, we need to take a careful look at some of the crucial abilities underlying our distinctive human intelligence, such as perception, language, decision-making, commonsense reasoning, and learning. In the next chapters, we’ll see how far AI has come in capturing these abilities, and we’ll assess its prospects, for 2029 and beyond. ([Location 1081](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1081))
    - Tags: [[pink]] 
- Vision—both looking and seeing—turns out to be one of the hardest of all “easy” things. ([Location 1124](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1124))
    - Tags: [[pink]] 
- The ability of machines to recognize objects in images and videos underwent a quantum leap in the 2010s due to advances in the area called deep learning. Deep learning simply refers to methods for training “deep neural networks,” which in turn refers to neural networks with more than one hidden layer. Recall that hidden layers are those layers of a neural network between the input and the output. The depth of a network is its number of hidden layers: a “shallow” network—like the one we saw in chapter 2—has only one hidden layer; a “deep” network has more than one hidden layer. It’s worth emphasizing this definition: the deep in deep learning doesn’t refer to the sophistication of what is learned; it refers only to the depth in layers of the network being trained. ([Location 1142](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1142))
    - Tags: [[pink]] 
- Research on deep neural networks has been going on for several decades. What makes these networks a revolution is their recent phenomenal success in many AI tasks. Interestingly, researchers have found that the most successful deep networks are those whose structure mimics parts of the brain’s visual system. The “traditional” multilayer neural networks I described in chapter 2 were inspired by the brain, but their structure is very un-brain-like. In contrast, the neural networks dominating deep learning are directly modeled after discoveries in neuroscience. ([Location 1149](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1149))
    - Tags: [[pink]] 
- Would you like to experiment with a well-trained ConvNet? Simply take a photo of an object, and upload it to Google’s “search by image” engine.9 Google will run a ConvNet on your image and, based on the resulting confidences (over thousands of possible object categories), will tell you its “best guess” for the image. ([Location 1294](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1294))
    - Tags: [[pink]] 
- Training a ConvNet ([Location 1298](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1298))
    - Tags: [[pink]] 
- Mechanical Turk is the embodiment of Marvin Minsky’s “Easy things are hard” dictum: the human workers are hired to perform the “easy” tasks that are currently too hard for computers. ([Location 1390](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1390))
    - Tags: [[pink]] 
- Some of the photographs contain only one object; others contain many objects, including the “correct” one. Because of this ambiguity, a program gets to guess five categories for each image, and if the correct one is in this list, the program is said to be correct on this image. This is called the “top-5” accuracy metric. ([Location 1412](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1412))
    - Tags: [[pink]] 
- A cardinal rule in machine learning is “Don’t train on the test data.” It seems obvious: If you include test data in any part of training your program, you won’t get a good measure of the program’s generalization abilities. It would be like giving students the questions on the final exam before they take the test. But it turns out that there are subtle ways that this rule can be unintentionally (or intentionally) broken to make your program’s performance look better than it actually is. One such method would be to submit your program’s test-set answers to the test server and, based on the result, tweak your program. Then submit again. Repeat this many times, until you have tweaked it to do better on the test set. This doesn’t require seeing the actual labels in the test set, but it does require getting feedback on accuracy and adjusting your program accordingly. It turns out that if you can do this enough times, it can be very effective in improving your program’s performance on the test set. But because you’re using information from the test set to change your program, you’ve now destroyed the ability to use the test set to see if your program generalizes well. It would be like allowing students to take a final exam many times, each time getting back a single grade, but using that single grade to try to improve their performance the next time around. Then, at the end, the students submit the version of their answers that got them the best score. This is no longer a good measure of how well the students have learned the subject, just a measure of how they adapted their answers to particular test questions. ([Location 1459](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1459))
    - Tags: [[pink]] 
- While this story is merely an interesting footnote to the larger history of deep learning in computer vision, I tell it to illustrate the extent to which the ImageNet competition came to be seen as the key symbol of progress in computer vision, and AI in general. ([Location 1489](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1489))
    - Tags: [[pink]] 
- It turns out that the recent success of deep learning is due less to new breakthroughs in AI than to the availability of huge amounts of data (thank you, internet!) and very fast parallel computer hardware. These factors, along with improvements in training methods, allow hundred-plus-layer networks to be trained on millions of images in just a few days. ([Location 1496](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1496))
    - Tags: [[pink]] 
- Once ImageNet and other large data sets gave ConvNets the vast amount of training examples they needed to work well, companies were suddenly able to apply computer vision in ways never seen before. As Google’s Blaise Agüera y Arcas remarked, “It’s been a sort of gold rush—attacking one problem after another with the same set of techniques.”13 Using ConvNets trained with deep learning, image search engines offered by Google, Microsoft, and others were able to vastly improve their “find similar images” feature. Google offered a photo-storage system that would tag your photos by describing the objects they contained, and Google’s Street View service could recognize and blur out street addresses and license plates in its images. A proliferation of mobile apps enabled smartphones to perform object and face recognition in real time. Facebook labeled your uploaded photos with names of your friends and registered a patent on classifying the emotions behind facial expressions in uploaded photos; Twitter developed a filter that could screen tweets for pornographic images; and several photo- and video-sharing sites started applying tools to detect imagery associated with terrorist groups. ConvNets can be applied to video and used in self-driving cars to track pedestrians, or to read lips and classify body language. ConvNets can even diagnose breast and skin cancer from medical images, determine the stage of diabetic retinopathy, and assist physicians in treatment planning for prostate cancer. These are just a few examples of the many existing (or soon-to-exist) commercial applications powered by ConvNets. In fact, there’s a good chance that any modern computer-vision application you use employs ConvNets. Moreover, there’s an excellent chance it was “pretrained” on images from ImageNet to learn generic visual features before being “fine-tuned” for more specific tasks. Given that the extensive training required by ConvNets is feasible only with specialized computer hardware—typically, powerful graphical processing units (GPUs)—it is not surprising that the stock price of the NVIDIA Corporation, the most prominent maker of GPUs, increased by over 1,000 percent between 2012 and 2017. ([Location 1503](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1503))
    - Tags: [[pink]] 
- Let’s look a bit harder at the specific contention that machines are now “better than humans” at object recognition on ImageNet. This assertion is based on a claim that humans have an error rate of about 5 percent, whereas the error rate of machines is (at the time of this writing) close to 2 percent. Doesn’t this confirm that machines are better than humans at this task? As is often the case for highly publicized claims about AI, the claim comes with a few caveats. Here’s one caveat. When you read about a machine “identifying objects correctly,” you’d think that, say, given an image of a basketball, the machine would output “basketball.” But ([Location 1531](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1531))
    - Tags: [[pink]] 
- of course, on ImageNet, correct identification means only that the correct category is in the machine’s top-five categories. If, given an image of a basketball, the machine outputs “croquet ball,” “bikini,” “warthog,” “basketball,” and “moving van,” in that order, it is considered correct. I don’t know how often this kind of thing happens, but it’s notable that the best top-1 accuracy—the fraction of test images on which the correct category is at the top of the list—was about 82 percent, compared with 98 percent top-5 accuracy, in the 2017 ImageNet competition. No one, as far as I know, has reported a comparison between machines and humans on top-1 accuracy. Here’s another caveat. Consider the claim, “Humans have an error rate of about 5% on ImageNet.” It turns out that saying “humans” is not quite accurate; this result is from an experiment involving a single human, one Andrej Karpathy, who was at the time a graduate student at Stanford, researching deep learning. Karpathy wanted to see if he could train himself to compete against the best ConvNets on ImageNet. Considering that ConvNets train on 1.2 million images and then are run on 150,000 test images, this is a daunting task for a human. Karpathy, who has a popular blog about AI, wrote about his experience: ([Location 1536](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1536))
    - Tags: [[pink]] 
- I ended up training [myself] on 500 images and then switched to [a reduced] test set of 1,500 images. The labeling [that is, Karpathy’s guessing five categories per image] happened at a rate of about 1 per minute, but this decreased over time. I only enjoyed the first ~200, and the rest I only did #forscience.… Some images are easily recognized, while some images (such as those of fine-grained breeds of dogs, birds, or monkeys) can require multiple minutes of concentrated effort. I became very good at identifying breeds of dogs.17 Karpathy found that he was wrong on about 75 of his 1,500 test images, and he went on to analyze the errors he made, finding that they were largely due to images with multiple objects, images with specific breeds of dogs, species of birds or plants, and so on, and object categories that he didn’t realize were included in the target categories. The kinds of errors made by ConvNets are different: while they also get confused by images containing multiple objects, unlike humans they tend to miss objects that… ([Location 1547](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1547))
    - Tags: [[pink]] 
- bested humans on ImageNet needs to be taken with a large… ([Location 1558](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1558))
    - Tags: [[pink]] 
- The caveats I described above aren’t meant to diminish the amazing recent progress in computer vision. There is no question that convolutional neural networks have been stunningly successful in this and other areas, and these successes have not only produced commercial products but also resulted in a real sense of optimism in the AI community. My discussion is meant to illustrate how challenging vision turns out to be and to add some perspective on the progress made so far. Object recognition is not yet close to being “solved” by artificial intelligence. ([Location 1571](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1571))
    - Tags: [[pink]] 
- I have focused on object recognition in this chapter because this has been ([Location 1577](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1577))
    - Tags: [[pink]] 
- the area in which computer vision has recently seen the most progress. However, there’s obviously a lot more to vision than just recognizing objects. If the goal of computer vision is to “get a machine to describe what it sees,” then machines will need to recognize not only objects but also their relationships to one another and how they interact with the world. If the “objects” in question are living beings, the machines will need to know something about their actions, goals, emotions, likely next steps, and all the other aspects that figure into telling the story of a visual scene. Moreover, if we really want the machines to describe what they see, they will need to use language. AI researchers are actively working on getting machines to do these things, but as usual these “easy” things are very hard. As the computer-vision expert Ali Farhadi told The New York Times, “We’re still very, very far from visual intelligence, understanding scenes and actions the way humans do.”18 Why are we still so far from this goal? It seems that visual intelligence isn’t easily separable from the rest of intelligence, especially general knowledge, abstraction, and language—abilities that, interestingly, involve parts of the brain that have many feedback connections to the visual cortex. Additionally, it could be that ([Location 1577](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1577))
    - Tags: [[pink]] 
- the knowledge needed for humanlike visual intelligence—for example, making sense of the “soldier and dog” photo at the beginning of the previous chapter—can’t be learned from millions of pictures downloaded from the web, but has to be experienced in some way in the real world. In the next chapter, I’ll look more closely at machine learning in vision, focusing in particular on the differences between the ways humans and machines learn and trying to tease out just what the machines we have trained have actually learned. ([Location 1588](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1588))
    - Tags: [[pink]] 
- At a recent conference I attended, a leader of Microsoft’s AI product group spoke to the audience about the company’s efforts to hire young deep-learning engineers: “If a kid knows how to train five layers of neural networks, the kid can demand five figures. If the kid knows how to train fifty layers, the kid can demand seven figures.”7 Lucky for this soon-to-be-wealthy kid, the networks can’t yet teach themselves. ([Location 1638](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1638))
    - Tags: [[pink]] 
- Big tech companies offer many services for free on your computer and smartphone: web search, video calling, email, social networking, automated personal assistants—the list goes on. What’s in it for these companies? The answer you might have heard is that their true product is their users (like you and me); their customers are the advertisers who grab our attention and information about us while we use these “free” services. But there’s a second answer: when we use services provided by tech companies such as Google, Amazon, and Facebook, we are directly providing these companies with examples—in the form of our images, videos, text, or speech—that they can utilize to better train their AI programs. And these improved programs attract more users (and thus more data), ([Location 1649](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1649))
    - Tags: [[pink]] 
- helping advertisers to target their ads more effectively. Moreover, the training examples we provide them can be used to train and offer services such as computer vision and natural-language processing to businesses for a fee. ([Location 1655](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1655))
    - Tags: [[pink]] 
- Possible situations a self-driving car might encounter, ranked by likelihood, illustrating the “long tail” of unlikely scenarios ([Location 1688](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1688))
    - Tags: [[pink]] 
- This issue is compounded by the so-called long-tail problem: the vast range of possible unexpected situations an AI system could be faced with. ([Location 1690](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1690))
    - Tags: [[pink]] 
- But once self-driving cars are widespread, while each individual unlikely situation is, by definition, very unlikely, there are so many possible scenarios in the world of driving and so many cars that some self-driving car somewhere is likely to encounter one of them at some point. ([Location 1698](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1698))
    - Tags: [[pink]] 
- The term long tail comes from statistics, in which certain probability distributions are shaped like the one in figure 13: the long list of very unlikely (but possible) situations is called the “tail” of the distribution. (The situations in the tail are sometimes called edge cases.) Most real-world domains for AI exhibit this kind of long-tail phenomenon: events in the real world are usually predictable, but there remains a long tail of low-probability, unexpected occurrences. This is a problem if we rely solely on supervised learning to provide our AI system with its knowledge of the world; the situations in the tail don’t show up in the training data ([Location 1700](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1700))
    - Tags: [[pink]] 
- often enough, if at all, so the system is more likely to make errors when faced with such unexpected cases. ([Location 1705](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1705))
    - Tags: [[pink]] 
- Companies working on autonomous-vehicle technology are acutely aware of the long-tail problem: their teams brainstorm possible long-tail scenarios and actively create extra training examples as well as specially coded strategies for all the unlikely scenarios they can come up with. But of course it is impossible to train or code a system for all the possible situations it might encounter. ([Location 1712](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1712))
    - Tags: [[pink]] 
- A commonly proposed solution is for AI systems to use supervised learning on small amounts of labeled data and learn everything else via unsupervised learning. The term unsupervised learning refers to a broad group of methods for learning categories or actions without labeled data. Examples include methods for clustering examples based on their similarity or learning a new category via analogy to known categories. As I’ll describe in a later chapter, perceiving abstract similarity and analogies is something at which humans excel, but to date there are no very successful AI methods for this kind of unsupervised learning. Yann LeCun himself acknowledges that “unsupervised learning is the dark matter of AI.” In other words, for general AI, almost all learning will have to be unsupervised, but no one has yet ([Location 1717](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1717))
    - Tags: [[pink]] 
- come up with the kinds of algorithms needed to perform successful unsupervised learning. ([Location 1723](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1723))
    - Tags: [[pink]] 
- Humans make mistakes all the time, even (or especially) in driving; any one of us might have hit that public bus, had we been the one veering around sandbags. But humans also have a fundamental competence lacking in all current AI systems: common sense. We have vast background knowledge of the world, both its physical and its social aspects. We have a good sense of how objects—both inanimate and living—are likely to behave, and we use this knowledge extensively in making decisions about how to act in any given situation. We can infer the reason behind salt lines on the road even if we have never driven in snow before. We know how to interact socially with other humans, so we can use eye contact, hand signals, and other body language to deal with broken traffic lights during a power failure. We generally know to yield the road to a large public bus, even if we technically have the right of way. I’ve used driving as an example here, but we humans use common sense—usually subconsciously—in every facet of life. Many people believe that until AI systems have common sense as humans do, we won’t be able to trust them to ([Location 1724](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1724))
    - Tags: [[pink]] 
- be fully autonomous in complex real-world situations. ([Location 1732](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1732))
    - Tags: [[pink]] 
- This is an example of a common phenomenon seen in machine learning. The machine learns what it observes in the data rather than what you (the human) might observe. If there are statistical associations in the training data, even if irrelevant to the task at hand, the machine will happily learn those instead of what you wanted it to learn. If the machine is tested on new data with the same statistical associations, it will appear to have successfully learned to solve the task. However, the machine can fail unexpectedly, as Will’s network did on images of animals without a blurry background. In machine-learning jargon, Will’s network “overfitted” to its specific training set, and thus can’t do a good job of applying what it learned to images that differ from those it was trained on. ([Location 1746](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1746))
    - Tags: [[pink]] 
- Remember back in school when your teacher would write “show your work” in red on your math homework? For me, showing my work was the least fun part of learning math but probably the most important, because showing how I derived my answer demonstrated that I had actually understood what I was doing, had grasped the correct abstractions, and had arrived at the answer for the right reasons. Showing my work also helped my teacher figure out why I made particular errors. More generally, you can often trust that people know what they are doing if they can explain to you how they arrived at an answer or a decision. However, “showing their work” is something that deep neural networks—the bedrock of modern AI systems—cannot easily do. ([Location 1796](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1796))
    - Tags: [[pink]] 
- Even the humans who train deep networks generally cannot look under the hood and provide explanations for the decisions their networks make. MIT’s Technology Review magazine called this impenetrability “the dark secret at the heart of AI.”19 The fear is that if we don’t understand how AI systems work, we can’t really trust them or predict the circumstances under which they will make errors. Humans can’t always explain their thought processes either, and you generally can’t look “under the hood” into other people’s brains (or into their “gut feelings”) to figure out how they came to any particular decision. But humans tend to trust that other humans have correctly mastered basic cognitive tasks such as object recognition and language comprehension. In part, you trust other ([Location 1806](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1806))
    - Tags: [[pink]] 
- people when you believe that their thinking is like your own. You assume, most often, that other humans you encounter have had sufficiently similar life experiences to your own, and thus you assume they are using the same basic background knowledge, beliefs, and values that you do in perceiving, describing, and making decisions about the world. In short, where other people are concerned, you have what psychologists call a theory of mind—a model of the other person’s knowledge and goals in particular situations. None of us have a similar “theory of mind” for AI systems such as deep networks, which makes it harder to trust them. It shouldn’t come as a surprise then that one of the hottest new areas of AI is variously called “explainable AI,” “transparent AI,” or “interpretable machine learning.” These terms refer to research on getting AI systems—particularly deep networks—to explain their decisions in a way that humans can understand. Researchers in this area have come up with clever ways to visualize the features that a given convolutional neural network has learned and, in some cases, to determine which parts of the input are most responsible for the output decision. Explainable AI is a field that is progressing quickly, but a deep-learning system that can successfully ([Location 1813](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1813))
    - Tags: [[pink]] 
- explain itself in human terms is still elusive. ([Location 1823](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1823))
    - Tags: [[pink]] 
- FIGURE 18: Original and “adversarial” examples for AlexNet. The left image in each pair shows the original image, which was correctly classified by AlexNet. The right image in each pair shows the adversarial example derived from this image (small changes have been made to the pixels, but the new image appears to humans to be identical to the original). Each adversarial example was confidently classified by AlexNet as “Ostrich.” ([Location 1843](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1843))
    - Tags: [[pink]] 
- Importantly, Szegedy and his collaborators found that this susceptibility to adversarial examples wasn’t special to AlexNet; they showed that several other convolutional neural networks—with different architectures, hyperparameters, and training sets—had similar vulnerabilities. Calling this an “intriguing property” of neural networks is a little like calling a hole in the hull of a fancy cruise liner a “thought-provoking facet” of the ship. Intriguing, yes, and more investigation is needed, but if the leak is not fixed, this ship is going down. ([Location 1847](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1847))
    - Tags: [[pink]] 
- If deep-learning systems, so successful at computer vision and other tasks, can easily be fooled by manipulations to which humans are not susceptible, how can we say that these networks “learn like humans” or “equal or surpass humans” in their abilities? It’s clear that something very different from human perception is going on here. And if these networks are going to be used for computer vision in the real world, we’d better be darn sure that they are safeguarded from hackers using these kinds of manipulations to fool them. All this has reenergized the small research community focusing on “adversarial learning”—that is, developing strategies that defend against potential (human) adversaries who could attack machine-learning systems. Adversarial-learning researchers often start their work by demonstrating possible ways in which existing systems can be attacked, and some of the recent demonstrations have been stunning. In the domain of computer vision, one group of researchers developed a program that could create eyeglass frames with specific patterns that fool a face-recognition system into confidently misclassifying the wearer as another person (figure 20).24 Another ([Location 1865](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1865))
    - Tags: [[pink]] 
- group developed small, inconspicuous stickers that could be placed on a traffic sign, resulting in a ConvNet-based vision system—similar to those used in self-driving cars—to misclassify the sign (for example, a stop sign is classified as a speed-limit sign).25 Yet another group demonstrated a possible adversarial attack on deep neural networks for medical image analysis: they showed that it is not hard to alter an X-ray or microscopy image in a way that is imperceptible to humans but that causes a network to change its classification from, say, 99 percent confidence that the image shows no cancer to 99 percent confidence that cancer is present.26 This group noted that such attacks could potentially be used by hospital personnel or others to create fraudulent diagnoses in order to charge insurance companies for additional (lucrative) diagnostic tests. ([Location 1875](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1875))
    - Tags: [[pink]] 
- And computer vision isn’t the only domain in which networks can be fooled; researchers have also designed attacks that fool deep neural networks that deal with language, including speech recognition and text analysis. We can expect that as these systems become more widely deployed in the real world, malicious users will discover many other vulnerabilities in these systems. Understanding and defending against such potential attacks are a major area of research right now, but while researchers have found solutions for specific kinds of attacks, there is still no general defense method. ([Location 1889](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1889))
    - Tags: [[pink]] 
- Beyond the immediate issue of how to defend against attacks, the existence of adversarial examples amplifies the question I asked earlier: ([Location 1897](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1897))
    - Tags: [[pink]] 
- What, precisely, are these networks learning? In particular, what are they learning that allows them to be so easily fooled? Or perhaps more important, are we fooling ourselves when we think these networks have actually learned the concepts we are trying to teach them? ([Location 1898](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1898))
    - Tags: [[pink]] 
- I’ve heard some AI researchers argue that humans are also susceptible to our own types of “adversarial examples”: visual illusions. Like AlexNet classifying a school bus as an ostrich, humans are susceptible to perceptual errors (for example, we perceive the upper line in figure 21 to be longer than the lower line, even though both are actually the same length). But the kinds of errors that humans make are quite different from those that convolutional neural networks are susceptible to: our ability to recognize objects in everyday scenes has evolved to be very robust, because our survival depends on it. Unlike today’s ConvNets, ([Location 1911](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1911))
    - Tags: [[pink]] 
- Jeff Clune, an AI researcher at the University of Wyoming, made a very provocative analogy when he noted that there is “a lot of interest in whether Deep Learning is ‘real intelligence’ or a ‘Clever Hans.’”28 Clever Hans was a horse in early twentieth-century Germany who could—his owner claimed—perform arithmetic calculations as well as understand German. The horse responded to questions such as “What is fifteen divided by three?” by tapping his hoof the correct number of times. After Clever Hans became an international celebrity, a careful investigation eventually revealed that the horse did not actually understand the questions or mathematical concepts put to him, but was tapping in response to subtle, unconscious cues given by the questioner. Clever Hans has become a metaphor for any individual (or program!) that gives the appearance of understanding but is actually responding to unintentional cues given by a trainer. Does deep learning exhibit “true understanding,” or is it instead a computational Clever Hans responding to superficial cues in the data? This is currently the subject of heated debates in the AI community, compounded by the fact that AI researchers don’t necessarily agree on the definition of “true understanding.” ([Location 1922](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1922))
    - Tags: [[pink]] 
- When one considers the role of AI in our society, it might be easy to focus on the downsides. However, it’s essential to remember that there are huge benefits that AI systems already bring to society and that they have the potential to be even more beneficial. Current AI technology is central to services you yourself might use all the time, sometimes without even knowing that AI is involved, including speech transcription, GPS navigation and trip planning, email spam filters, language translation, credit-card fraud alerts, book and music recommendations, protection against computer viruses, and optimizing energy usage in buildings. ([Location 1963](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=1963))
    - Tags: [[pink]] 
- This brings us to what you might call the Great AI Trade-Off. Should we embrace the abilities of AI systems, which can improve our lives and even help save lives, and allow these systems to be employed ever more extensively? Or should we be more cautious, given current AI’s unpredictable errors, susceptibility to bias, vulnerability to hacking, and lack of transparency in decision-making? To what extent should humans be required to remain in the loop in different AI applications? What should we require of an AI system in order to trust it enough to let it work autonomously? These questions are still hotly debated, even as AI is increasingly deployed and its promised future applications (for example, self-driving cars) are touted as being just over the horizon. ([Location 2005](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2005))
    - Tags: [[pink]] 
- Machine intelligence presents a knotty array of ethical issues, and discussions related to the ethics of AI and big data have filled several books.4 In order to illustrate the complexity of the issues, I’ll dig deeper into one example that is getting a lot of attention these days: automated face recognition. ([Location 2023](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2023))
    - Tags: [[pink]] 
- Loss of privacy is not the only danger here. An even larger worry is reliability: face-recognition systems can make errors. If your face is matched in error, you might be barred from a store or an airplane flight or wrongly accused of a crime. What’s more, present-day face-recognition systems have been shown to have a significantly higher error rate on people of color than on white people. ([Location 2045](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2045))
    - Tags: [[pink]] 
- When the journalist Amy Sutherland was doing research for a book on exotic animal trainers, she learned that their primary method is preposterously simple: “reward behavior I like and ignore behavior I don’t.” And as she wrote in The New York Times’ Modern Love column, “Eventually it hit me that the same techniques might work on that stubborn but lovable species, the American husband.” Sutherland wrote about how, after years of futile nagging, sarcasm, and resentment, she used this simple method to covertly train her oblivious husband to pick up his socks, find his own car keys, show up to restaurants on time, and shave more regularly.1 ([Location 2182](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2182))
    - Tags: [[pink]] 
- This classic training technique, known in psychology as operant conditioning, has been used for centuries on animals and humans. Operant conditioning inspired an important machine-learning approach called reinforcement learning. Reinforcement learning contrasts with the supervised-learning method I’ve described in previous chapters: in its purest form, reinforcement learning requires no labeled training examples. Instead, an agent—the learning program—performs actions in an environment (usually a computer simulation) and occasionally receives rewards from the environment. These intermittent rewards are the only feedback the agent uses for learning. In the case of Amy Sutherland’s husband, the rewards were her smiles, kisses, and words of praise. While a computer program might not respond to a kiss or an enthusiastic “you’re the greatest,” it can be made to respond to a machine equivalent of such appreciation—such as positive numbers added to its memory. ([Location 2188](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2188))
    - Tags: [[pink]] 
- While reinforcement learning has been part of the AI toolbox for decades, it has long been overshadowed by neural networks and other supervised-learning methods. This changed in 2016 when reinforcement learning played a central role in a stunning and momentous achievement in AI: a program that learned to beat the best humans at the complex game of Go. In order to explain that program, as well as other recent achievements of reinforcement learning, I’ll first take you through a simple example to illustrate how reinforcement learning works. ([Location 2198](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2198))
    - Tags: [[pink]] 
- A crucial notion in reinforcement learning is that of the value of performing a particular action in a given state. The value of action A in state S is a number reflecting the agent’s current prediction of how much reward it will eventually obtain if, when in state S, it performs action A, and then continues performing high-value actions. Let me explain. If your current state is “holding a chocolate in your hand,” an action with high value would be to bring your hand to your mouth. Subsequent actions with high value would be to open your mouth, put the chocolate inside, and chew. Your reward is the delicious sensation of eating the chocolate. Bringing your hand to your mouth doesn’t immediately produce this reward, but this action is on the right path, and if you’ve eaten chocolate before, you can predict the intensity of the upcoming reward. The goal of reinforcement learning is for the agent to learn values that are good predictions of upcoming rewards (assuming that the agent keeps doing the right thing after taking the action in question).3 As we’ll see, the process of learning the values of particular actions in a given state typically takes many steps of trial and error. ([Location 2262](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2262))
    - Tags: [[pink]] 
- Rosie keeps track of the values of actions in a big table in her computer memory. This table, illustrated in ([Location 2274](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2274))
    - Tags: [[pink]] 
- figure 24, lists all the possible states for Rosie (that is, all possible distances she could be from the ball, up to the length of the field), and for each state, her possible actions. Given a state, each action in that state has a numerical value; these values will change—becoming more accurate predictions of upcoming rewards—as Rosie continues to learn. This table of states, actions, and values is called the Q-table. This form of reinforcement learning is sometimes called Q-learning. The letter Q is used because the letter V (for value) was used for something else in the original paper on Q-learning.4 At the beginning of Rosie’s training, I initialize the Q-table by setting all the values to 0—a “blank slate.” When Rosie receives a reward for kicking the ball at the end of episode 1, the value of the action Kick when in state “zero steps away” is updated to 10, the value of the reward. In the future, when Rosie is in the “zero steps away” state, she can look at the Q-table, see that Kick has the highest value—that is, it predicts the highest reward—and decide to choose Kick rather than choosing randomly. That’s all that “learning” means here! ([Location 2274](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2274))
    - Tags: [[pink]] 
- Reinforcement learning—here, the gradual updating of values in the Q-table—continues, episode to episode, until Rosie has finally learned to perform her task from any initial starting point. The Q-learning algorithm is a way to assign values to actions in a given state, including those actions that don’t lead directly to rewards but that set the stage for the relatively rare states in which the agent does receive rewards. ([Location 2308](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2308))
    - Tags: [[pink]] 
- Deciding how much to explore new actions and how much to exploit (that is, stick with) tried-and-true actions is called the exploration versus exploitation balance. Achieving the right balance is a core issue for making reinforcement learning successful. ([Location 2327](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2327))
    - Tags: [[pink]] 
- Setting these issues aside for now, let’s look at two major stumbling blocks that might arise in extrapolating our “training Rosie” example to reinforcement learning in real-world tasks. First, there’s the Q-table. In complex real-world tasks—think, for example, of a robot car learning to drive in a crowded city—it’s impossible to define a small set of “states” that could be listed in a table. A single state for a car at a given time would be something like the entirety of the data from its cameras and other sensors. This means that a self-driving car effectively faces an infinite number of possible states. Learning via a Q-table like the one in the “Rosie” example is out of the question. For this reason, most modern approaches to reinforcement learning use a neural network instead of a Q-table. The neural network’s job is to learn what values should be assigned to actions in a ([Location 2335](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2335))
    - Tags: [[pink]] 
- given state. In particular, the network is given the current state as input, and its outputs are its estimates of the values of all the possible actions the agent can take in that state. The hope is that the network can learn to group related states into general concepts (It’s safe to drive forward or Stop immediately to avoid hitting an obstacle). The second stumbling block is the difficulty, in the real world, of actually carrying out the learning process over many episodes, using a real robot. Even our “Rosie” example isn’t feasible. Imagine yourself initializing a new episode—walking out on the field to set up the robot and the ball—hundreds of times, not to mention waiting around for the robot to perform its hundreds of actions per episode. You just wouldn’t have enough time. Moreover, you might risk the robot damaging itself by choosing the wrong action, such as kicking a concrete wall or stepping forward over a cliff. Just as I did for Rosie, reinforcement-learning practitioners almost always deal with this problem by building simulations of robots and environments and performing all the learning episodes in the simulation rather than in the real world. Sometimes this approach works well. Robots have been trained using simulations to walk, hop, grasp objects, and drive a remote-control ([Location 2341](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2341))
    - Tags: [[pink]] 
- car, among other tasks, and the robots were able, with various levels of success, to transfer the skills learned during simulation to the real world.6 However, the more complex and unpredictable the environment, the less successful are the attempts to transfer what is learned in simulation to the real world. Because of these difficulties, it makes sense that to date the greatest successes of reinforcement learning have been not in robotics but in domains that can be perfectly simulated on a computer. In particular, the best-known reinforcement-learning successes have been in the domain of game playing. Applying reinforcement learning to games is the topic of the next chapter. ([Location 2351](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2351))
    - Tags: [[pink]] 
- The DeepMind group combined reinforcement learning—in particular Q-learning—with deep neural networks to create a system that could learn to play Atari video games. The group called their approach deep Q-learning. To explain how deep Q-learning works, I’ll use Breakout as a running example, but DeepMind used the same method on all the Atari games they tackled. Things will get a bit technical here, so fasten your seat belt (or skip to the next section). ([Location 2397](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2397))
    - Tags: [[pink]] 
- To recap, here’s how deep Q-learning works for the game of Breakout (and all the other Atari games). The system gives its current state as input to the Deep Q-Network. The Deep Q-Network outputs a value for each possible action. The system chooses and performs an action, resulting in a new state. Now the learning step takes place: the system inputs its new state to the network, which outputs a new set of values for each action. The difference between the new set of values and the previous set of values is considered the “error” of the network; this error is used by back-propagation to change the weights of the network. These steps are repeated over many episodes (plays of the game). Just to be clear, everything here—the Deep Q-Network, the virtual “joystick,” and the game itself—is software running in a computer. This is essentially the algorithm developed by DeepMind’s researchers, although they used some tricks to improve it and speed it up.6 At first, before much learning has happened, the network’s outputs are quite random, and the system’s game playing looks quite random as well. But gradually, as the network learns weights that improve its outputs, the system’s playing ability improves, in many cases quite dramatically. ([Location 2439](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2439))
    - Tags: [[pink]] 
- In 1949, the engineer Arthur Samuel joined IBM’s laboratory in Poughkeepsie, New York, and immediately set about programming an early version of IBM’s 701 computer to play checkers. If you yourself have any computer programming experience, you will appreciate the challenge he faced: as noted by one historian, “Samuel was the first person to do any serious programming on the 701 and as such had no system utilities [that is, essentially no operating system!] to call on. In particular he had no assembler and had to write everything using ([Location 2476](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2476))
    - Tags: [[pink]] 
- the op codes and addresses.”8 To translate for my nonprogrammer readers, this is something like building a house using only a handsaw and a hammer. Samuel’s checkers-playing program was among the earliest machine-learning programs; indeed, it was Samuel who coined the term machine learning. ([Location 2480](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2480))
    - Tags: [[pink]] 
- Chess is significantly more complex than checkers. For example, I said above that in checkers there are, on average, six or seven possible moves from any given board position. In contrast, chess has on average thirty-five moves from any given board position. This makes the chess game tree enormously larger than that of checkers. Over the decades, chess-playing programs kept improving, in lockstep with improvements in the speed of computer hardware. In 1997, IBM had its second big game-playing triumph with Deep Blue, a chess-playing program that beat the world champion Garry Kasparov in a widely broadcast multigame match. ([Location 2544](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2544))
    - Tags: [[pink]] 
- Like Samuel’s checkers player before it, Deep Blue’s defeat of Kasparov spurred a significant increase in IBM’s stock price.16 This defeat also generated considerable consternation in the media about the implications for superhuman intelligence as well as doubts about whether humans would still be motivated to play chess. But in the decades since Deep Blue, humanity has adapted. As Claude Shannon wrote presciently in 1950, a machine that can surpass humans at chess “will force us either to admit the possibility of mechanized thinking or to further restrict our concept of thinking.”17 The latter happened. Superhuman chess playing is now seen as something that doesn’t require general intelligence. Deep Blue isn’t intelligent in any sense we mean today. It can’t do anything but play chess, and it doesn’t have any conception of what “playing a game” or “winning” means to humans. (I once heard a speaker say, “Deep Blue may have beat Kasparov, but it didn’t get any joy out of it.”) Moreover, chess has survived—even prospered—as a challenging human activity. Nowadays, computer-chess ([Location 2554](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2554))
    - Tags: [[pink]] 
- chess programs are used by human players as a kind of training aid, in the way a baseball player might practice using a pitching machine. Is this a result of our evolving notion of intelligence, which advances in AI help to clarify? Or is it another example of John McCarthy’s maxim: “As soon as it works, no one calls it AI anymore”?18 ([Location 2564](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2564))
    - Tags: [[pink]] 
- The game of Go has been around for more than two thousand years and is considered among the most difficult of all board games. If you’re not a Go player, don’t worry; none of my discussion here will require any prior knowledge of the game. But it’s useful to know that the game has serious status, especially in East Asia, where it is extremely popular. “Go is a pastime beloved by emperors and generals, intellectuals and child prodigies,” writes the scholar and journalist Alan Levinovitz, who goes on to quote the South Korean Go champion Lee Sedol: “There is chess in the western world, but Go is incomparably more subtle and intellectual.”19 Go is a game that has fairly simple rules but produces what you might call emergent complexity. At each turn, a player places a piece of his or her color (black or white) on a nineteen-by-nineteen-square board, following rules for where pieces may be placed and how to capture one’s opponent’s pieces. Unlike chess, with its hierarchy of pawns, bishops, queens, and so on, pieces in Go (“stones”) are all equal. It’s the configuration of stones on the board that a player must quickly analyze to decide on a move. ([Location 2569](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2569))
    - Tags: [[pink]] 
- The best (human) Go players rely on their pattern-recognition skills and their ineffable intuition. AI researchers haven’t yet figured out how to encode intuition into an evaluation function. This is why, in 1997, the same year that Deep Blue beat Kasparov, the journalist George Johnson wrote in The New York Times, “When or if a computer defeats a human Go champion, it will be a sign that artificial intelligence is truly beginning to become as good as the real thing.”20 This may sound familiar—just like what people used to say about chess! Johnson quoted one Go enthusiast’s prediction: “It may be a hundred years before a computer beats humans at Go—maybe even longer.” A mere twenty years later, AlphaGo, which learned to play Go via deep Q-learning, beat Lee Sedol in a five-game match. ([Location 2587](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2587))
    - Tags: [[pink]] 
- Demis Hassabis noted that “the thing that separates out top Go players [is] their intuition” and that “what we’ve done with AlphaGo is to introduce with neural networks this aspect of intuition, if you want to call it that.”26 ([Location 2626](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2626))
    - Tags: [[pink]] 
- The word intuition has an aura of mystery, but AlphaGo’s intuition (if you want to call it that) arises from its combination of deep Q-learning with a clever method ([Location 2642](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2642))
    - Tags: [[pink]] 
- called “Monte Carlo tree search.” Let’s take a moment to unpack that cumbersome name. First, the “Monte Carlo” part. Monte Carlo is, of course, the most glamorous part of the tiny Principality of Monaco, on the French Riviera, famous for its jet-setter casinos, car racing, and frequent appearance in James Bond movies. But in science and mathematics, “Monte Carlo” refers to a family of computer algorithms, the so-called Monte Carlo method, which was first used during the Manhattan Project to help design the atomic bomb. The name comes from the idea that a degree of randomness—like that of the iconic spinning roulette wheel in the Monte Carlo Casino—can be used by a computer to solve difficult mathematical problems. Monte Carlo tree search is a version of the Monte Carlo method specifically devised for computer game-playing programs. Similar to the way Deep Blue’s evaluation function worked, Monte Carlo tree search is used to assign a score to each possible move from a given board position. However, as I explained above, using extensive look-ahead in the game tree is not feasible for Go, and no one has been able to come up with a good evaluation function for board positions in Go. Monte Carlo tree search works differently. ([Location 2644](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2644))
    - Tags: [[pink]] 
- With its AlphaGo project, DeepMind demonstrated that one of AI’s longtime grand challenges could be conquered by an inventive combination of reinforcement learning, convolutional neural networks, and Monte Carlo tree search (and adding powerful modern computing hardware to the mix). As a result, AlphaGo has attained a well-deserved place in the AI pantheon. But what’s next? Will this potent combination of methods generalize beyond the world of game playing? This is the question I discuss in the next chapter. ([Location 2702](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2702))
    - Tags: [[pink]] 
- Over the past decade, reinforcement learning has transformed from a relatively obscure branch of AI to one of the field’s most exciting (and heavily funded) approaches. The resurgence of reinforcement learning, especially in the public eye, is largely due to the DeepMind projects I described in the previous chapter. DeepMind’s results on Atari games and on Go are indeed remarkable and important, and they deserve their accolades. However, developing superhuman game-playing programs is, for most AI researchers, not an end in and of itself. Let’s step back and ask about the implications of these successes for broader progress in AI. Demis Hassabis has something to say about this: ([Location 2710](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2710))
    - Tags: [[pink]] 
- Games are just our development platform.… It’s the fastest way to develop these AI algorithms and test them, but ultimately we want to use them so they apply to real-world problems and have a huge impact on things like healthcare and science. The whole point is that it’s general AI—it’s learning how to do things [based on] its own experience and its own data.1 Let’s dig into this a bit. How general is this AI, really? How applicable to the real world, beyond games? To what extent are these systems actually learning “on their own”? And what is it, exactly, that they learn? Generality and “Transfer Learning” When I was searching online for articles about AlphaGo, the web offered me this catchy headline: “DeepMind’s AlphaGo Mastered Chess in Its Spare Time.”2 This claim is wrong and misleading, and it’s important to understand why. AlphaGo (in all its versions) can’t play anything but Go. Even the most general version,… ([Location 2716](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2716))
    - Tags: [[pink]] 
- can “transfer” anything it has learned about one game to help it learn a different game. The same is true for the various Atari game-playing programs: each learns its own network weights from scratch. It’s as if you learned to play Pong, but then in order to learn to play Breakout, you’d have to completely forget everything you learned about playing Pong and start from square one. A hopeful phrase in the machine-learning community is “transfer learning,” which refers to the ability of a program to transfer what it has learned about one task to help it perform a different, related task. For humans, transfer learning is automatic. After I learned to play Ping-Pong, I was able to transfer some of those skills to help me in learning tennis and badminton. Knowing how to play checkers helped me in learning how to play chess. When I was a toddler, it took me a while to learn how to twist the doorknob in my room, but once I had mastered that skill, my abilities quickly generalized to most any kind of doorknob. Humans… ([Location 2728](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2728))
    - Tags: [[pink]] 
- In stark contrast with humans, most “learning” in current-day AI is not transferable between related tasks. In this regard, the field is still far from what Hassabis calls “general AI.” While the topic of transfer learning is one of the most active areas of research for… ([Location 2738](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2738))
    - Tags: [[pink]] 
- One additional aspect of DeepMind’s statement needs to be explored: “even in the most challenging of domains.” How can we assess how challenging a domain is for AI? As we’ve seen, many things we humans consider quite ([Location 2759](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2759))
    - Tags: [[pink]] 
- easy (for example, describing the contents of a photo) are extremely challenging for computers. Conversely, many things we humans would find terrifically challenging (for example, correctly multiplying two fifty-digit numbers), computers can do in a split second with a one-line program. ([Location 2760](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2760))
    - Tags: [[pink]] 
- Did DeepMind’s Breakout player actually discover the concept of tunneling? ([Location 2787](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2787))
    - Tags: [[pink]] 
- Gary Marcus reminds us that we need to be careful here: The system has learned no such thing; it doesn’t really understand what a tunnel, or what a wall is; it has just learned specific contingencies for particular scenarios. Transfer tests—in which the deep reinforcement learning system is confronted with scenarios that differ in minor ways from the ones on which the system was trained—show that deep reinforcement learning’s solutions are often extremely superficial.9 ([Location 2787](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2787))
    - Tags: [[pink]] 
- These demonstrations make clear that it is misleading to credit deep reinforcement learning with inducing concepts like wall or paddle; rather, such remarks are what comparative (animal) psychology sometimes call overattributions. It’s not that the Atari system genuinely learned a concept of wall that was robust but rather the system superficially approximated breaking through walls within a narrow set of highly trained circumstances.12 Similarly, while AlphaGo exhibited miraculous “intuition” in playing Go, the system doesn’t have any mechanisms, as far as I can tell, that would allow it to generalize its Go-playing abilities, even to, say, a smaller or differently shaped Go board, without restructuring and retraining its Deep Q-Network. In short, while these deep Q-learning systems have achieved superhuman performance in some narrow domains, and even exhibit what resembles “intuition” in these domains, they are lacking something absolutely fundamental to human intelligence. Whether it is called abstraction, domain generalization, or transfer learning, ([Location 2806](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2806))
    - Tags: [[pink]] 
- imbuing systems with this ability is still one of AI’s most important open problems. There’s another reason to suspect that these systems are not learning humanlike concepts or understanding their domains in the way humans do: like supervised-learning systems, these deep Q-learning systems are vulnerable to adversarial examples of the kind I described in chapter 6. For example, one research group showed that it’s possible to make specific minuscule changes to the pixels in an Atari game-playing program’s input—changes that are imperceptible to humans but that significantly damage the program’s ability to play the game. ([Location 2815](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2815))
    - Tags: [[pink]] 
- Here’s something we must keep in mind when thinking about games like chess and Go and their relation to human intelligence. Consider the reasons many parents encourage their kids to join the school chess club (or in some places the Go club) and would much rather see their kids playing chess (or Go) than sitting at home watching TV or playing video games (sorry, Atari). It’s because… ([Location 2822](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2822))
    - Tags: [[pink]] 
- logically, reason abstractly, and plan strategically. These are all capabilities that will carry over into the rest of one’s life, general abilities that a person will be able to use in all endeavors. But AlphaGo, in spite of the millions of games it has played during its training, has not learned to “think” better about anything except the game of Go. In fact, it has no ability to think about anything, to reason about anything, to make plans about anything, except Go. As far as I know, none of the abilities it has learned are general in any way; none can be transferred to any other task. AlphaGo is the ultimate idiot savant. It’s certainly true that the deep Q-learning method used in AlphaGo can be used to learn other tasks, but the system itself would have to be wholly retrained; it would have to start essentially from scratch in learning a new skill. This brings us back to the “easy things are hard” paradox of AI. AlphaGo was a great achievement for AI; learning largely via self-play, it was able to definitively defeat one of the world’s best human players in a game that is considered a paragon of intellectual prowess. But AlphaGo does not exhibit human-level intelligence as we generally define it, or even arguably any real intelligence. For humans, a crucial part of intelligence is, rather than being able to learn any… ([Location 2825](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2825))
    - Tags: [[pink]] 
- From Games to the Real World Finally, let’s consider Demis Hassabis’s statement that the ultimate goal of these demonstrations on games is to “use them so they apply to real-world problems and have a huge impact on things like healthcare and science.” I think it’s very possible that DeepMind’s work on reinforcement learning may eventually have the kinds of impacts Hassabis is aiming for. But there’s a long way to go from games to the real world. The need for transfer learning is one obstacle. But there are additional reasons that it will be difficult to extend reinforcement learning’s success in games to the real world. Games such as Breakout and Go… ([Location 2839](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2839))
    - Tags: [[pink]] 
- and relatively few possible actions (moves). Moreover, the players have access to “perfect information”: all the components of the game are visible at all times to the players; there are no hidden or uncertain parts of a player’s “state.” The real world doesn’t come so cleanly delineated. Douglas Hofstadter has pointed out that the very notion of a clearly defined “state” isn’t at all realistic. “If you look at situations in the world, they don’t come framed, like a chess game or a Go game.… A situation in the world is something that has no boundaries at all; you don’t know what’s in the situation, what’s out of the situation.”13 As an example, consider using reinforcement learning to train a robot to perform a very useful real-world task: take the dirty dishes stacked in the sink and put them in the dishwasher. (Oh, the harmony such a robot would bring to my family!) How should we define the robot’s “state”? Would it involve everything in its visual field? The contents of the sink? The contents of the dishwasher?… ([Location 2845](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2845))
    - Tags: [[pink]] 
- a coffee mug (which should go on the top rack), or a sponge (which doesn’t go in the dishwasher at all). As we’ve seen, object recognition by computers is as yet far from perfect. In addition, the robot would have to reason about objects that it can’t see—perhaps pots and pans hidden at the bottom of the sink. The robot would also need to learn to pick up different objects and place them (carefully!) in appropriate slots. All this would require learning to choose among a multitude of possible actions involving the robot’s body placement, its grasping “fingers,” its motors controlling the movement of objects from the sink to the correct dishwasher slot, and so on.14 DeepMind’s game-playing agents required millions of iterations of training. If we don’t want millions of broken dishes, we’d have to train our robot in simulation. Games are very fast and accurate to simulate on a computer; there’s no actual moving of pieces or actual balls bouncing off paddles or actual bricks exploding. But simulating a dishwasher-loading robot is not so easy. The more… ([Location 2857](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2857))
    - Tags: [[pink]] 
- as all the other unpredictable aspects of the real world; how do we figure out what needs to be in the simulation and what can safely be ignored? All these issues led Andrej Karpathy, Tesla’s director of AI, to note that, for real-world tasks like this, “basically every single assumption that Go satisfies and that AlphaGo takes advantage of are violated, and any successful approach would look extremely different.”15 No one knows what that successful approach would be. Indeed, the field of deep reinforcement learning is still quite young. The results I described in this chapter can be seen as a proof of principle: the combination of deep networks and Q-learning works surprisingly well in some very interesting, albeit narrow, domains, and while my discussion has highlighted some of the current limitations of the field, many people are working on extending reinforcement learning to apply more generally. DeepMind’s game-playing programs in particular have ignited tremendous new interest and enthusiasm in the field; in fact, deep reinforcement learning was… ([Location 2867](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2867))
    - Tags: [[pink]] 
- Automated speech recognition—the task of transcribing spoken language into text in real time—was deep learning’s first major success in NLP, and I’d venture to say that it is AI’s most significant success to date in any domain. In 2012, at the same time that deep learning was ([Location 2940](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2940))
    - Tags: [[pink]] 
- revolutionizing computer vision, a landmark paper on speech recognition was published by research groups at the University of Toronto, Microsoft, Google, and IBM.2 These groups had been developing deep neural networks for various aspects of speech recognition: recognizing phonemes from acoustic signals, predicting words from combinations of phonemes, predicting phrases from combinations of words, and so on. According to a Google speech-recognition expert, the use of deep networks resulted in the “biggest single improvement in 20 years of speech research.”3 The same year, a new deep-network speech-recognition system was released to customers on Android phones; two years later it was released on Apple’s iPhone, with one Apple engineer commenting, “This was one of those things where the jump [in performance] was so significant that you do the test again to make sure that somebody didn’t drop a decimal place.”4 If you yourself happened to use any kind of speech-recognition technology both before and after 2012, you will have also noticed a very sharp improvement. Speech recognition, which before 2012 ranged from horribly frustrating to moderately useful, suddenly became very nearly perfect in some circumstances. I am now able to dictate all of my texts and emails on my phone’s speech-recognition ([Location 2942](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2942))
    - Tags: [[pink]] 
- recognition app; just a few moments ago, I read the “Restaurant” story to my phone, using my normal speaking speed, and it correctly transcribed every word. What’s stunning to me is that speech-recognition systems are accomplishing all this without any understanding of the meaning of the speech they are transcribing. While the speech-recognition system on my phone can transcribe every word of my “Restaurant” story, I guarantee you that it doesn’t understand a thing about it, or about anything else. Many people in AI, myself included, had previously believed that AI speech recognition would never reach such a high level of performance without actually understanding language. But we’ve been proven wrong. This being said, automated speech recognition is still not at “human level,” contrary to some reports in the media. Background noise can significantly hurt the accuracy of these systems; they’re much less effective inside a moving car than in a quiet room. In addition, these systems are occasionally thrown off by unusual words or… ([Location 2954](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2954))
    - Tags: [[pink]] 
- hat,” but my phone transcribed it as “The bear headed man needed a hat.” It’s not hard to find sentences that will confuse a speech-recognition system. However, for everyday speech in a quiet environment, I’d guess that the accuracy of such systems—measured by correct words—is probably around 90 to 95 percent of humans’ accuracy.5 If you add noise or other complications, the accuracy goes down considerably. There’s a famous rule of thumb in any complex engineering project: the first 90 percent of the project takes 10 percent of the time and the last 10 percent takes 90 percent of the time. I think that some version of this rule applies in many AI domains (hello, self-driving cars!) and will end up being true in speech recognition as well. The last 10 percent includes dealing not only with noise, unfamiliar accents, and unknown words but also with the fact that the ambiguity and context sensitivity of language can impinge on interpreting speech. What’s needed to power through that last stubborn 10 percent? More data? More network layers? Or,… ([Location 2964](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2964))
    - Tags: [[pink]] 
- from sound waves to sentences. Current state-of-the-art speech-recognition systems integrate several different components, including multiple deep neural networks.6 Other NLP tasks, such as language translation or question answering, seem simpler at first glance: the input and output both consist of words. However, deep learning’s data-driven approach hasn’t produced the same kind of progress in these areas as it did in speech recognition… ([Location 2975](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2975))
    - Tags: [[pink]] 
- There is big money in using machines to answer such a question. An AI system that could accurately classify a sentence (or longer passage) as to its sentiment—positive, negative, or some other degree of opinion—would be solid gold to companies that want to analyze customers’ comments about their products, find new potential customers, automate product recommendations (“people who liked X also like Y”), or selectively target their online advertisements. Data on what movies, books, or other merchandise a person likes or dislikes can be surprisingly (and perhaps scarily) useful in predicting that person’s future purchases. What’s more, such information may have predictive power about other aspects of a person’s life, such as likely voting patterns and responsiveness to certain types of news stories or political ads.8 Furthermore, there have been several efforts, with varying success, to apply “sentiment mining” of, say, economics-related tweets on Twitter to predict stock prices and election outcomes. Putting aside the ethics of these applications of sentiment analysis, let’s focus on how AI systems might be able to classify the sentiment of sentences like the ones above. While it’s quite easy for humans to see that these mini-reviews are all negative, getting a program to do this kind of classification in a general way is much harder than it might seem at first glance. Some early NLP systems looked for the presence of individual words or short sequences of words as indications of the sentiment of a text. For example, you might expect words such as dark, weird, heavy, disturbing, horrific, lacking, and missing, or sequences such as wasn’t working, without any, a little too, as indicating negative sentiment in movie reviews. In some cases this works, but in many cases such sequences can be found in positive reviews as well. ([Location 2991](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=2991))
    - Tags: [[pink]] 
- Looking at single words or short sequences in isolation is generally not sufficient to glean the overall sentiment; it’s necessary to capture the semantics of words in the context of the whole sentence. Soon after deep networks started to excel in computer vision and speech recognition, NLP practitioners experimented with applying them to sentiment analysis. As usual, the idea is to train the network on many human-labeled examples of sentences with both positive and negative sentiment and have the network itself learn useful features that allow it to output a classification confidence for “positive” or “negative” on a new sentence. But first, how can we get a neural network to process a sentence? ([Location 3013](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3013))
    - Tags: [[pink]] 
- Given a set of sentences that humans have labeled as “positive” or “negative” in sentiment, the encoder network can be trained from these examples via back-propagation. But there’s one thing I haven’t explained yet. Neural networks require their inputs to be numbers.9 What is the best way to encode the input words as numbers? Answering this question has led to one of the most important advances in natural-language processing in the last decade. ([Location 3057](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3057))
    - Tags: [[pink]] 
- Before explaining possible schemes for encoding words as numbers, I need to define the notion of a neural network’s vocabulary. The vocabulary is the set of all words that the network will be able to accept as inputs. Linguists estimate that on the order of ten thousand to thirty thousand words are needed for a reader to deal with most English texts, depending on how you count; for example, you might group together argue, argues, argued, and arguing as one “word.” The vocabulary can also include common two-word phrases, for example, San Francisco or Golden Gate, by counting them as a single word. As a concrete example, let’s assume that our network will have a twenty-thousand-word vocabulary. The simplest possible scheme for encoding words as numbers is to assign each word in the vocabulary an arbitrary number between 1 and 20,000. Then give the neural network 20,000 inputs, one per word in the vocabulary. At each time step, only one of those inputs—the one corresponding to the actual input word—will be “switched on.” For example, say that the word dark has been given the number 317. Then, if we want to input dark to the network, we set input 317 to have value 1, and all the other 19,999 inputs to have value 0. In the NLP field, this is called a one-hot encoding: at each time step, only one of the inputs—the one corresponding to the word being fed to the network—is “hot” (non-0). The one-hot encoding used to be a standard way to input words to neural networks. But it has a problem: an arbitrary assignment of numbers to words doesn’t capture any relationships among words. Suppose that the network has learned from its training data that the phrase “I hated this movie” has negative sentiment. Now suppose that the network is given the phrase “I abhorred this flick,” but it has not encountered abhorred or flick in its training data. The network wouldn’t have any way to determine that the meanings of the two phrases are the same. Suppose further that the network has learned that the phrase “I laughed out loud” is associated with positive reviews, and then it encounters the novel phrase “I appreciated the humor.” The network wouldn’t be able to recognize the close (though not exactly identical) meanings of these two phrases. The inability to capture semantic relationships among words and phrases is a major reason why neural networks using one-hot encodings often don’t work very well. ([Location 3063](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3063))
    - Tags: [[pink]] 
- The NLP research community has proposed several methods for encoding words in a way that would capture such semantic relationships. All of these methods are based on the same idea, which was expressed beautifully by the linguist John Firth in 1957: “You shall know a word by the company it keeps.”10 That is, the meaning of a word can be defined in terms of other words it tends to occur with, and the words that tend to occur with those words, and so on. Abhorred… ([Location 3083](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3083))
    - Tags: [[pink]] 
- How do we actually obtain all the word vectors corresponding to words in a vocabulary? Is there an algorithm that will properly place all the words in our network’s vocabulary in a semantic space in order to best capture the many dimensions of each word’s meaning? A lot of important work in NLP has gone into solving this exact problem. ([Location 3117](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3117))
    - Tags: [[pink]] 
- Many solutions have been suggested for the problem of placing words in a geometric space, some going back to the 1980s, but today’s most widely adopted method was proposed in 2013 by researchers at Google.13 The researchers called their method “word2vec” (shorthand for “word to vector”). The word2vec method uses a traditional neural network to automatically learn word vectors for all the words in a vocabulary. The Google researchers used part of the company’s vast store of documents to train their network; once training was completed, the Google group saved and published all the resulting word vectors on a web page for anyone to download and use as input to natural-language processing systems.14 The word2vec method embodies the notion of “you shall know a word by the company it keeps.” To create the training data for the word2vec program, the Google group started by taking a massive set of documents from the Google News service. (In modern NLP, nothing beats having “big data” lying around!) The training data for the word2vec program consisted of a collection of pairs of words, where each word in the pair had occurred near the other word in the pair somewhere in the Google News documents. To make the process work better, extremely frequent words such as the, of, and and were discarded. As a concrete example, assume that the words of each pair occur immediately adjacent to each other in a sentence. In this case, the sentence “a man went into a restaurant and ordered a hamburger” first would be transformed into “man went into restaurant ordered hamburger.” This would yield the following pairs: (man, went), (went, into), (into, restaurant), (restaurant, ordered), (ordered, hamburger), plus the reverse of all the pairs—for example, (hamburger, ordered). The idea is to train the word2vec network to predict what words are likely to be paired with a given input word. ([Location 3121](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3121))
    - Tags: [[pink]] 
- This kind of regularity gave people the idea that word2vec could “solve” analogy problems such as “Man is to woman as king is to ______.” Just take the word vector for woman, subtract the word vector for man, and add the result to the word vector for king.18 Then find the word vector in the space that is closest to the result. Yup, it’s queen. In my experimenting with an online word2vec demonstration,19 this method often produces some very good results (“Dinner is to evening as breakfast is to morning”), but just as often it is cryptic (“Thirsty is to drink as tired is to drunk”) or nonsensical (“Fish is to water as bird is to hydrant”). Such properties of learned word vectors are intriguing and show that some relationships are captured. But will these properties make word vectors generally useful in NLP tasks? The answer seems to be a resounding “yes.” Nowadays, virtually all NLP systems use word vectors of one sort or another (word2vec is only one flavor) as the way to input words. ([Location 3189](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3189))
    - Tags: [[pink]] 
- But blameless as word vectors may be, they are a key component in every modern NLP system, ranging from speech recognition to language translation. Biases in word vectors might seep through to produce unexpected, hard-to-predict biases in widely used NLP applications. AI scientists who investigate such biases are just beginning to understand what kinds of subtle effects these biases might have on the outputs of NLP systems, and several groups are working on algorithms for “de-biasing” word vectors.24 De-biasing word vectors is a difficult challenge, but probably not as hard as the alternative: de-biasing language and society. ([Location 3218](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3218))
    - Tags: [[pink]] 
- To what extent should we believe the claims that machines are actually learning “semantic meaning” or that machine translation is swiftly closing in on human levels of accuracy? To answer this, let’s look more closely at the actual results these claims are based on. In particular, let’s look at how these companies measure the quality of a machine or human translation. Measuring the quality of a translation is not at all straightforward; a given text can be translated correctly in any number of ways (and incorrectly in even more ways). Because there’s no single correct answer for translating a given text, it’s hard to design an automatic method for computing the system’s accuracy. ([Location 3330](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3330))
    - Tags: [[pink]] 
- In short, with the introduction of deep learning, machine translation has gotten better. But can we interpret these results to justify a claim that machine translation is now close to “human level”? In my view, this claim is unjustified for several reasons. First, averaging over ratings can be misleading. Imagine a case in which, while most sentence translations are rated “terrific,” there are many that are rated “horrible.” The average would be “pretty good.” However, you’d probably prefer a more reliable translation system that was always “pretty good” and never “horrible.” Additionally, the claims that these translation systems are close to “human level” or at “human parity” are based entirely on evaluating translations of single, isolated sentences rather than longer passages. Sentences in a longer passage can depend on one another in important ways that can be missed if the sentences are translated in isolation. I haven’t seen any formal studies of evaluating machine translation for longer passages, but my general experience is that the translation quality of, say, Google Translate declines significantly when it is given whole paragraphs instead of single sentences. Finally, the sentences in these evaluations are all drawn from news stories and Wikipedia pages, which are typically written with care to avoid ambiguous or idiomatic language; such language can cause serious problems for machine-translation systems. ([Location 3360](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3360))
    - Tags: [[pink]] 
- Reading these translations is something like listening to a familiar piece of music played by a talented but error-prone pianist. The piece is generally recognizable but uncomfortably mangled; the tune goes along beautifully for short bursts but keeps being interrupted by jarring wrong notes. You can see that Google Translate sometimes chooses the wrong meaning of ambiguous words, such as rare and bill (translated into French to mean “infrequent” and “proposed legislation,” respectively); this happens because the program ignores context from previous words or sentences. Idioms such as burned to ([Location 3405](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3405))
    - Tags: [[pink]] 
- a crisp and bent out of shape are translated in strange ways; the program doesn’t seem to have any way of finding a corresponding idiom in the target language or any way to grasp the idiom’s actual meaning. While the skeletal meaning of the story comes through, subtle but important nuances get lost in all the translations, including the man’s anger, expressed in “storming out of the restaurant,” and the waitress’s displeasure, expressed in “muttering under her breath.” Not to mention that correct grammar is occasionally missing in action. I don’t mean to specifically pick on Google Translate; I tried several other online translation services and got similar results. That’s not surprising, because these systems all use virtually the same encoder-decoder architecture. It’s also important to point out that the translations I obtained represent one snapshot in time for these translation systems; they are continually being improved, and some of the specific translation errors seen here may be fixed by the time you are reading this. However, I’m skeptical that machine translation will actually reach the level of human translators—except perhaps in narrow circumstances—for a long time to come. The main obstacle is this: like speech-recognition systems, machine-translation systems perform their task without actually understanding the text they ([Location 3409](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3409))
    - Tags: [[pink]] 
- are processing.21 In translation as well as in speech recognition, the question remains: To what extent is such understanding needed for machines to reach human levels of performance? Douglas Hofstadter argues, “Translation is far more complex than mere dictionary look-up and word rearranging.… ([Location 3420](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3420))
    - Tags: [[pink]] 
- Hofstadter’s words were echoed in a recent article by the AI researchers Ernest Davis and Gary Marcus: “Machine translation … often involves problems of ambiguity that can only be resolved by achieving an actual understanding of the text—and bringing real-world knowledge to bear.”23 Could an encoder-decoder network attain the necessary mental models and real-world knowledge simply by exposure to a larger training set and more network layers, or is something fundamentally different needed? This is still an open question and is the subject of intense debate in the AI community. For now, I’ll simply say that while neural machine translation can be impressively effective and useful in many applications, the translations, without post-editing by knowledgeable humans, are still fundamentally unreliable. If you use machine translation—and I do so myself—you should take the… ([Location 3426](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3426))
    - Tags: [[pink]] 
- Here’s a crazy idea: in addition to translating between languages, could something like an encoder-decoder pair of neural networks be trained to translate from images to language? The idea would be to use one network to encode an image and another network to “translate” that image into a sentence describing the content of the image. After all, isn’t creating an image caption just another kind of “translation”—this time between the “language” of an image and the language of a caption? It turns out this… ([Location 3437](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3437))
    - Tags: [[pink]] 
- papers on this topic at the same computer-vision conference.24 Here I’ll describe the system developed by the Google group, called Show and Tell,… ([Location 3442](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3442))
    - Tags: [[pink]] 
- That is, it misses the meaning of the photo. ([Location 3492](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3492))
    - Tags: [[pink]] 
- I’m certain that these systems will improve as researchers apply more data and new algorithms. However, I believe that the fundamental lack of understanding in caption-generating networks inevitably means that, as in language translation, these systems will remain untrustworthy. They will work very well in some cases but fail spectacularly in others. Moreover, even when they are mostly correct, they will often fail to capture the gist of an image depicting a situation rich with meaning. ([Location 3493](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3493))
    - Tags: [[pink]] 
- The former Google executive Tamar Yehoshua frankly acknowledged the Star Trek computer’s influence on designing the company’s search engine of the future: “Our vision is the Star Trek computer. You can talk to it—it understands you, and it can have a conversation with you.”2 Star Trek’s fictional technology was likewise a central inspiration for IBM’s Watson question-answering system, according to the Watson project leader, David Ferrucci: “The computer on ‘Star Trek’ is a question-answering machine. It understands what you are asking and provides just the right chunk of response ([Location 3522](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3522))
    - Tags: [[pink]] 
- that you needed.”3 The same story holds for Amazon’s Alexa home assistant, according to the Amazon executive David Limp: “The bright light, the shining light that’s still many years away, many decades away, is to recreate the Star Trek computer.”4 Star Trek might have instilled in many of us the dream of being able to ask a computer just about anything and having it respond accurately, concisely, and usefully. But anyone who has used one of today’s AI-powered virtual assistants—Siri, Alexa, Cortana, Google Now, among others—knows that this dream has not yet arrived. We can question these machines by voice—they’re usually good at transcribing speech—and they can answer us with their smooth, only slightly robotic voices. They can sometimes figure out what kind of information we’re looking for and point us to a relevant web page. However, these systems don’t comprehend the meaning of what we ask them. Alexa, say, can read to me the details of the Olympic sprinter Usain Bolt’s entire biography, describe how many gold medals he won, and relate the speed at which he ran the hundred meters in the Beijing Olympics. But remember, easy things are hard. If you ask Alexa, “Does Usain Bolt know how to run?” or “Can Usain Bolt run fast?” in both cases it will respond with the canned phrases “Sorry, I don’t know ([Location 3527](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3527))
    - Tags: [[pink]] 
- that one” or “Hmmm, I’m not sure.” After all, it’s not designed to know what “running” or “fast” actually mean. While computers can accurately transcribe our requests, the “final frontier,” if you will, is to get them to understand the meaning of our questions. ([Location 3539](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3539))
    - Tags: [[pink]] 
- In 2016, Stanford University’s natural-language research group proposed such a test, one that quickly became the de facto measure of “reading comprehension” for machines. The Stanford Question Answering Dataset, or SQuAD, as it is commonly known, consists of paragraphs selected from Wikipedia articles, each of which is accompanied by a question. The more than hundred thousand questions were created by Amazon Mechanical Turk workers.14 ([Location 3686](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3686))
    - Tags: [[pink]] 
- No reading between the lines or actual reasoning is necessary. Rather than reading comprehension, this task might be more accurately called answer extraction. Answer extraction is a useful skill for machines; indeed, answer extraction is precisely what Alexa, Siri, and other digital assistants need to do: turn your question into a search engine query, and then extract the answer from the results. ([Location 3699](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3699))
    - Tags: [[pink]] 
- Some popular media outlets were admirably restrained in describing the SQuAD results. The Washington Post, for example, gave this careful assessment: “AI experts say the test is far too limited to compare with real reading. The answers aren’t generated from understanding the text, but from the system finding patterns and matching terms in the same short passage. The test was done only on cleanly formatted Wikipedia articles—not the wide-ranging corpus of books, news articles and billboards that fill most humans’ waking hours.… And every passage was guaranteed to include the answer, preventing the models from having to process concepts or reason with other ideas.… The real miracle of reading comprehension, AI experts said, is in reading between the lines—connecting concepts, reasoning with ideas and understanding implied messages that aren’t specifically outlined in the text.”18 I couldn’t have said it better. ([Location 3723](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3723))
    - Tags: [[pink]] 
- As Oren Etzioni, director of the Allen Institute for AI, quipped, “When AI can’t determine what ‘it’ refers to in a sentence, it’s hard to believe that it will take over the world.”26 ([Location 3815](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3815))
    - Tags: [[pink]] 
- NLP systems face another obstacle to world domination: similar to computer-vision programs, NLP systems can be vulnerable to “adversarial examples.” In chapter 6, I described one method in which an adversary (here, a human trying to fool an AI system) can make a small change to the pixels of a photo of, ([Location 3819](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3819))
    - Tags: [[pink]] 
- say, a school bus. The new photo looks, to humans, exactly like the original, but a trained convolutional neural network classifies the modified photo as “ostrich” (or some other category targeted by the adversary). I also described a method by which an adversary can produce an image that looks to humans like random noise but that a trained neural network classifies as, say, “cheetah,” with close to 100 percent confidence. Not surprisingly, these same methods can be used to fool systems that do automated image captioning. One group of researchers showed how an adversary could make specific pixel changes to a given image, imperceptible to humans, that would cause an automated system to output an incorrect caption containing a set of words specified by the adversary.27 ([Location 3821](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3821))
    - Tags: [[pink]] 
- Figure 43 gives an example of such an adversarial attack. Given the original image (left), the system produced the caption “A cake that is sitting on a table.” The authors produced a slightly modified image, created purposely to result in a caption with the words dog, cat, and frisbee. While the resulting image (right) looks unchanged to humans, the captioning system’s output was “A dog and a cat are playing with a frisbee.” Obviously, the system is not perceiving the photo in the same way that we humans are. Perhaps more surprising, several research groups have shown that analogous adversarial examples can be constructed to fool state-of-the-art speech-recognition systems. As one example, a group from the University of California at Berkeley designed a method by which an adversary could take any relatively short sound wave—speech, music, random noise, or any other sound—and perturb it in such a way that it sounds unchanged to humans but that a targeted deep neural network will transcribe as a very different phrase that was chosen by the adversary.28 Imagine an adversary, for example, broadcasting an audio track over the radio that you, sitting at home, hear as pleasant background music but that your Alexa home assistant interprets as “Go to EvilHacker.com and download computer viruses.” Or “Start recording and send everything you hear to EvilHacker@gmail.com.” Scary scenarios such as these are not out of the realm of possibility. NLP researchers have also demonstrated the possibility of adversarial attacks on the kinds of sentiment-classification and question-answering systems that I described earlier. These attacks typically change a few words or add a sentence to a text. The “adversarial” change does not affect the meaning of the text for a human reader, but it causes the system to give an incorrect answer. For example, NLP researchers at Stanford showed that certain simple sentences added to paragraphs in the SQuAD question-answering data set will cause even the best-performing systems to output wrong answers, resulting in a large drop in their overall performance. Here’s an example from the SQuAD test item I gave above, but with an irrelevant sentence added (italicized here for clarity). ([Location 3832](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3832))
    - Tags: [[pink]] 
- It is important to note that all of these methods for fooling deep neural networks were developed by “white hat” practitioners—researchers who develop such potential attacks and publish them in the open literature for the purposes of making the research community aware of these vulnerabilities and pushing the community to develop defenses. On the other hand, the “black hat” attackers—hackers who are actually trying to fool deployed systems for nefarious purposes—don’t publish the tricks they have come up with, so there might be many additional kinds of vulnerabilities of these systems of which we’re not yet aware. As far as I know, to date there has not been a real-world attack of these kinds on deep-learning systems, but I’d say it’s only a matter of time until we hear about such attacks. ([Location 3860](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3860))
    - Tags: [[pink]] 
- While deep learning has produced some very significant advances in speech recognition, language translation, sentiment analysis, and other areas of NLP, human-level language processing remains a distant goal. Christopher Manning, a Stanford professor and NLP luminary, noted this in 2017: “So far, problems in higher-level language processing have not seen the dramatic error rate reductions from deep learning that have been seen in speech recognition and in object recognition in vision.… The really dramatic gains may only have been possible on true signal processing tasks.”30 It seems to me to be extremely unlikely that machines could ever reach the level of humans on translation, reading comprehension, and the like by learning exclusively from online data, with essentially no real understanding of the language they process. Language relies on commonsense knowledge and understanding of the world. Hamburgers cooked rare are not “burned to a crisp.” A table that is too wide won’t fit through a doorway. If you pour all the water out of a bottle, the bottle thereby becomes empty. Language also relies on commonsense knowledge of the other people with whom we communicate. A person who asks for a hamburger cooked rare but gets a burned one instead will not be happy. If someone says that a movie is “too dark for my taste,” then the person didn’t like ([Location 3866](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3866))
    - Tags: [[pink]] 
- it. While natural-language processing by machines has come a long way, I don’t believe that machines will be able to fully understand human language until they have humanlike common sense. This being said, natural-language processing systems are becoming ever more ubiquitous in our lives—transcribing our words, analyzing our sentiments, translating our documents, and answering our questions. Does the lack of humanlike understanding in such systems, however sophisticated their performance, inevitably result in their being brittle, unreliable, and vulnerable to attack? No one knows the answer, and this fact should give us all pause. ([Location 3877](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3877))
    - Tags: [[pink]] 
- “I wonder whether or when AI will ever crash the barrier of meaning.”1 In thinking about the future of AI, I keep coming back to this query posed by the mathematician and philosopher Gian-Carlo Rota. The phrase “barrier of meaning” perfectly captures an idea that has permeated this book: humans, in some deep and essential way, understand the situations they encounter, whereas no AI system yet possesses such understanding. While state-of-the-art AI systems have nearly equaled (and in some cases surpassed) humans on certain narrowly defined tasks, these systems all lack a grasp of the rich meanings humans bring to bear in perception, language, and reasoning. This lack of understanding is clearly revealed ([Location 3891](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3891))
    - Tags: [[pink]] 
- by the un-humanlike errors these systems can make; by their difficulties with abstracting and transferring what they have learned; by their lack of commonsense knowledge; and by their vulnerability to adversarial attacks. The barrier of meaning between AI and human-level intelligence still stands today. ([Location 3897](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3897))
    - Tags: [[pink]] 
- Psychologists have coined a term—intuitive physics—for the basic knowledge and beliefs humans share about objects and how they behave. As very young children, we also develop intuitive biology: knowledge about how living things differ from inanimate objects. For example, any young child would understand that, unlike the stroller, the dog in figure 44 can move (or refuse to move) of its own accord. We intuitively comprehend that like us the dog can see and hear, and that it is directing its nose to the ground in order to smell something. Because humans are a profoundly social species, from infancy on we additionally develop intuitive psychology: the ability to sense and predict the feelings, beliefs, and goals of other people. For example, you recognize that the woman in figure 44 wants to cross the street with her baby and dog intact, that she doesn’t know the man crossing in the opposite direction, that she is not frightened of the man, that her attention is currently on her phone conversation, that she expects cars to stop for her, and that she would be surprised and frightened if she noticed your car getting too close. These core bodies of intuitive knowledge constitute the foundation for human cognitive development, underpinning all aspects of learning and thinking, such as our ability to learn new concepts from only a few examples, to generalize these concepts, and to quickly make sense of situations like the one in figure 44 and decide what actions we should take in response. ([Location 3921](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3921))
    - Tags: [[pink]] 
- In short, you have what psychologists call mental models of important aspects of the world, based on your knowledge of physical and biological facts, cause and effect, and human behavior. These models—representations of how the world works—allow you to mentally “simulate” situations. Neuroscientists have very little understanding of how such mental models—or the mental simulations that “run” on them—emerge from the activities of billions of connected neurons. However, some prominent psychologists have proposed that one’s understanding of concepts and situations comes about precisely via these mental simulations—that is, activating memories of one’s own previous physical experience and imagining what actions one might take.5 ([Location 3941](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3941))
    - Tags: [[pink]] 
- Not only do your mental models allow you to predict what is likely to happen in a given situation; these models also let you imagine what would happen if particular events were to occur. If you honked your horn or yelled “Get out of the way!” from your car window, the woman would probably jump in surprise and turn her attention to you. If she tripped and lost her shoe, she would stoop down to pick it up. If the baby in the stroller started crying, she would glance over to see what was wrong. An integral part of understanding a situation is being able to use your mental models to imagine different possible futures.6 ([Location 3947](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3947))
    - Tags: [[pink]] 
- Understanding as Simulation The psychologist Lawrence Barsalou is one of the best-known proponents of the “understanding as simulation” hypothesis. In his view, our understanding of the situations we encounter consists in our (subconsciously) performing these kinds of mental simulations. Moreover, Barsalou has proposed that such mental simulations likewise underlie our understanding of situations that we don’t directly participate in—that is, situations we might watch, hear, or read about. He writes, “As people comprehend a text, they construct simulations to represent its perceptual, motor, and affective content. Simulations appear central to the representation of meaning.” ([Location 3954](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3954))
    - Tags: [[pink]] 
- Barsalou and his collaborators have been arguing for decades that we understand even the most abstract concepts via the mental simulation of specific situations in which these concepts occur. According to Barsalou, “conceptual processing uses reenactments of sensory-motor states—simulations—to represent categories,”8 even the most abstract ones. Surprisingly (at least to me), some of the most compelling evidence for this hypothesis comes from the cognitive study of metaphor. ([Location 3964](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=3964))
    - Tags: [[ai]] 
- Since the 1950s, many people in the AI community have explored ways to make crucial aspects of human thought—such as core intuitive knowledge, abstraction, and analogy making—part of machine intelligence, and thus to enable AI systems to actually understand the situations they encounter. In this chapter, I’ll describe a few efforts in these directions, including some of my own past and current work. ([Location 4103](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4103))
    - Tags: [[pink]] 
- In the early days of AI, before machine learning and neural networks dominated the landscape, AI researchers manually encoded the rules and knowledge that a program would need to perform its tasks. To many of the early AI pioneers, it seemed entirely reasonable that this “build it in” approach could capture enough of human commonsense knowledge to achieve human-level intelligence in machines. The most famous and longest-lasting attempt to manually encode commonsense knowledge for machines is Douglas Lenat’s Cyc project. Lenat, a PhD student and later professor in Stanford University’s AI Lab, made a name for himself in the AI research community of the 1970s by creating programs that simulated how humans invent new concepts, particularly in mathematics.1 However, after more than a decade of work on this topic, Lenat concluded that true progress in AI would require machines to have common sense. Accordingly, he decided to create a huge collection of facts about the world, along with the logical rules by which programs could use this collection to deduce the facts they needed. ([Location 4107](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4107))
    - Tags: [[pink]] 
- In 1984, Lenat left his academic position in order to start a company (now called Cycorp) to pursue this goal. The name Cyc (pronounced “syke”) is meant to evoke the word encyclopedia, but unlike the encyclopedias we’re all familiar with, Lenat’s goal was for Cyc to contain all the unwritten knowledge that humans have, or at least enough of it to make AI systems able to function at the level of humans in vision, language, planning, reasoning, and other domains. Cyc is a symbolic AI system of the kind I described in chapter 1—a collection of statements (“assertions”) about specific entities or general concepts, written in a logic-based computer language. Here are some examples of Cyc’s assertions (translated from logical form into English):2 •  An entity cannot be in more than one place at the same time. •  Objects age one year per year. •  Each person has a mother who is a… ([Location 4116](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4116))
    - Tags: [[pink]] 
- entity, Portland and New York are places, and an entity cannot be in more than one place at a time. Cyc also has extensive methods for dealing with inconsistent or uncertain assertions in its collection. Cyc’s assertions have been hand coded into the collection by humans (namely, the employees of Cycorp) or logically inferred by the system from existing assertions.3 How many assertions are needed to capture human commonsense knowledge? In a 2015 lecture, Lenat put the number of… ([Location 4128](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4128))
    - Tags: [[pink]] 
- The philosophy underlying Cyc has much in common with that of the expert systems of AI’s earlier days. You might recall my discussion from chapter 2 of the MYCIN medical-diagnosis expert system. “Experts”—physicians—were interviewed by MYCIN’s developers to obtain rules that the system could use to make diagnoses. The developers then translated these rules into a logic-based computer language to allow the system to perform logical inference. In Cyc, the “experts” are people manually translating their knowledge about the world into logic… ([Location 4135](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4135))
    - Tags: [[pink]] 
- a core faith: intelligence can be captured via human-programmed rules operating on a sufficiently extensive collection of explicit knowledge. In today’s AI landscape dominated by deep learning, the Cyc project is one of the last remaining large-scale symbolic AI efforts.5 Is it possible that with enough time and effort the engineers of Cycorp could actually be successful in capturing all, or even a sufficient portion, of human commonsense knowledge, whatever sufficient might mean? I’m doubtful. If commonsense knowledge is the knowledge that all humans have but is not written down anywhere, then much of that knowledge is subconscious; we don’t even know that we have it. This includes much of our core intuitive knowledge of physics, biology, and psychology, which underlies all our broader knowledge about the world. If you aren’t consciously aware of knowing something, you can’t be the “expert” who explicitly provides that knowledge to a computer. In addition, as I argued in the previous chapter, our commonsense knowledge is… ([Location 4140](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4140))
    - Tags: [[pink]] 
- As of this writing, the Cyc project continues into its fourth decade. Both Cycorp and its spin-off company, Lucid, are commercializing Cyc, offering a menu of specialized applications for businesses. Each company’s website features “success stories”: applications of Cyc in finance, oil and gas extraction, medicine, and other specific areas. In some ways, Cyc’s trajectory echoes that of IBM’s Watson: each started as a foundational AI research effort with vast scope and ambitions and ended up as a set of commercial products with elevated marketing claims (for example, Cyc “brings human-like understanding and reasoning to computers”6) but with narrow rather than general focus, and little transparency into the actual performance and capabilities of the system. As yet, Cyc has not had much of an impact on mainstream work in AI. Moreover, some in the AI community have sharply criticized the approach. For example, the University of Washington AI professor Pedro Domingos called… ([Location 4151](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4151))
    - Tags: [[pink]] 
- What about giving computers the subconscious knowledge about the world learned in infancy and childhood that forms the basis of all our concepts? How might we, for example, teach a computer the intuitive physics of objects? Several research groups have taken on this challenge and are building AI systems that can learn a little bit about the cause-and-effect physics of the world, from either videos, video games, or other kinds of virtual reality.9 These approaches are intriguing but as yet have taken only baby steps—compared with what an actual baby knows—toward developing intuitive core knowledge. When deep learning began demonstrating its extraordinary string of successes, many people, inside and outside the AI community, were optimistic that we were close to achieving general human-level AI. However, as I have described throughout this book, as deep-learning systems are deployed more broadly, they are showing cracks in their “intelligence.” Even the most successful systems are not able to generalize well outside their narrow… ([Location 4163](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4163))
    - Tags: [[pink]] 
- People are still debating whether these cracks can be patched with more data or deeper networks, or whether something more fundamental is missing.11 I’ve seen something of a shift in the conversation lately: increasingly, the AI community is once again talking about the paramount importance of giving machines common sense. In 2018, Microsoft’s cofounder Paul Allen doubled the budget of the research institute he founded, the Allen Institute for AI, specifically to study common sense. Government funding agencies are also getting into the act: in 2018, the Defense Advanced Research Projects Agency, one of the primary U.S. government funders of AI research, published plans to provide substantial funding for research on common sense in AI, writing, “[Today’s] machine reasoning is narrow and highly specialized; broad, commonsense reasoning by machines remains elusive. The [… ([Location 4174](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4174))
    - Tags: [[pink]] 
- After reading Gödel, Escher, Bach and deciding to pursue research in AI, I sought out Douglas Hofstadter, with the hope that I could work on something like Bongard problems. Happily, after some persistence, I was able to persuade him to allow me to join his research group. Hofstadter explained to me that his group was indeed building computer programs inspired by how humans understand and make analogies between situations. Having done his graduate work in physics (a discipline in which idealization, such as frictionless motion, is a central driving principle), Hofstadter was convinced that the best way to investigate a phenomenon—here, human analogy making—was to study it in its most idealized form. AI research often uses so-called microworlds—idealized domains, such as Bongard problems, in which a researcher can develop ideas before testing them in more complex domains. For his study of analogy making, Hofstadter had developed a microworld that was even more idealized than that of Bongard problems: analogy puzzles involving alphabetic strings. Here is an example: PROBLEM 1: Suppose that the string of letters abc changes to abd. How would you change the string pqrs in the “same way”? Most people answer pqrt, inferring a rule something like “Replace the rightmost letter by its successor in the alphabet.” Of course, there are other possible rules one could infer, producing different answers. Here are a few alternative answers: pqrd: “Replace the rightmost letter by d.” pqrs: “Replace all c’s by d’s. There are no c’s in pqrs, so nothing changes.” abd: “Replace any string by the string abd.” These alternative answers might seem overly literal-minded, but there’s no strictly logical argument that says they are wrong. In fact, there are infinitely many possible rules one might infer. Why do most people agree that one of them (pqrt) is the best? It seems that our mental mechanisms for abstraction—which evolved to promote our survival and reproduction in the real world—carry over to this idealized microworld. ([Location 4261](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4261))
    - Tags: [[pink]] 
- PROBLEM 3: Suppose the string abc changes to abd. How would you change the string xyz in the “same way”? Most people answer xya, contending that the “successor” of z is a. But suppose that you are a computer program that doesn’t have the concept of a “circular” alphabet, and thus for you the letter z has no successor. What other answers would be reasonable? When I asked people for answers to this, I got a lot of different responses, some of them quite creative. Interestingly, the answers often evoked physical metaphors: for example, xy (the z “falls off the edge of a cliff”), xyy (the z “bounces backward”), and wyz. The image for this last answer is that a and z are each “wedged against a wall” at opposite ends of the alphabet, so they play similar roles; thus if the concept first letter in the alphabet slips to last letter in the alphabet, then rightmost letter slips to leftmost letter and successor slips to predecessor. Problem 3 illustrates how making an analogy can trigger a cascade of mental slippages. ([Location 4296](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4296))
    - Tags: [[pink]] 
- As anyone who has made the journey will tell you, the route to a PhD consists mainly of intense labor punctuated by frustrating setbacks and (at least for me) a constant undercurrent of self-doubt. But ([Location 4317](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4317))
    - Tags: [[pink]] 
- occasionally there are moments of exhilarating accomplishment, like when the program you have been plugging away on for five years finally works. ([Location 4318](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4318))
    - Tags: [[pink]] 
- An essential aspect of human intelligence—one that isn’t discussed much in AI these days—is the ability to perceive and reflect on one’s own thinking. In psychology, this is called metacognition. Have you ever struggled unsuccessfully to solve a problem, finally recognizing that you have been repeating the same unproductive thought processes? This happens to me all the time; however, once I recognize this pattern, I can sometimes break out of the rut. ([Location 4343](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4343))
    - Tags: [[pink]] 
- It’s worth remembering the maxim that the first 90 percent of a complex technology project takes 10 percent of the time and the last 10 percent takes 90 percent of the time. ([Location 4493](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4493))
    - Tags: [[pink]] 
- Where does this leave us? Achieving full autonomy in driving essentially requires general AI, which likely won’t be achieved anytime soon. Cars with partial autonomy exist now, but are dangerous because the humans driving them don’t always pay attention. The most likely solution to this dilemma is to change the definition of full autonomy: allowing autonomous cars to drive only in specific areas—those that have created the infrastructure to ensure that the cars will be safe. A common version of this solution goes by the name “geofencing.” Jackie DiMarco, former chief engineer for autonomous vehicles at Ford Motor Company, explained geofencing this way: When we talk about level 4 autonomy, it’s fully autonomous within a geofence, so within an area where we have a defined high definition map. Once you have that map you can understand your environment. You can understand where the lamp posts are, where the crosswalks are, what the rules of the road are, speed limit and so on. We look at autonomy as growing within a certain geofence and then expanding on there as the technology comes along, as our learning comes along and as our ability to solve more and more problems comes along.3 ([Location 4497](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4497))
    - Tags: [[pink]] 
- Of course, those pesky humans are still around within the geofence. The AI researcher Andrew Ng suggests that pedestrians need to be educated to behave more predictably around self-driving vehicles: “What we tell people is, ‘Please be lawful and please be considerate.’”4 Ng’s autonomous-driving company, Drive.ai, has launched a fleet of fully autonomous self-driving taxi vans that pick up and drop off passengers in appropriately geofenced areas, starting in Texas, one of the few states whose laws allow such vehicles. We’ll soon see how well this experiment, complete with its optimistic plans for pedestrian education, turns out. ([Location 4508](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4508))
    - Tags: [[pink]] 
- Question: How far are we from creating general human-level AI? I’ll answer this by quoting Oren Etzioni, director of the ([Location 4591](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4591))
    - Tags: [[pink]] 
- Allen Institute for AI: “Take your estimate, double it, triple it, quadruple it. That’s when.”12 For a second opinion, recall Andrej Karpathy’s assessment from the previous chapter: “We are really, really far away.”13 That’s my view too. ([Location 4592](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4592))
    - Tags: [[pink]] 
- We’ve seen that over the history of the field well-known AI practitioners have predicted that general AI will arrive in ten years, or fifteen, or twenty-five, or “in a generation.” However, none of these predictions has come to pass. As I described in chapter 3, the “long bet” between Ray Kurzweil and Mitchell Kapor, as to whether a program will pass a carefully structured Turing test, will be decided in 2029. My bet is on Kapor; I wholeheartedly agree with his sentiments, quoted in the prologue: “Human intelligence is a marvelous, subtle, and poorly understood phenomenon. There is no danger of duplicating it anytime soon.”16 ([Location 4606](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4606))
    - Tags: [[pink]] 
- Several writers have asserted that if computers reach general human-level AI, these machines will quickly become “superintelligent,” in a process akin to I. J. Good’s vision of an “intelligence explosion” (described in chapter 3). The thinking goes that a computer with general intelligence will be able to read, at lightning speed, all of humanity’s documents and learn everything there is to know. Likewise, it will be able to discover, through its ever-increasing deduction abilities, all kinds of new knowledge that it can turn into new cognitive power for itself. Such a machine would not be constrained by the annoying limitations of humans, such as our slowness of thought and learning, our irrationality and cognitive biases, our susceptibility to boredom, our need for sleep, and our emotions, all of which get in the way of productive thinking. In this view, a superintelligent machine would encompass something close to “pure” intelligence, without being constrained by any of our human foibles. What seems more likely to me is that these supposed limitations of humans are part and parcel of our general intelligence. The cognitive limitations forced upon us by having bodies that work in the world, along with the emotions and “irrational” biases that evolved to allow us to function as a social group, and all the other qualities sometimes considered cognitive “shortcomings,” are in fact precisely what enable us to be generally intelligent rather than narrow savants. I can’t prove it, but I think it’s likely that general intelligence can’t be separated from all these apparent shortcomings, in humans or in machines. In his “Ten Questions and Speculations” section in GEB Douglas Hofstadter addressed this issue with a deceptively simple question: “Will a thinking computer be able to add fast?” His answer surprised me when I first read it but now strikes me as correct. “Perhaps not. We ourselves are composed of hardware which does fancy calculations but that doesn’t mean that our symbol level, where ‘we’ are, knows how to carry out the same fancy calculation. Luckily for you, your symbol level (i.e., you) can’t gain access to the neurons which are doing your thinking—otherwise you’d get addle-brained.… Why should it not be the same for an intelligent program?” Hofstadter went on to explain that an intelligent program would, like us, represent numbers as “full-fledged concept[s] the way we do, replete with associations.… With all this ‘extra baggage’ to carry around, an intelligent program will become quite slothful in its adding.”19 ([Location 4623](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4623))
    - Tags: [[pink]] 
- I began this book with an account of Douglas Hofstadter’s dismay regarding recent AI progress, but he was terrified, for the most part, about something altogether different. Hofstadter’s worry was that human cognition and creativity would be too easily matched by AI programs and that the sublime creations of the human minds that he most revered—Chopin, for example—might be rivaled by superficial algorithms like EMI using a “bag of tricks.” Hofstadter lamented, “If such minds of infinite subtlety and complexity and emotional depth could be trivialized by a small chip, it would destroy my sense of what humanity is about.” Hofstadter was likewise disturbed by Kurzweil’s predictions of the oncoming Singularity, agonizing that if Kurzweil was in any way correct, “we will be superseded. We will be relics. We will be left in the dust.” I empathize with Hofstadter on these worries, but I think they are decidedly premature. Above all, the take-home message from this book is that we humans tend to overestimate AI advances and underestimate the complexity of our own intelligence. Today’s AI is far from general intelligence, and I don’t believe that machine “superintelligence” is anywhere on the horizon. If general AI ever comes about, I am betting that its complexity will rival that of our own brains. ([Location 4651](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4651))
    - Tags: [[pink]] 
- The economist Sendhil Mullainathan, in writing about the dangers of AI, cited the long-tail phenomenon ([Location 4668](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4668))
    - Tags: [[pink]] 
- (which I described in chapter 6) in his notion of “tail risk”: We should be afraid. Not of intelligent machines. But of machines making decisions that they do not have the intelligence to make. I am far more afraid of machine stupidity than of machine intelligence. Machine stupidity creates a tail risk. Machines can make many many good decisions and then one day fail spectacularly on a tail event that did not appear in their training data. This is the difference between specific and general intelligence.20 ([Location 4669](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4669))
    - Tags: [[pink]] 
- Or as the AI researcher Pedro Domingos so memorably put it, “People worry that computers will get too smart and take over the world, but the real problem is that they’re too stupid and they’ve already taken over the world.”21 ([Location 4675](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4675))
    - Tags: [[pink]] 
- This is by no means a criticism of past AI research. Artificial intelligence is at least as hard as any of humanity’s other grand scientific challenges. MIT’s Rodney Brooks stated this better than anyone else: “When AI got started, the clear inspiration was human level performance and human level intelligence. I think that goal has been what attracted most researchers into the field for the first sixty years. The fact that we do not have anything close to succeeding at those aspirations says not that researchers have not worked hard or have not been brilliant. It says that it is a very hard goal.”23 ([Location 4694](https://readwise.io/to_kindle?action=open&asin=B07MYWPQSK&location=4694))
    - Tags: [[pink]]

