---
tags:
  - readwise
---

# Super Thinking

![rw-book-cover](https://images-na.ssl-images-amazon.com/images/I/51NVQREIpjL._SL200_.jpg)

## Metadata
- Author: [[Gabriel Weinberg, Lauren McCann]]
- Full Title: Super Thinking
- Category: #books

## Highlights
- These recurring concepts are called mental models. Once you are familiar with them, you can use them to quickly create a mental picture of a situation, which becomes a model that you can later apply in similar situations. ([Location 51](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=51))
    - Tags: [[pink]] 
- This is the book we wish someone had gifted us many years ago. No matter where you are in life, this book is designed to help jump start your super thinking journey. This reminds us of another adage, “The best time to plant a tree was twenty years ago. The second best time is now.” ([Location 139](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=139))
    - Tags: [[pink]] 
- Carl Jacobi was a nineteenth-century German mathematician who often used to say, “Invert, always invert” (actually he said, “Man muss immer umkehren,” because English wasn’t his first language). He meant that thinking about a problem from an inverse perspective can unlock new solutions and strategies. For example, most people approach investing their money from the perspective of making more money; the inverse approach would be investing money from the perspective of not losing money. Or consider healthy eating. A direct approach would be to try to construct a healthy diet, perhaps by making more food at home with controlled ingredients. An inverse approach, by contrast, would be to try to avoid unhealthy options. You might still go to all the same eating establishment but simply choose the healthier options when there. The concept of inverse thinking can help you with the challenge of making good decisions. The inverse of being right more is being wrong less. Mental models are a tool set that can help you be wrong less. They are a collection of concepts that help you more effectively navigate our complex world. ([Location 146](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=146))
    - Tags: [[pink]] 
- The central mental model to help you become a chef with your thinking is arguing from first principles. It’s the practical starting point to being wrong less, and it means thinking from the bottom up, using basic building blocks of what you think is true to build sound (and sometimes new) conclusions. First principles are the group of self-evident assumptions that make up the foundation on which your conclusions rest—the ([Location 191](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=191))
    - Tags: [[pink]] 
- the ingredients in a recipe or the mathematical axioms that underpin a formula. ([Location 194](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=194))
    - Tags: [[pink]] 
- Tesla founder Elon Musk illustrates how this process works in practice in an interview on the Foundation podcast: First principles is kind of a physics way of looking at the world. . . . You kind of boil things down to the most fundamental truths and say, “What are we sure is true?” . . . and then reason up from there. . . . Somebody could say . . . “Battery packs are really expensive and that’s just the way they will always be. . . . Historically, it has ([Location 205](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=205))
    - Tags: [[pink]] 
- cost $600 per kilowatt-hour, and so it’s not going to be much better than that in the future.” . . . With first principles, you say, “What are the material constituents of the batteries? What is the stock market value of the material constituents?” . . . It’s got cobalt, nickel, aluminum, carbon, and some polymers for separation, and a seal can. Break that down on a material basis and say, “If we bought that on the London Metal Exchange, what would each of those things cost?” . . . It’s like $80 per kilowatt-hour. So clearly you just need to think of clever ways to take those materials and combine them into the shape of a battery cell and you can have batteries that are much, much cheaper than anyone realizes. When arguing from first principles, you are deliberately starting from scratch. You are explicitly avoiding the potential trap of conventional wisdom, which could turn out to be wrong. Even if you end up in agreement with conventional wisdom, by taking the first-principles approach, you will gain a much deeper understanding of the subject at hand. Any problem can be approached from first principles. Take your next career move. Most people looking for work will apply to too many jobs and take the first job that is offered to them, which is likely not the optimal choice. When using first principles, you’ll instead begin by thinking about what you truly value in a career (e.g., autonomy, status, mission, etc.), your required job parameters (financial, location, title, etc.), and your previous experience. When you add those up, you will get a much better picture of what might work best for your next career move, and then you can actively seek that out. Thinking alone, though, even from first principles, only gets you so far. Your first principles are merely assumptions that may be true, false, or somewhere in between. Do you really value autonomy in a job, or do you just think you do? Is it really true you need to go back to school to switch careers, or might it actually be unnecessary? ([Location 210](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=210))
    - Tags: [[pink]] 
- Ultimately, to be wrong less, you also need to be testing your assumptions in the real world, a process known as de-risking. There is risk that one or more of your assumptions are untrue, and so the conclusions you reach could also be false. ([Location 226](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=226))
    - Tags: [[pink]] 
- Unfortunately, people often make the mistake of doing way too much work before testing assumptions in the real ([Location 252](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=252))
    - Tags: [[pink]] 
- world. In computer science this trap is called premature optimization, where you tweak or perfect code or algorithms (optimize) too early (prematurely). If your assumptions turn out to be wrong, you’re going to have to throw out all that work, rendering it ultimately a waste of time. ([Location 252](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=252))
    - Tags: [[pink]] 
- Back in startup land, there is another mental model to help you test your assumptions, called minimum viable product, or MVP. The MVP is the product you are developing with just enough features, the minimum amount, to be feasibly, or viably, tested by real people. ([Location 256](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=256))
    - Tags: [[pink]] 
- And boxer Mike Tyson (prior to his 1996 bout against Evander Holyfield): “Everybody has a plan until they get punched in the mouth.” No matter the context, what they’re all saying is that your first plan is probably wrong. ([Location 261](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=261))
    - Tags: [[pink]] 
- The MVP forces you to evaluate your assumptions quickly. One way you can be wrong with your assumptions is by coming up with too many or too complicated assumptions up front when there are clearly simpler sets you can start with. Ockham’s razor helps here. It advises that the simplest explanation is most likely to be true. When you encounter competing explanations that plausibly explain a set of data equally well, you probably want to choose the simplest one to investigate first. This model is a razor because it “shaves off” unnecessary assumptions. It’s named after fourteenth-century English philosopher William of Ockham, though the underlying concept has much older roots. The Greco-Roman astronomer Ptolemy (circa A.D. 90–168) stated, “We consider it a good principle to explain the phenomena by the simplest hypotheses possible.” More recently, the composer Roger Sessions, paraphrasing Albert Einstein, put it like this: “Everything should be made as simple as it can be, but not simpler!” In medicine, it’s known by this saying: “When you hear hoofbeats, think of horses, not zebras.” ([Location 272](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=272))
    - Tags: [[pink]] 
- Ockham’s razor is not a “law” in that it is always true; it just offers guidance. Sometimes the true explanation can indeed be quite complicated. However, there is no reason to jump immediately to the complex explanation when you have simpler alternatives to explore first. If you don’t simplify your assumptions, you can fall into a couple of traps, described in our next mental models. First, most people are, unfortunately, hardwired to latch onto unnecessary assumptions, a predilection called the conjunction fallacy, studied by Amos Tversky and Daniel Kahneman, who provided this example in the October 1983 Psychological Review: Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable? 1. Linda is a bank teller. 2. Linda is a bank teller and is active in the feminist movement. In their study, most people answered that number 2 is more probable, but that’s impossible unless all bank tellers are also active in the feminist movement. The fallacy arises because the probability of two events in conjunction is always less than or equal to the probability of either one of the events occurring alone, a concept illustrated in the Venn diagram on the next page. You not only have a natural tendency to think something specific is more probable than something general, but you also have a similarly fallacious tendency to explain data using too many assumptions. The mental model for this second fallacy is overfitting, a concept from statistics. ([Location 291](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=291))
    - Tags: [[pink]] 
- Overfitting occurs when you use an overly complicated explanation when a simpler one will do. It’s what happens when you don’t heed Ockham’s razor, when you get sucked into the conjunction fallacy or make a similar unforced error. It can occur in any situation where an explanation introduces unnecessary assumptions. ([Location 309](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=309))
    - Tags: [[pink]] 
- You go through life seeing everything from your perspective, which varies widely depending on your particular life experiences and current situation. In physics your perspective is called your frame of reference, a concept central to Einstein’s theory of relativity. Here’s an example from everyday life: If you are in a moving train, your reference frame is inside the train, which appears at rest to you, with objects inside the train not moving relative to one another, or to yourself. However, to someone outside the train looking in, you and all the objects in the train are moving at great speed, as seen from their different frame of reference, which is stationary to them. In fact, everything but the speed of light—even time—appears different in different frames of reference. If you’re trying to be as objective as possible when making a decision or solving a problem, you always want to account for your frame of reference. You will of course be influenced by your perspective, but you don’t want to be unknowingly influenced. And if you think you may not have the full understanding of a situation, then you must actively try to get it by looking from a variety of different frames of reference. A frame-of-reference mental trap (or useful trick, depending on your perspective) is framing. Framing refers to the way you present a situation or explanation. When you present an important issue to your coworker or family member, you try to frame it in a way that might help them best understand your perspective, setting the stage for a beneficial conversation. For example, if you want your organization to embark on an innovative yet expensive project, you might frame it to your colleagues as a potential opportunity to outshine the competition rather than as an endeavor that would require excessive resources. The latter framing may have it rejected out of hand. ([Location 321](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=321))
    - Tags: [[pink]] 
- You also need to be aware that family members and coworkers are constantly framing issues for you as well, and your perception of their ideas can vary widely based on how they are framed. When someone presents a new idea or decision to you, take a step back and consider other ways in which it could be framed. If a colleague tells you they are leaving for another job to seek a better opportunity, that may indeed be true, but it also may be true that they want to leave the organization after feeling overlooked. Multiple framings can be valid yet convey vastly different perspectives. ([Location 335](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=335))
    - Tags: [[pink]] 
- A related trap/trick is nudging. Aldert Vrij presents a compelling example in his book Detecting Lies and Deceit: ([Location 354](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=354))
    - Tags: [[pink]] 
- Another concept you will find useful when making purchasing decisions is anchoring, which describes your tendency to rely too heavily on first impressions when making decisions. You get anchored to the first piece of framing information you encounter. This tendency is commonly exploited by businesses when making offers. ([Location 364](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=364))
    - Tags: [[pink]] 
- Anchoring isn’t just for numbers. Donald Trump uses this mental model, anchoring others to his extreme positions, so that what seem like compromises are actually agreements in his favor. He wrote about this in his 1987 book Trump: The Art of the Deal: My style of deal-making is quite simple and straightforward. I aim very high, and then I just keep pushing and pushing to get what I’m after. Sometimes I settle for less than I sought, but in most cases I still end up with what I want. ([Location 381](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=381))
    - Tags: [[pink]] 
- More broadly, these mental models are all instances of a more general model, availability bias, which occurs when a bias, or distortion, creeps into your objective view of reality thanks to information recently made available to you. ([Location 386](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=386))
    - Tags: [[pink]] 
- With the rise of personalized recommendations and news feeds on the internet, availability bias has become a more and more pernicious problem. Online this model is called the filter bubble, a term coined by author Eli Pariser, ([Location 407](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=407))
    - Tags: [[pink]] 
- who wrote a book on it with the same name. Because of availability bias, you’re likely to click on things you’re already familiar with, and so Google, Facebook, and many other companies tend to show you more of what they think you already know and like. Since there are only so many items they can show you—only so many links on page one of the search results—they therefore filter out links they think you are unlikely to click on, such as opposing viewpoints, effectively placing you in a bubble. ([Location 409](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=409))
    - Tags: [[pink]] 
- When you put many similar filter bubbles together, you get echo chambers, where the same ideas seem to bounce around the same groups of people, echoing around the collective chambers of these connected filter bubbles. Echo chambers result in increased partisanship, as people have less and less exposure to alternative viewpoints. And because of availability bias, they consistently overestimate the percentage of people who hold the same opinions. It’s easy to focus solely on what is put in front of you. It’s much harder to seek out an objective frame of reference, but that is what you need to do in order to be wrong less. ([Location 419](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=419))
    - Tags: [[pink]] 
- Consequently, to be wrong less when thinking about people, you must find ways to increase your empathy, opening up a deeper understanding of what other people are really thinking. This section explores various mental models to help you do just that. In any conflict between two people, there are two sides of the story. Then there is the third story, the story that a third, impartial observer would recount. Forcing yourself to think as an impartial observer can help you in any conflict situation, including difficult business negotiations and personal disagreements. The third story helps you see the situation for what it really is. But how do you open yourself up to it? Imagine a complete recording of the situation, and then try to think about what an outside audience would say was happening if they watched or listened to the recording. What story would they tell? How much would they agree with your story? Authors Douglas Stone, Bruce Patton, and Sheila Heen explore this model in detail in their book Difficult Conversations: “The key is learning to describe the gap—or difference—between your story and the other person’s story. Whatever else you may think and feel, you can at least agree that you and the other person see things differently.” If you can coherently articulate other points of view, even those directly in conflict with your own, then you will be less likely to make biased or incorrect judgments. You will dramatically increase your empathy—your understanding of other people’s frames of reference—whether or not you agree. Additionally, if you acknowledge the perspective of the third story within difficult conversations, it can have a disarming effect, causing others involved to act less defensively. That’s because you are signaling your willingness and ability to consider an objective point of view. Doing so encourages others involved to do the same. Another tactical model that can help you empathize is the most respectful interpretation, or MRI. In any situation, you can explain a person’s behavior in many ways. MRI asks you to you interpret the other parties’ actions in the most respectful way possible. It’s giving people the benefit of the doubt. ([Location 430](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=430))
    - Tags: [[pink]] 
- The next time you feel inclined to make an accusation, take a step back and think about whether that is really a fair assumption to make. Using MRI may seem naïve, but like the third story, this model isn’t asking you to give up your point of view. Instead, MRI asks you to approach a situation from a perspective of respect. You remain open to other interpretations and withhold judgment until necessary. Another way of giving people the benefit of the doubt for their behavior is called Hanlon’s razor: never attribute to malice that which is adequately explained by carelessness. Like Ockham’s razor, Hanlon’s razor seeks out the simplest explanation. And when people do something harmful, the simplest explanation is usually that they took the path of least resistance. That is, they carelessly created the negative outcome; they did not cause the outcome out of malice. ([Location 453](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=453))
    - Tags: [[pink]] 
- The third story, most respectful interpretation, and Hanlon’s razor are all attempts to overcome what psychologists call the fundamental attribution error, where you frequently make errors by attributing others’ behaviors to their internal, or fundamental, motivations rather than external factors. You are guilty of the fundamental attribution error whenever you think someone was mean because she is mean rather than thinking she was just having a bad day. You of course tend to view your own behavior in the opposite way, which is called self-serving bias. When you are the actor, you often have self-serving reasons for your behavior, but when you are the observer, you tend to blame the other’s intrinsic nature. (That’s why this model is also sometimes called actor-observer bias.) ([Location 464](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=464))
    - Tags: [[pink]] 
- It can be challenging to acknowledge that a good portion of your success stems from luck. Many people instead choose to believe that the world is completely fair, orderly, and predictable. This view is called the just world hypothesis, where people always get what they deserve, good or bad, because of their actions alone, with no accounting for luck or randomness. This view is summed up as you reap what you sow. Ironically, belief in a just world can get in the way of actual justice by leading people to victim-blame: The sexual assault victim “should have worn different clothes” or the welfare recipient “is just lazy.” Victims of circumstance are actually blamed for their circumstances, with no accounting for factors of randomness like the birth lottery. ([Location 488](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=488))
    - Tags: [[pink]] 
- The problem with the just world hypothesis and victim-blaming is that they make broad judgments about why things are happening to people that are often inaccurate at the individual level. You should also keep in mind that the model of learned helplessness can make it hard for some people to strive for improvement without some assistance. Learned helplessness describes the tendency to stop trying to escape difficult situations because we have gotten used to difficult conditions over time. Someone learns that they are helpless to control their circumstances, so they give up trying to change them. In a series of experiments summarized in “Learned Helplessness” in the February 1972 Annual Review of Medicine, psychologist Martin Seligman placed dogs in a box where they were repeatedly shocked at random intervals. Then he placed them in a similar box where they could easily escape the shocks. However, they did not actually try to escape; they simply lay down and waited for the shocks to stop. On the other hand, dogs who were not shocked would quickly jump out of the box. ([Location 494](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=494))
    - Tags: [[pink]] 
- You don’t want to make a fundamental attribution error by assuming that your colleague is incapable of doing something when they really just need the proper guidance. All the mental models in this section—from the third story to learned helplessness—can help you increase your empathy. When applying them, you are effectively trying to understand people’s actual circumstances and motivations better, trying as best you can to walk a mile in their shoes. ([Location 511](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=511))
    - Tags: [[pink]] 
- Just as you can be anchored to a price, you can also be anchored to an entire way of thinking about something. In other words, it can be very difficult to convince you of a new idea when a contradictory idea is already entrenched in your thinking. Like many kids in the U.S., our sons are learning “Singapore math,” an approach to arithmetic that includes introducing pictorial steps in order to develop a deeper understanding of basic concepts. Even to mathematically inclined parents, this alternative way of doing arithmetic can feel foreign after so many years of thinking about it another way. Singapore Math: Addition Singapore math teaches addition using “number bonds” that break apart numbers so that students can add in groups of ten. In science, this phenomenon is documented in Thomas Kuhn’s book The Structure of Scientific Revolutions, which popularized the paradigm shift model, describing how accepted scientific theories change over time. Instead of a gradual, evolving progression, Kuhn describes a bumpy, messy process in which initial problems with a scientific theory are either ignored or rationalized away. Eventually so many issues pile up that the scientific discipline in question is thrown into a crisis mode, and the paradigm shifts to a new explanation, entering a new stable era. Essentially, the old guard holds on to the old theories way too long, even in the face of an obvious-in-hindsight alternative. Nobel Prize–winning physicist Max Planck explained it like this in his Scientific Autobiography and Other Papers: “A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it,” or, more succinctly, “Science progresses one funeral at a time.” ([Location 515](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=515))
    - Tags: [[pink]] 
- Like Wegener, Semmelweis didn’t fully understand the scientific mechanism that underpinned his theory and crafted an initial explanation that turned out to be somewhat incorrect. However, they both noticed obvious and important empirical truths that should have been investigated by other scientists but were reflexively rejected by these scientists because the suggested explanations were not in line with the conventional thinking of the time. Today, this is known as a Semmelweis reflex. Individuals still hang on to old theories in the face of seemingly overwhelming evidence—it happens all the time in science and in life in general. The human tendency to gather and interpret new information in a biased way to confirm preexisting beliefs is called confirmation bias. Unfortunately, it’s extremely easy to succumb to confirmation bias. Correspondingly, it is hard to question your own core assumptions. There is a reason why many startup companies that disrupt industries are founded by industry outsiders. There is a reason why many scientific breakthroughs are discovered by outsiders to the field. There is a reason why “fresh eyes” and “outside the box” are clichés. The reason is because outsiders aren’t rooted in existing paradigms. Their reputations aren’t at stake if they question the status quo. They are by definition “free thinkers” because they are free to think without these constraints. Confirmation bias is so hard to overcome that there is a related model called the backfire effect that describes the phenomenon of digging in further on a position when faced with clear evidence that disproves it. In other words, it often backfires when people try to change your mind with facts and figures, having the opposite effect on you than it should; you become more entrenched in the original, incorrect position, not less. ([Location 557](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=557))
    - Tags: [[pink]] 
- You may also succumb to holding on to incorrect beliefs because of disconfirmation bias, where you impose a stronger burden of proof on the ideas you don’t want to believe. Psychologist Daniel Gilbert put it like this in an April 16, 2006, article for The New York Times, “I’m O.K., You’re Biased”: When our bathroom scale delivers bad news, we hop off and then on again, just to make sure we didn’t misread the display or put too much pressure on one foot. When our scale delivers good news, we smile and head for the shower. By uncritically accepting evidence when it pleases us, and insisting on more when it doesn’t, we subtly tip the scales in our favor. ([Location 575](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=575))
    - Tags: [[pink]] 
- The pernicious effects of confirmation bias and related models can be explained by cognitive dissonance, the stress felt by holding two contradictory, dissonant, beliefs at once. Scientists have actually linked cognitive dissonance to a physical area in the brain that plays a role in helping you avoid aversive outcomes. Instead of dealing with the underlying cause of this stress—the fact that we might actually be wrong—we take the easy way out and rationalize the conflicting information away. It’s a survival instinct! Once you start looking for confirmation bias and cognitive dissonance, we guarantee you will spot them all over, including in your own thoughts. A real trick to being wrong less is to fight your instincts to dismiss new information and instead to embrace new ways of thinking and new paradigms. The meme on the next page perfectly illustrates how cognitive dissonance can make things we take for granted seem absurd. There are a couple of tactical mental models that can help you on an everyday basis to overcome your ingrained confirmation bias and tribalism. First, consider thinking gray, a concept we learned from Steven Sample’s book The Contrarian’s Guide to Leadership. You may think about issues in terms of black and white, but the truth is somewhere in between, a shade of gray. As Sample puts it: Most people are binary and instant in their judgments; that is, they immediately categorize things as good or bad, true or false, black or white, friend or foe. A truly effective leader, however, needs to be able to see the shades of gray inherent in a situation in order to make wise decisions as to how to proceed. The essence of thinking gray is this: don’t form an opinion about an important matter until you’ve heard all the relevant facts and arguments, or until circumstances force you to form an opinion without recourse to all the facts (which happens occasionally, but much less frequently than one might imagine). F. Scott Fitzgerald once described something similar to thinking gray when he observed that the test of a first-rate mind is the ability to hold two opposing thoughts at the same time while still retaining the ability to function. This model is powerful because it forces you to be patient. By delaying decision making, you avoid confirmation ([Location 581](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=581))
    - Tags: [[pink]] 
- bias since you haven’t yet made a decision to confirm! It can be difficult to think gray because all the nuance and different points of view can cause cognitive dissonance. However, it is worth fighting through that dissonance to get closer to the objective truth. A second mental model that can help you with confirmation bias is the Devil’s advocate position. This was once an official position in the Catholic Church used during the process of canonizing people as saints. Once someone is canonized, the decision is eternal, so it was critical to get it right. Hence this position was created for someone to advocate from the Devil’s point of view against the deceased person’s case for sainthood. More broadly, playing the Devil’s advocate means taking up an opposing side of an argument, even if it is one you don’t agree with. One approach is to force yourself literally to write down different cases for a given decision or appoint different members in a group to do so. Another, more effective approach is to proactively include people in a decision-making process who are known to hold opposing viewpoints. Doing so will help… ([Location 599](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=599))
    - Tags: [[pink]] 
- You probably do not have the right experience intuitively to handle everything that life throws at you, and so you should be especially wary of your intuition in any new or unfamiliar situation. ([Location 626](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=626))
    - Tags: [[pink]] 
- At the same time, intuition can help guide you to the right answer much more quickly. For example, the more you work with mental models, the more your intuition about which one to use in a given situation will be right, and the faster you will get to better decisions working with these models. In other words, as we explained at the beginning of this chapter, using mental models over time is a slow and steady way to become more antifragile, making you better able to deal with new situations over time. Of course, the better the information you put into your brain, the better your intuition will be. One way to accelerate building up useful intuition like this is to try consistently to argue from first principles. Another is to take every opportunity you can to figure out what is actually causing things to happen. The remaining mental models in this chapter can help you do just that. ([Location 630](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=630))
    - Tags: [[pink]] 
- The root cause, by contrast, is what you might call the real reason something happened. People’s explanations for their behavior are no different: anyone can give you a reason for their behavior, but that might not be the real reason they did something. For example, consistent underperformers at work usually have a plausible excuse for each incident, but the real reason is something more fundamental, such as lack of skills, motivation, or effort. ([Location 641](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=641))
    - Tags: [[pink]] 
- As part of its work, the commission conducted a postmortem. In medicine, a postmortem is an examination of a dead body to determine the root cause of death. As a metaphor, postmortem refers to any examination of a prior situation to understand what happened and how it could go better next time. At DuckDuckGo, it is mandatory to conduct a postmortem after every project so that the organization can collectively learn and become stronger (antifragile). One technique commonly used in postmortems is called 5 Whys, where you keep asking the question “Why did that happen?” until you reach the root causes. Why did the Challenger’s hydrogen tank ignite? Hot gases were leaking from the solid rocket motor. Why was hot gas leaking? A seal in the motor broke. Why did the seal break? The O-ring that was supposed to protect the seal failed. Why did the O-ring fail? It was used at a temperature outside its intended range. Why was the O-ring used outside its temperature range? Because on launch day, the temperature was below freezing, at 29 degrees Fahrenheit. (Previously, the coldest launch had been at 53 degrees.) Why did the launch go forward when it was so cold? Safety concerns were ignored at the launch meeting. Why were safety concerns ignored? There was a lack of proper checks and balances at NASA. That was the root cause, the real reason the Challenger disaster occurred. ([Location 649](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=649))
    - Tags: [[pink]] 
- Sometimes you may want something to be true so badly that you fool yourself into thinking it is likely to be true. This feeling is known as optimistic probability bias, because you are too optimistic about the probability of success. ([Location 671](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=671))
    - Tags: [[pink]] 
- The reason that root causes are so important is that, by addressing them, you can prevent the same mistakes from happening in the future. An apt analogy is that by investigating root causes, you are not just treating the symptoms but treating the underlying disease. ([Location 676](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=676))
    - Tags: [[pink]] 
- To avoid mental traps, you must think more objectively. Try arguing from first principles, getting to root causes, and seeking out the third story. Realize that your intuitive interpretations of the world can often be wrong due to availability bias, fundamental attribution error, optimistic probability bias, and other related mental models that explain common errors in thinking. Use Ockham’s razor and Hanlon’s razor to begin investigating the simplest objective explanations. Then test your theories by de-risking your assumptions, avoiding premature optimization. Attempt to think gray in an effort to consistently avoid confirmation bias. Actively seek out other perspectives by including the Devil’s advocate position and bypassing the filter bubble. Consider the adage “You are what you eat.” You need to take in a variety of foods to be a healthy person. Likewise, taking in a variety of perspectives will help you become a super thinker. ([Location 684](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=684))
    - Tags: [[pink]] 
- ALL YOUR ACTIONS HAVE CONSEQUENCES, but sometimes those consequences are unexpected. On the surface, these unintended consequences seem unpredictable. However, if you dig deeper, you will find that unintended consequences often follow predictable patterns and can therefore be avoided in many situations. You just need to know which patterns to look out for—the right mental models. ([Location 695](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=695))
    - Tags: [[pink]] 
- Through this chapter, we want to help you avoid unintended consequences like these. You will be much less likely to fall into their traps if you are equipped with the right mental models to help you better predict and deal with these situations. ([Location 714](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=714))
    - Tags: [[pink]] 
- In an 1833 essay, “Two Lectures on the Checks to Population,” economist William Lloyd described a similar, but hypothetical, overgrazing scenario, now called the tragedy of the commons. However, unbeknownst to him, his hypothetical situation had really occurred in Boston Common two hundred years earlier (and many other times before and since). More affluent families did in fact keep buying more cows, leading to overgrazing, until, in 1646, a limit of seventy cows was imposed on Boston Common. Any shared resource, or commons, is vulnerable to this tragedy. Overfishing, deforestation, and dumping waste have obvious parallels to overgrazing, though this model extends far beyond environmental issues. Each additional spam message benefits the spammer who sends it while simultaneously degrading the entire email system. Collective overuse of antibiotics in medicine and agriculture is leading to dangerous antibiotic resistance. People make self-serving edits to Wikipedia articles, diminishing the overall reliability of the encyclopedia. In each of these cases, an individual makes what appears to be a rational decision (e.g., prescribing an antibiotic to a patient who might have a bacterial infection). They use the common resource for their own benefit at little or no cost (e.g., each course of treatment has only a small chance of increasing resistance). But as more and more people make the same decision, the common resource is collectively depleted, reducing the ability for everyone to benefit from it ([Location 723](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=723))
    - Tags: [[pink]] 
- in the future (e.g., the antibiotic becomes much less useful). More broadly, the tragedy of the commons arises from what is called the tyranny of small decisions, where a series of small, individually rational decisions ultimately leads to a system-wide negative consequence, or tyranny. It’s death by a thousand cuts. ([Location 734](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=734))
    - Tags: [[pink]] 
- You can also find the tyranny of small decisions in your own life. Think of those small credit card purchases or expenses that seem individually warranted at the time, but collectively add up to significant credit card bills or cash crunches. Professionally, it may be the occasional distractions and small procrastinations that, in aggregate, make your deadlines hard to reach. The tyranny of small decisions can be avoided when someone who has a view over the whole system can veto or curb particular individual decisions when broad negative impacts can be foreseen. When the decisions are all your own, you could do this for yourself. For example, to stop your out-of-control spending, you could self-impose a budget, checking each potential purchase against the budget to see if it’s compatible with your spending plan. You could do the same for your time management, by more strictly regulating your calendar. ([Location 745](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=745))
    - Tags: [[pink]] 
- Another cause of issues like the tragedy of the commons is the free rider problem, where some people get a free ride by using a resource without paying for it. People or companies who cheat on their taxes are free riders to government services they use, such as infrastructure and the legal system. If you’ve ever worked on a team project where one person didn’t do anything substantive, that person was free-riding on the rest of the group. Another familiar example: Has anyone ever leeched off your wi-fi or Netflix account? Or perhaps you’ve been the free rider? Free-riding is commonplace with public goods, such as national militaries, broadcast television, even the air we breathe. As you can see from these examples, it is usually difficult to exclude people from using public goods, because they are broadly available (public). Since one person’s use does not significantly reduce a public good’s availability to others, it might seem as though there is no harm in free-riding. However, if enough people free-ride on a public good, then it can degrade to the point of creating a tragedy of the commons. Vaccinations provide an illustrative example that combines all these models (tragedy of the commons, free rider problem, tyranny of small decisions, public goods), plus one more: herd immunity. Diseases can spread only when they have an eligible host to infect. However, when the vast ([Location 753](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=753))
    - Tags: [[pink]] 
- majority of people are vaccinated against a disease, there are very few eligible new hosts, since most people (in the herd) are immune from infection due to getting vaccinated. As a result, the overall public is less susceptible to outbreaks of the disease. In this example, the public good is a disease-free environment due to herd immunity, and the free riders are those who take advantage of this public good by not getting vaccinated. The tyranny of small decisions can arise when enough individuals choose not to get vaccinated, resulting in an outbreak of the disease, creating a tragedy of the commons. In practice, the percentage of people who need to be vaccinated for a given disease to achieve herd immunity varies by how contagious the disease is. For measles, an extremely contagious disease, the threshold is about 95 percent. That means an outbreak is possible if the measles vaccination rate in a community falls below 95 percent! Before the measles vaccine was introduced in 1963, more than 500,000 people a year contracted measles in the United States, resulting in more than 400 annual deaths. After the vaccine was in popular use, measles deaths dropped to literally zero. In recent years, some parents… ([Location 763](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=763))
    - Tags: [[pink]] 
- Herd immunity as a concept is useful beyond the medical context. It applies directly in maintaining social, cultural, business, and industry norms. If enough infractions are left unchecked, their incidence can start increasing quickly, creating a new negative norm that can be difficult to unwind. For example, in Italy, a common phrase is used to describe the current cultural norm around paying taxes: “Only fools pay.” Though Italy has been actively fighting tax evasion in the past decade, this pervasive cultural norm of tax avoidance took hold over a longer period and is proving hard to reverse. In situations like these, dropping below a herd immunity threshold can create lasting harm. It can be difficult to put the genie back in the bottle. Imagine a once pristine place that is now littered with garbage and graffiti. Once it has become dirtied, that state can quickly become the new normal, and the longer it remains dirty, the more likely it will remain in the dirty state. Hollowed-out urban centers like Detroit or disaster-ridden areas like parts of New Orleans have seen this scenario play out in the recent past. People who don’t want to live with the effects of the degradation but also don’t want to do the hard work to clean it up may simply move out of the area or visit less, further degrading the space due to lack of a tax base to fund proper maintenance. It then takes a much larger effort to revitalize the area than it would have taken ([Location 802](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=802))
    - Tags: [[pink]] 
- to keep it nice in the first place. Not only do the funds need to be found for the revitalization effort, but the expectation that it should be a nice place has to be reset, and then people need to be drawn back to it. All these unintended consequences we’ve been talking about have a name from economics: externalities, which are consequences, good or bad, that affect an entity without its consent, imposed from an external source. The infant who cannot be vaccinated, for example, receives a positive externality from those who choose to vaccinate (less chance of getting the disease) and a negative externality from those who do not (more chance of getting the disease). Similarly, air pollution by a factory creates a negative externality for the people living nearby—low air quality. If that same company, though, trained all its workers in first aid, residents would receive a positive externality if some of those workers used that training to save lives outside of work. Externalities occur wherever there are spillover effects, which happen when an effect of an activity spills over outside the core interactions of the activity. The effects of smoking spill over to surrounding people through secondhand smoke and, more broadly, through increased public healthcare expenditures. Sometimes spillover effects can be more subtle. When you buy a car, you add congestion to the roads you drive on, a cost borne by everyone who drives on the same roads. Or when you keep your neighbors up with loud music, you deprive them of sleep, causing them to be less productive. ([Location 813](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=813))
    - Tags: [[pink]] 
- Addressing negative externalities is often referred to as internalizing them. Internalizing is an attempt to require the entity that causes the negative externality to pay for it. Ideally the “price” attached to the unwanted activity is high enough that it totally covers the cost of dealing with that activity’s consequences. A high price can also stop the harm from occurring in the first place. If you see a sign warning of a five-hundred-dollar fine for littering, you will be sure to find a trash can. There are many ways to internalize negative externalities, including taxes, fines, regulation, and lawsuits. Smoking externalities are internalized via cigarette taxes and higher health insurance premiums for smokers. Traffic congestion externalities are internalized through tolls. On a personal level, your neighbor might file a noise complaint against you if you consistently play music too loud. Another way to internalize externalities is… ([Location 827](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=827))
    - Tags: [[pink]] 
- that an externality can be internalized efficiently without further need for intervention (that is, without a government or other authority regulating the externality) if the following conditions are met: Well-defined property rights Rational actors Low transaction costs When these conditions are met, entities surrounding the externality will transact among themselves until the extra costs are internalized. If you recall the Boston Common example, the externality from overgrazing was internalized by setting a limit on the number of cows per farmer (regulation). There were no property rights though. The Coase theorem holds that instead of limiting the cows, another solution would have been to simply divide the grazing rights to the commons property among the farmers. The farmers could then trade the grazing rights among themselves, creating an efficient marketplace for the use of the commons. Governments have similarly tried to address the negative externalities from the burning of fossil fuels (e.g., climate change) through cap-and-trade systems, which are modern-day… ([Location 836](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=836))
    - Tags: [[pink]] 
- the imposed limit on the number of cows that could graze on Boston Common. Then companies can trade permits on an open exchange. Such a system satisfies the conditions of the Coase theorem because property rights are well defined through the permitting process, companies act rationally to maximize their profits, and the open market provides low transaction costs. If you’re in charge of any system or policy, you want to think through the possible negative externalities ahead of time and devise ways to avoid them. What spillover effects could occur, and who would be affected by them? Is there a common resource that free riders… ([Location 846](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=846))
    - Tags: [[pink]] 
- Another set of unintended consequences can arise when people assess risk differently based on their individual positions and perspectives. These types of complications happen a lot with insurance, where risk assessments create financial consequences. For example, will you drive more recklessly in a rental car after you purchase extra rental insurance, simply because you’re more protected financially from a crash? On average, people do. This phenomenon, known as moral hazard, is where you take on more risk, or hazard, once you have information that encourages you to believe you are more protected. It has been a concern of the insurance industry since the seventeenth century! Sometimes moral hazard may involve only one person: wearing a bike helmet may give you a false sense of security, leading you to bike more recklessly, but you are the one who bears all the costs of a bike crash. Moral hazards can also occur when a person or company serves as an agent for another person or company, making decisions on behalf of this entity, known as the principal. The problem arises when the agent takes on more risk than the principal would if the principal were acting alone,… ([Location 854](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=854))
    - Tags: [[pink]] 
- net worth as much. Agency can lead to other issues as well, collectively known as the principal-agent problem, where the self-interest of the agent may lead to suboptimal results for the principal across a wide variety of circumstances. Politicians don’t always act in the best interest of their constituents; real estate agents don’t always act in the best interest of their sellers; financial brokers don’t always act in the best interest of their clients; corporate management doesn’t always… ([Location 865](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=865))
    - Tags: [[pink]] 
- Moral hazard and principal-agent problems can occur because of asymmetric information, where one side of a transaction has different information than the other side; that is, the available information is not symmetrically distributed. Real estate agents have more information about the real estate market than sellers, so it is hard to question their recommendations. Similarly, a financial adviser generally has more information about the financial markets than their clients. ([Location 882](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=882))
    - Tags: [[pink]] 
- The “Death Spiral” of Adverse Selection ([Location 908](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=908))
    - Tags: [[pink]] 
- The mental models from the last section (tragedy of the commons, externalities, etc.) and those from this section (moral hazard, information asymmetry, etc.) are signs of market failure, where open markets without intervention can create suboptimal results, or fail. To correct a market failure, an outside party must intervene in some way. Unfortunately, these interventions themselves can also fail, a result called government failure or political failure. ([Location 915](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=915))
    - Tags: [[pink]] 
- In any situation where risk and reward are separated across different entities, like this one, you want to look out for risk-related unintended consequences. ([Location 946](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=946))
    - Tags: [[pink]] 
- Goodhart’s law summarizes the issue: When a measure becomes a target, it ceases to be a good measure. This more common phrasing is from Cambridge anthropologist Marilyn Strathern in her 1997 paper “‘Improving Ratings’: Audit in the British University System.” However, the “law” is named after English economist Charles Goodhart, whose original formulation in a conference paper presented at the Reserve Bank of Australia in 1975 stated: “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.” Social psychologist Donald T. Campbell formulated a ([Location 956](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=956))
    - Tags: [[pink]] 
- similar “law” (known as Campbell’s law) in his 1979 study, “Assessing the Impact of Planned Social Change.” He explains the concept a bit more precisely: “The more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor.” Both describe the same basic phenomenon: When you try to incentivize behavior by setting a measurable target, people focus primarily on achieving that measure, often in ways you didn’t intend. Most importantly, their focus on the measure may not correlate to the behavior you hoped to promote. ([Location 961](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=961))
    - Tags: [[pink]] 
- It’s like a wish-granting genie who finds loopholes in your wishes, meeting the letter of the wish but not its spirit, and rendering you worse off than when you started. In fact, there is a mental model for this more specific situation, called the cobra effect, describing when an attempted solution actually makes the problem worse. This model gets its name from a situation involving actual cobras. When the British were governing India, they were concerned about the number of these deadly snakes, and so they started offering a monetary reward for every snake brought to them. Initially the policy worked well, and the cobra population decreased. But soon, local entrepreneurs started breeding cobras just to collect the bounties. After the government found out and ended the policy, all the cobras that were being used for breeding were released, increasing the cobra population even further. A similar thing happened under French rule of Vietnam. In Hanoi, the local government created a bounty program for rats, paying the bounty based on a rat’s tail. Enterprising ratcatchers, however, would catch and release the rats after just cutting off their tails; that way the rats could go back and reproduce. Whenever you create an incentive structure, you must heed Goodhart’s law and watch out for perverse incentives, lest you be overrun by cobras and rats! The Streisand effect applies to an even more specific situation: when you unintentionally draw more attention to something when you try to hide it. It’s named for entertainer Barbra Streisand, who sued a photographer and website in 2003 for displaying an aerial photo of her mansion, which she wanted to remain private. Before the suit, the image had been downloaded a total of six times from the site; after people saw news stories about the lawsuit, the site was visited hundreds of thousands of times, and now the photo is free to license and is on display on Wikipedia and many other places. As was said of Watergate, It’s not the crime, it’s the cover-up. ([Location 975](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=975))
    - Tags: [[pink]] 
- A related model to watch out for is the hydra effect, named after the Lernaean Hydra, a beast from Greek mythology that grows two heads for each one that is cut off. When you arrest one drug dealer, they are quickly replaced by another who steps in to meet the demand. When you shut down an internet site where people share illegal ([Location 992](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=992))
    - Tags: [[pink]] 
- movies or music, more pop up in its place. Regime change in a country can result in an even worse regime. An apt adage is Don’t kick a hornet’s nest, meaning don’t disturb something that is going to create a lot more trouble than it is worth. With all these traps—Goodhart’s law, along with the cobra, hydra, and Streisand effects—if you are going to think about changing a system or situation, you must account for and quickly react to the clever ways people may respond. There will often be individuals who try to game the system or otherwise subvert what you’re trying to do for their personal gain or amusement. If you do engage, another trap to watch out for is the observer effect, where there is an effect on something depending on how you observe it, or even who observes it. An everyday example is using a tire pressure gauge. In order to measure the pressure, you must also let out some of the air, reducing the pressure of the tire in the process. Or, when the big boss comes to town, everyone acts on their best behavior and dresses in nicer clothes. The observer effect is certainly something to be aware of when making actual measurements, but you should also consider how people might indirectly change their behavior as they become less anonymous. Think of… ([Location 994](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=994))
    - Tags: [[pink]] 
- traffic patterns before and after the 2013 revelations by Edward Snowden about the U.S. National Security Agency’s internet spying tactics, finding a 20 percent decline in terrorism-related article views involving terms like al-Qaeda, Taliban, and car bomb. The implication is that when people realized they were being watched by their governments, some of them stopped reading… ([Location 1006](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1006))
    - Tags: [[pink]] 
- In the legal context where the term chilling effect originated, it refers to when people feel discouraged, or chilled, from freely exercising their rights, for fear of lawsuits or prosecution. More generally, chilling effects are a type of observer… ([Location 1011](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1011))
    - Tags: [[pink]] 
- This negative unintended consequence could be considered collateral damage. In a military context, this term means injuries, damage, inflicted on unintended, collateral, targets. You can apply this model to any negative side effects that result from an action. The U.S. government maintains a No Fly List of people who are prohibited from commercial air travel within, into, or out of the U.S. There have been many cases of people with the same names as those on the list who experienced the collateral damage of being denied boarding and missing flights, including a U.S. Marine who was prevented from boarding a flight home from his military tour in Iraq. When people are deported or jailed, even for good reason, collateral damage can be inflicted on their family members. For instance, the loss of income could take a financial toll, or children could experience the trauma of growing up without one or both parents, possibly ending up in foster care. Sometimes collateral damage can impact the entity that inflicted the damage in the first place, which is called blowback. Blowback sometimes can occur well after the initial action. The U.S. supported Afghan insurgents in the 1980s against the USSR. Years later these same groups joined al-Qaeda to fight against the U.S., using some of the very same weapons the U.S. had provided decades earlier. ([Location 1026](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1026))
    - Tags: [[pink]] 
- In other words, seemingly small changes in incentive structures can really matter. You should align the outcome you desire as closely as possible with the incentives you provide. You should expect people generally to act in their own perceived self-interest, and so you want to be sure this perceived self-interest directly supports your goals. ([Location 1051](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1051))
    - Tags: [[pink]] 
- IT’S GETTING HOT IN HERE In the first section of this chapter, we warned about the tyranny of small decisions, where a series of isolated and seemingly good decisions can nevertheless add up to a bad outcome. There is a broader class of unintended consequences to similarly watch out for, which also involve making seemingly good short-term decisions that can still add up to a bad outcome in the long term. The mental model often used to describe this class of unintended consequences is called the boiling frog: Suppose a frog jumps into a pot of cold water. Slowly the heat is turned up and up and up, eventually boiling the frog to death. ([Location 1054](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1054))
    - Tags: [[pink]] 
- It turns out real frogs generally jump out of the hot water in this situation, but the metaphorical boiling frog persists as a useful mental model describing how a gradual change can be hard to react to, or even perceive. The boiling frog has been used as a cautionary tale in a variety of contexts, from climate change to abusive relationships to the erosion of personal privacy. It is sometimes paired with another animal metaphor, also scientifically untrue—that of the ostrich with its head in the sand, ignoring the signs of danger. In each case the unintended consequence of not acting earlier is eventually an extremely unpleasant state that is hard to get out of—global warming, domestic violence, mass surveillance. These unintended consequences are likely to arise when people don’t plan for the long term. From finance, short-termism describes these types of situations, when you focus on short-term results, such as quarterly earnings, over long-term results, such as five-year profits. If you focus on just short-term financial results, you won’t invest enough in the future. Eventually you will be left behind by competitors who are making those long-term investments, or you could be swiftly disrupted by new upstarts (which we cover in Chapter 9). ([Location 1058](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1058))
    - Tags: [[pink]] 
- This model can likewise be extended to any area to describe the unintended consequences of short-term thinking: relationship debt, diet debt, cleaning debt. ([Location 1081](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1081))
    - Tags: [[pink]] 
- In these scenarios, you need to keep up with your “payments” or else the debt can become overwhelming: the out-of-control messy house, the expanding waistline, or the deteriorating relationship. These outstanding debts impact your long-term flexibility. The general model for this impact comes from economics and is called path dependence, meaning that the set of decisions, or paths, available to you now is dependent on your past decisions. ([Location 1086](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1086))
    - Tags: [[pink]] 
- On a personal level, many people are likely to stay near the town where they went to school once they graduate. This creates a massive long-term impact on their available career and family choices. ([Location 1093](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1093))
    - Tags: [[pink]] 
- To escape the fate of the old lady or the boiling frog, you need to think about the long-term consequences of short-term decisions. For any decision, ask yourself: What kind of debt am I incurring by doing this? What future paths am I taking away by my actions today? Another model from economics offers some reprieve from the limitations of path dependence: preserving optionality. The idea is to make choices that preserve future options. Maybe as a business you put some excess profits into a rainy-day fund, or as an employee you dedicate some time to learning new skills that might give you options for future employment. Or, when faced with a decision, maybe you can delay deciding at all (see thinking gray in Chapter 1) and, instead, continue to wait for more information, keeping your options open until you are more certain of a better path to embark upon. ([Location 1110](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1110))
    - Tags: [[pink]] 
- The downside of keeping many options open is that it often requires more resources, increasing costs. Think of going to school while you also have a full-time job, maintaining multiple homes, or exploring several lines of business in one parent company. You need to find the right balance between preserving optionality and path dependence. One model that can help you figure out how to strike this balance in certain situations is the precautionary principle: when an action could possibly create harm of an unknown magnitude, you should proceed with extreme caution before enacting the policy. It’s like the medical principle of “First, do no harm.” ([Location 1123](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1123))
    - Tags: [[pink]] 
- These mental models are the most useful when thinking about existential risks. After all, in the tale of the boiling frog, the frog dies. Therefore, you want first to assess what substantial harms could arise in the long term, then work backward to assess how your short-term decisions (or lack thereof) might be contributing to long-term negative scenarios (a process that we cover in more depth in Chapter 6). With this knowledge, you can then take the necessary level of precaution, paying down technical debt as needed, happily preventing yourself from becoming the boiling frog. ([Location 1138](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1138))
    - Tags: [[pink]] 
- On the side of an ancient Greek temple, home to the Oracle of Delphi, was inscribed the precept Nothing in excess. Our modern equivalent is too much of a good thing. It’s natural to want more of something good, but too much of it can be bad. ([Location 1143](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1143))
    - Tags: [[pink]] 
- Of course, you need some information to make good decisions, but too much information leads to information overload, which complicates a decision-making process. The excess information can overload the processing capacity of the system, be it a single person, group, or even computer, causing decision making to take too long. There is a name for this unintended consequence: analysis paralysis, where your decision making suffers from paralysis because you are over-analyzing the large amount of information available. This is why you can spend too much time trying to make that coffeemaker decision or choosing where to go out to dinner when faced with an endless list of choices from Yelp. More seriously, people often stay in a job they don’t like because they are unsure of what to do next given all the possibilities. The model perfect is the enemy of good drives home this point—if you wait for the perfect decision, or perfect anything, really, you may be waiting a long time indeed. And by not making a choice, you are actually making a choice: you are choosing the status quo, which could be considerably worse than one of the other choices you could already have made. ([Location 1150](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1150))
    - Tags: [[pink]] 
- There is a natural conflict between the desire to make decisions quickly and the feeling that you need to accumulate more information to be sure you are making the right choice. You can deal with this conflict by categorizing decisions as either reversible decisions or irreversible decisions. Irreversible decisions are hard if not impossible to unwind. And they tend to be really important. Think of selling your business or having a kid. This model holds that these decisions require a different decision-making process than their reversible counterparts, which should be treated much more fluidly. In a letter to shareholders, Amazon CEO Jeff Bezos stressed the importance of this model: Some decisions are consequential and irreversible or nearly irreversible—one-way doors—and these decisions must be made methodically, carefully, slowly, with great deliberation and consultation. If you walk through and don’t like what you see on the other side, you can’t get back to where you were before. . . . But most decisions aren’t like that—they are changeable, reversible—they’re two-way doors. If you’ve made a suboptimal [reversible] decision, you don’t have to live with the consequences for that long. You can reopen the door and go back through. . . . As organizations get larger, there seems to be a tendency to use the heavy-weight [irreversible] decision-making process on most decisions, including many [reversible] decisions. The end result of this is slowness, unthoughtful risk aversion, failure to experiment sufficiently, and consequently diminished invention. ([Location 1159](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1159))
    - Tags: [[pink]] 
- They found that a greater number of choices increased the decision time logarithmically, in a formulation now known as Hick’s law. Hick’s law is regularly cited as an important factor in user-experience designs, such as in the design of restaurant menus, website navigation, and forms (offline or online). For instance, on a menu, having a vegetarian section allows vegetarians to efficiently narrow the sections of the menu they should read through. Being able to determine quickly whether there are enough vegetarian options on a menu might be a big factor in whether a family with a vegetarian would choose to eat at your restaurant. In your own life, you can use Hick’s law to remember that decision time is going to increase with the number of choices, and so if you want people to make quick decisions, reduce the number of choices. One way to do this is to give yourself or others a multi-step decision with fewer choices at each step, such as asking what type of restaurant to go to (Italian, Mexican, etc.), and then offering another set of choices within the chosen category. In addition to increased decision-making time, there is evidence that a wealth of options can create anxiety in certain contexts. This anxiety is known as the paradox of choice, named after a 2004 book of the same name by American psychologist Barry Schwartz. ([Location 1175](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1175))
    - Tags: [[pink]] 
- Hick’s law and the paradox of choice explain downsides of having many choices. There is also a model that explains the downside of making many decisions in a limited period: decision fatigue. As you make more and more decisions, you get fatigued, leading to a worsening of decision quality. After taking a mental break, you effectively reset and start making higher-quality decisions again. ([Location 1198](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1198))
    - Tags: [[pink]] 
- In this chapter we’ve covered an array of unintended consequences, from market failure to perverse incentives, from too much focus on the short term to too much of a good thing. Most generally, consider heeding Murphy’s law: Anything that can go wrong, will go wrong. It’s named after aerospace engineer Edward Murphy, from his remarks after his measurement devices failed to perform as expected. It was intended as a defensive suggestion, to remind you to be prepared and to have a plan for when things go wrong. ([Location 1212](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1212))
    - Tags: [[pink]] 
- Look around—when you see unintended consequences in a situation, be it personal, professional, or in the wider world, one of these models is usually lurking behind. Next time, see if you can identify the underlying mental model behind the situation, and also try to think ahead about how it might apply to your own plans under consideration. ([Location 1219](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1219))
    - Tags: [[pink]] 
- KEY TAKEAWAYS In any situation where you can spot spillover effects (like a polluting factory), look for an externality (like bad health effects) lurking nearby. Fixing it will require intervention either by fiat (like government regulation) or by setting up a marketplace system according to the Coase theorem (like cap and trade). Public goods (like education) are particularly susceptible to the tragedy of the commons (like poor schools) via the free rider problem (like not paying taxes). Beware of situations with asymmetric information, as they can lead to principal-agent… ([Location 1223](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1223))
    - Tags: [[pink]] 
- Short-termism can easily lead to the accumulation of technical debt and create disadvantageous path dependence; to counteract it, think about preserving optionality and keep in mind the precautionary principle. Internalize the distinction between irreversible and reversible decisions, and don’t… ([Location 1230](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1230))
    - Tags: [[pink]] 
- POLARIS IS THE BRIGHTEST STAR in the Little Dipper, a constellation also known as Ursa Minor, or Little Bear. You can easily find Polaris in the night sky because it is the last star in the handle of the Little Dipper, and the two outermost stars on the ladle of the Big Dipper point directly to it. Finding Polaris Since at least as far back as the Middle Ages, Polaris has played a critical role in navigation. Given its unique location, almost directly above the North Pole, Polaris appears nearly fixed in the night sky, despite the Earth’s rotation. You can know roughly what direction you’re headed in just by looking up at it. If you want to head north, simply orient yourself toward Polaris. A typical northern hemisphere star trail with Polaris in the center. In the business world, there is a mental model that draws on Polaris for inspiration, called north star, which refers to the guiding vision of a company. For example, DuckDuckGo’s north star is “to raise the standard of trust online.” If you know your north star, you can point your actions toward your desired long-term future. Without a north star, you can be easily “lost at sea,” susceptible to the unintended consequences of short-termism (see… ([Location 1236](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1236))
    - Tags: [[pink]] 
- It’s okay if your north star evolves as you progress toward it. You may discover greater clarity about what you want to accomplish, or a life event (e.g., marriage, kids, career/location change) may propel you in another direction. You may also need a new north star if you reach your destination! For instance, a teenager’s north star might be getting into a certain university program, but once that has been reached, a new north star will be needed. A north star is a long-term vision, so it is also okay if you don’t reach it anytime soon. However, if you don’t know where you want to go, how do you expect to ever get there? Your north star will help guide you through various life choices, slowly but steadily navigating you closer to your goals. In his 1996 book The Road Ahead, entrepreneur and philanthropist Bill Gates commented on this power of incremental progress: “People often overestimate what will happen in the next two years and underestimate what will happen in the next ten.” Gates wrote this statement in a business context, as a warning to not ignore far-off threats that can grow into major disrupters. That is, don’t underestimate how far emerging competitors can advance or how much technology can change in ten years. Think of how Netflix progressed in a decade from a tiny niche to disrupting the entire cable-television industry. This idea can also be powerful to you as an individual. Your incremental progress toward a goal may not be noticeable day to day. But over a long period of time, many small steps can get you really far if you stay pointed in the right direction. ([Location 1256](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1256))
    - Tags: [[pink]] 
- In a personal context, as long as you are pointed toward your north star, you have the opportunity to take advantage of the same concept by compounding your ability to move in your chosen direction. That’s because what you can accomplish draws on your cumulative knowledge, skills, and network. As these grow, so does your impact potential. ([Location 1274](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1274))
    - Tags: [[pink]] 
- Startup investor Paul Graham calls it the top idea in your mind in his 2010 essay of the same name: ([Location 1307](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1307))
    - Tags: [[pink]] 
- Everyone who’s worked on difficult problems is probably familiar with the phenomenon of working hard to figure something out, failing, and then suddenly seeing the answer a bit later while doing something else. There’s a kind of thinking you do without trying to. I’m increasingly convinced this type of thinking is not merely helpful in solving hard problems, but necessary. The tricky part is, you can only control it indirectly. I think most people have one top idea in their mind at any given time. That’s the idea their thoughts will drift toward when they’re allowed to drift freely. And this idea will thus tend to get all the benefit of that type of thinking, while others are starved of it. Which means it’s a disaster to let the wrong idea become the top one in your mind. If you are constantly switching between activities, you don’t end up doing much creative thinking at all. Author Cal Newport refers to the type of thinking that leads to breakthrough solutions as deep work. He advocates for dedicating long, uninterrupted periods of time to making progress on your most important problem. In a November 6, 2014, lecture titled “How to Operate,” entrepreneur and investor Keith Rabois tells a story about how Peter Thiel used this concept when he was CEO of PayPal: [Peter] used to insist at PayPal that every single person could only do exactly one thing. And we all rebelled, every single person in the company rebelled to this idea. Because it’s so unnatural, it’s so different than other companies where people wanted to do multiple things, especially as you get more senior, you definitely want to do more things and you feel insulted to be asked to do just one thing. ([Location 1308](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1308))
    - Tags: [[pink]] 
- Peter would enforce this pretty strictly. He would say, I will not talk to you about anything else besides this one thing I assigned you. I don’t want to hear about how great you are doing over here, just shut up, and Peter would run away. . . . The insight behind this is that most people will solve problems that they understand how to solve. Roughly speaking, they will solve B+ problems instead of A+ problems. A+ problems are high-impact problems for your company, but they are difficult. You don’t wake up in the morning with a solution, so you tend to procrastinate them. So imagine you wake up in the morning and create a list of things to do today, there’s usually the A+ one on the top of the list, but you never get around to it. And so you solve the second and third. Then you have a company of over a hundred people, so it cascades. You have a company that is always solving B… ([Location 1322](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1322))
    - Tags: [[pink]] 
- U.S. President Dwight Eisenhower famously quipped, “What is important is seldom urgent and what is urgent is seldom important.” This quote inspired Stephen Covey in 7 Habits of Highly Effective People to create the Eisenhower Decision Matrix, a two-by-two grid (matrix) that helps you prioritize important activities across both your personal and your professional life by categorizing them according to their urgency and importance. Eisenhower Decision Matrix   Urgent Not Urgent Important I-MANAGE • Crisis/emergency • Family obligations • Real deadlines II-FOCUS • Strategic planning • Relationship-building • Deep work Not Important III-TRIAGE • Interruptions • Many “pressing” matters • Most events IV-AVOID • Busywork • Picking out clothes • Most email and messages Activities in quadrant I (Urgent/Important, such as handling a medical emergency) need to be done immediately. Activities in quadrant II (Not Urgent/Important, such as deep work) are also crucial, and should be prioritized just after the activities from quadrant I. You should focus your creative energies on these quadrant II activities as much as possible, because working on them will drive you fastest toward your long-term goals. The activities in quadrant III (Urgent/Not Important, such as most events and many “pressing” matters) might be better delegated, outsourced, or just ignored. Finally, quadrant IV (Not Urgent/Not Important, such as busywork and most email) contains activities you should try to reduce or eliminate spending time on altogether. The essential insight to be gained from this matrix is that the important activities in quadrant II are often overshadowed by the urgent distractions in quadrant III. You can be tricked into addressing the tasks in quadrant III immediately because they have urgency, vying for your attention. However, if you let those distractions in quadrant III take up a lot of your time, you may never get to the important tasks in quadrant II. Similarly, the Not Urgent/Not Important things in quadrant IV can be attractive distractions because they provide immediate gratification (like completing a busywork task quickly) or are fun (like mindless phone games). It would be unhealthy to get rid of leisure completely in your life, but it is essential to evaluate how much of your time is being spent on leisure and unimportant activities so that they don’t get in the way… ([Location 1333](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1333))
    - Tags: [[pink]] 
- In his 1957 book Parkinson’s Law, Parkinson presents an example of a budget committee considering an atomic reactor and a bike shed, offering that “the time spent on any item of the agenda will be in inverse proportion to the sum involved.” The committee members are reluctant to deeply discuss all of the complicated aspects of the atomic reactor decision because it is challenging and esoteric. By contrast, everyone wants to weigh in with their opinion on the bike shed decision because it is easy and familiar relative to the reactor, even though it is also relatively unimportant. This phenomenon has become known as bike-shedding. You must try not to let yourself get sucked into these types of debates, because they rob you of time that can be spent on important issues. In the budget meeting, the agenda could instead be structured so that time is pre-allocated proportionally to the relative importance of each item, and items can also be ordered by importance. That way much greater time will be apportioned to… ([Location 1376](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1376))
    - Tags: [[pink]] 
- This section’s theme is succinctly summarized by a quote from a Fast Company interview with productivity consultant David Allen, author of Getting Things Done: “You can do anything, but not everything.” You must pick between the important activities in front of you, or else you will find yourself multitasking and lacking time for deep work. Allen also notes that “there is always more to do than there is time to do it, especially in an environment of so much possibility. We all want to be acknowledged; we all want our work to be meaningful. And in an attempt to achieve that goal, we all keep letting stuff enter our lives.” Luckily, there is an extremely powerful mental model from economics to guide you: opportunity cost. Every choice you make has a cost: the value of the best alternative opportunity you didn’t choose. As a rule, you want to choose the option with the lowest opportunity cost. ([Location 1398](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1398))
    - Tags: [[pink]] 
- In business, opportunity cost is sometimes formalized as the opportunity cost of capital, the return you’d get on the best alternative use of that capital, your second-best opportunity. For example, suppose you’re now running your business and you are returning 5 percent to the bottom line for every dollar you spend on an ongoing advertising campaign. You’re now deciding the best way to reinvest some of these profits back into the business. Whatever you select, you ought to be sure that you are making back at least 5 percent on your investment, because you could easily make that amount by investing more into the ad campaign. Thinking in terms of opportunity cost of capital pits your investment options against each other. Thus, you can make an informed choice among the array of projects and opportunities available to you. Similarly, in negotiations there is another application of opportunity cost called BATNA, which stands for best alternative to a negotiated agreement. If you have a job offer, your BATNA is the best alternative job offer you have in hand, including your current job. You shouldn’t accept an offer worse than your BATNA, because you can always take this better alternative offer (which could be the status quo). In less clear-cut situations, it can be more challenging to understand your BATNA, and so it helps to brainstorm and literally list out all of your alternatives. This process can help you uncover additional alternatives that aren’t immediately apparent. In any case, going into a negotiation knowing your BATNA is critical to making a decision that you won’t regret. ([Location 1417](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1417))
    - Tags: [[pink]] 
- In all these financial situations, the small force is the amount of money you initially put up, allowing you to wield a much larger force through the greater sum of money you have available via the debt you take on. For example, individuals usually purchase homes with down payments much smaller than the total price. In the United States this is typically 20 percent, but in the run-up to the 2007/2008 financial crisis, people bought houses with as little as zero percent down! But by taking on debt, people get to live in the homes they want. In negotiation, leverage refers to the power one side has over another. If you have the ability to give or take more things than the other party, you have more leverage. No matter what the circumstances, small amounts of leverage can have large effects. As applied to individuals, certain activities or actions have much greater leverage than others, and spending time or money on these high-leverage activities will produce the greatest effects. Therefore, you should take time to continually identify high-leverage activities. It’s getting more bang for your buck. You can apply this model in all areas of your life. The highest-leverage choice might not be the best fit every time, but the option that provides the most impact at the lowest cost always warrants consideration. Which job will give you the best opportunity to advance your career? Which home renovations might most increase the value of your home in an upcoming sale or most increase its livability? Which activities will most help your kids in the future, or bring them the most joy? To which causes or charities would your cash contributions make the most difference (a mental model itself called effective altruism)? How much and what type of exercise do you need to do to get the most benefits in the least amount of time? Thinking about leverage helps you factor opportunity cost into your decision making. As a rule, the highest leverage activities have the lowest opportunity cost. The Pareto principle can help you find high-leverage activities. It states that in many situations, 80 percent of the results come from approximately 20 percent of the effort. Addressing this 20 percent is therefore a high-leverage activity. ([Location 1452](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1452))
    - Tags: [[pink]] 
- While every relationship is not always 80/20, there is a common pattern for outcomes to be far from evenly distributed. This particular 80/20 arrangement of outcomes is known as a power law distribution, where relatively few occurrences account for a significantly outsized proportion of the total. (It is named after mathematical exponentiation, aka power, because the math that creates the distribution involves this operation.) ([Location 1475](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1475))
    - Tags: [[pink]] 
- The same principle applies to whole organizations. If you are trying to reduce costs and 80 percent of the budget is from 20 percent of the items, it makes sense to spend time seeing what you can do to make reductions in that 20 percent (as in our previous discussion of the U.S. budget). Similarly, if 80 percent of your company’s sales come from 20 percent of its customers, you need to make sure these customers are satisfied, and find more like them. And if 80 percent of the usage of your website comes from 20 percent of the features, focus on those features. Incidentally, these are also the class of features that should go into an MVP (see Chapter 1). After you determine the 80/20 and address the low-hanging fruit, each additional hour of work will unfortunately produce less and less impactful results. In economics, this model is called the law of diminishing returns. It is the tendency for continued effort to diminish in effectiveness after a certain level of result has been achieved. ([Location 1488](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1488))
    - Tags: [[pink]] 
- There is a similar concept called the law of diminishing utility, which says that the value, or utility, of consuming an additional item is usually, after a certain point, less than the value of the previous one consumed. Consider the difference between the enjoyment you receive from eating one donut versus eating a second or third donut. By the time you get to a sixth donut, you may no longer get any enjoyment out of it, and you might even start getting sick. When continuing beyond a point like this can actually make things worse, you move from diminishing returns to negative returns. This can happen when you are striving for perfection and it becomes counterproductive. There are lots of phrases related to this concept— ([Location 1499](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1499))
    - Tags: [[pink]] 
- overdoing it, trying too hard, etc. (see the Too Much of a Good Thing section in Chapter 2). ([Location 1505](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1505))
    - Tags: [[pink]] 
- One reason why people procrastinate so much is present bias, which is the tendency to overvalue near-term rewards in the present over making incremental progress on long-term goals (see short-termism in Chapter 2). It’s really easy to find reasons on any given day to skip going to the gym (too much work, bad sleep, feeling sick/sore, etc.), but if you do this too often, you’ll never reach your long-term fitness goals. ([Location 1534](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1534))
    - Tags: [[pink]] 
- Everyone discounts the future as compared with the present to some degree. For instance, given a choice between getting $100 today and $100 in a year, most everyone would choose to get it today. Suppose, though, you’re offered $100 in a year, but you can pay a fee to get the $100 today (minus the fee). How much would you be willing to pay? Would you pay $20 to get the $100 right now (netting $80) versus getting $100 in a year? When you cast this fee as a percentage, it effectively becomes an “interest rate,” called the discount rate (in the example above, it would be 25 percent, since $80 × 125% = $100). Like any interest rate, it can compound, but instead of compounding positively as we discussed earlier, the discount rate compounds negatively. This negative compounding discounts payments out into the future more and more, since you won’t be able to access them until much later. The discount rate is the cornerstone of the discounted cash flow method of valuing assets, investments, and offers. This model can help you properly determine the worth of arrangements that involve future payments, such as investment properties, stocks, and bonds. For example, let’s say you win the lottery and are offered a choice between getting one million dollars each year, forever, or a lump-sum payment today. How high does that lump-sum payment need to be before you will accept it? You might think initially it should be exceptionally high because the payments go out forever; but because of the compounding discount rate, the expected earnings far in the future aren’t actually worth that much to you today. At the discount rate of 5 percent per year, for example, the million dollars in cash flow from next year would be discounted to only $952,381 of value today ($1M/1.05). Two years out, because of compounding, the million that year becomes just $907,029 of value today ($1M/1.052). This continues with earnings further out being discounted more and more until they get discounted closer and closer to zero in today’s dollars. Fifty years out, at the 5 percent discount rate, the million dollars that year is worth only $87,204 to you today ($1M/1.0550). When you add the discounted earnings together from all future years, you get the net present value, or NPV, of the lottery payments. In this case, the total comes to twenty million dollars. That is, if a 5 percent discount rate is appropriate, you would value this stream of cash flows of one million dollars a year forever at only twenty million dollars today, assuming you could get that twenty million dollars right now in the lump-sum payment. And in fact, around 5 percent is what is typically offered by lotteries. Of course, this method is very sensitive to the discount rate (e.g., 5 percent versus 20 percent). At a 20 percent discount rate applied yearly, the NPV of this cash flow stream becomes valued at $5 million in today’s dollars instead of $20 million at the 5 percent discount rate. ([Location 1542](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1542))
    - Tags: [[pink]] 
- The right discount rate to apply in a business and investing context is something we will explore a bit more in Chapter 6. Here, though, one thing to consider is what you could do with that money if you had it now. From a purely financial point of view, if you could guarantee investing at a rate greater than the discount rate, then you would be better off getting the lump-sum payment and investing it. For example, if you think you can invest at a 6 percent rate, then you’d be okay with a 5 percent discount rate. Lotteries usually offer rates around this 5 percent level for similar reasons (because they could invest at that rate). Of course, you wouldn’t consider just the financial point of view. If you had the lump-sum payment today, you might better enjoy your winnings because having more money now gives you more options in terms of spending. On the other hand, many actual lottery winners regret taking the lump-sum payment because they end up spending too much initially. ([Location 1592](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1592))
    - Tags: [[pink]] 
- In personal situations, most people discount the future implicitly at relatively high discount rates. And they do so in a manner that is not actually fixed over time, which is called hyperbolic discounting. In other words, people really, really value instant gratification over delayed gratification, and this preference plays a central role in procrastination, along with other areas of life where people struggle with self-control, such as dieting, addiction, etc. When you’re on a diet, it’s hard to avoid the pull of that donut in the office. That’s because you get the short-term donut payoff right now, whereas the long-term dieting payoff, being so far in the future, is discounted in your mind close to zero (like company earnings fifty years in the future). ([Location 1600](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1600))
    - Tags: [[pink]] 
- In studies, this preference is often revealed through asking people variations of the $100 question, finding points at which people are willing to get a lesser amount of money sooner rather than a greater amount later. One such study, “Some Empirical Evidence on Dynamic Inconsistency” by economist Richard Thaler, found that people on average were equally willing to receive $15 immediately, $30 after three months, $60 after one year, or $100 after three years. These values imply decreasing annual discount rates, declining from 277 percent to 139 percent to 63 percent as the delays get longer. Once you are old enough (like us) to have plenty of regrets about procrastination, you can more easily appreciate that your future self is going to have even greater struggles if you continue to put things off. You must strive to keep these feelings of regret in mind as motivation to stay focused on the long-term benefits of your actions, viewing your present efforts as incremental progress toward your goals. In that way you can attempt to counteract your inherent present bias and resulting procrastination tendencies. ([Location 1605](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1605))
    - Tags: [[pink]] 
- Once you overcome procrastination and are actually making consistent progress toward a goal, the next trap you can fall into is failing to plan your time effectively. Parkinson’s law (yes, another law by the same Parkinson of Parkinson’s law of triviality) states that “work expands so as to fill the time available for its completion.” Does that ring true for you? It certainly does for us. When your top priority has a deadline far in the future, it doesn’t mean that you need to spend all your time on it until the deadline. The sooner you finish, the sooner you can move on to the next item on your list. You also never know when finishing early might help you—for instance, when something important and urgent pops up in your Eisenhower Decision Matrix. A couple of whimsical models capture the feelings surrounding end-of-project work. In his book Gödel, Escher, Bach, cognitive scientist Douglas Hofstadter coined Hofstadter’s law: It always takes longer than you expect, even when you take into account Hofstadter’s Law. In other words, things take longer than you expect, even when you consider that they take longer than you expect! Tom Cargill was credited (in the September 1985 Communications of the ACM) for the similar ninety-ninety rule from his time programming at Bell Labs in the 1980s: The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time. Both concepts highlight the fact that you’re generally bad at estimating when things will get done, because, unless you put a lot of effort into continuous project planning, you don’t realize all the little things that need to be completed to really button up a project. This has certainly proved true in writing this book! The deeper point, however, is that you often have a choice of when to call the project “done.” This choice can dramatically affect the project’s time requirements, and periodically questioning what constitutes “done” can save you from wasted effort. In the case of the clinical study reports mentioned in the previous section, there could have been a step after each draft, comparing it with a predefined set of objectives for the project, and evaluating whether the group should move on. ([Location 1635](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1635))
    - Tags: [[pink]] 
- When it comes to losses in particular, you need to acknowledge that they’ve already happened: you’ve already spent the resources on the project to date. When you allow these irrecoverable costs to cloud your decision making, you are falling victim to the sunk-cost fallacy. The costs of the project so far, including your time spent, have already been sunk. You can’t get them back. This can be a problem (fallacy) when these previous losses influence you to make a bad decision. An instance where sunk costs lead to an escalation of commitment is sometimes called the Concorde fallacy, named after the supersonic jet whose development program was plagued with prohibitive cost overruns and never came close to making a profit. Ask yourself: Is my project like the Concorde? Am I throwing good money after bad when it is better to just walk away? ([Location 1678](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1678))
    - Tags: [[pink]] 
- The group questioned after they made their bets rated their horses’ chances significantly higher. This supported the scientists’ prediction that, post-bet, bettors were more confident in their choices. Evidently, simply the act of committing to the bet convinced bettors that their odds of winning had increased (see cognitive dissonance in Chapter 1). Remaining data-driven can help you avoid this mistake. The “power of positive thinking” can only get you so far. ([Location 1692](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1692))
    - Tags: [[pink]] 
- In Chapter 1, we discussed postmortems, where you analyze project failures so that you can do better next time. But you don’t have to wait until the end of a project—you can also conduct mid-mortems ([Location 1700](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1700))
    - Tags: [[pink]] 
- mortems, and occasionally even pre-mortems, where you predict ahead of time where things could go off track. In Chapter 1, we also discussed the third story, where you look at conflicts from an objective point of view. You need to use the same point of view when evaluating your own projects. If you recognize that you cannot do that, then bring someone else in to help you get out of your own way. ([Location 1701](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1701))
    - Tags: [[pink]] 
- However, some problems, such as large computational ones, can become intractable even with the help of sophisticated tools. For a password that is exactly eight characters long (letters or numbers, case sensitive), there are 218 trillion possible combinations—impossible to try by hand, and even extremely time-consuming for a computer. At 1,000 passwords a second, it would still take you 6,923 years to work through all those combinations. A better method than trying every combination at random might be first to try combinations of words from the dictionary, recognizing that people often choose words for passwords. An even better method would consider common passwords, and words or numbers significant to this particular person, such as related birth dates, sports teams, or initials. This is a type of heuristic solution, a trial-and-error solution that is not guaranteed to yield optimal or perfect results, but in many cases is nevertheless very effective. You should consider heuristics because they can be a shortcut to a solution for the problem in front of you, even if they may not work as well in other situations. If the problem persists, however, and you keep adding more heuristic rules, this type of solution can become unwieldy. That’s what has happened to Facebook with content moderation. The company started out with a simple set of heuristic rules (e.g., no nudity), and gradually added more and more rules (e.g., nudity in certain circumstances, such as breastfeeding, is okay), until as of April 2018 it had amassed twenty-seven pages of heuristics. ([Location 1736](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1736))
    - Tags: [[pink]] 
- As passwords have become harder to crack, hackers have reframed their problem of “How can we best guess your password?” to “How can we best get your password?” From this angle, a better solution, unfortunately, is social engineering, where you are manipulated to give up your password willingly. Hackers literally engineer a social situation designed to get you to give up your password. Think of phishing emails pretending to be from your accounts but which actually originate from hackers. These social-engineering techniques have been behind most high-profile targeted hacks, including celebrities’ iCloud photo accounts (2014), the release of emails from the Democratic National Committee and Hillary Clinton’s campaign chair John Podesta (2016), and a breach at the U.S. Department of Justice exposing the names, phone numbers, and email addresses of thousands of FBI and Department of Homeland Security employees (2016). From brute force to reframing the problem, the mental models in this section can all be used as tactical solutions to help you achieve project success faster. When you have a hard problem in front of you, take a moment to consider whether one or more of these models are applicable. A remark attributed to an anonymous woodsman is, “Give me six hours to chop down a tree and I will spend the first four sharpening the axe.” More broadly, how to spend your time wisely really comes down to working smarter. ([Location 1795](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1795))
    - Tags: [[pink]] 
- KEY TAKEAWAYS Choose activities to work on based on their relevance to your north star. Focus your time on just one of these truly important activities at a time (no multitasking!), making it the top idea on your mind. Select between options based on opportunity cost models. Use the Pareto principle to find the 80/20 in any activity and increase your leverage at every turn. Recognize when you’ve hit diminishing returns and avoid negative returns. Use commitment and the default effect to avoid present bias, and periodic evaluations to avoid loss aversion and the sunk-cost fallacy. Look for shortcuts via existing design patterns, tools, or clever algorithms. Consider whether you can reframe the problem. ([Location 1807](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1807))
    - Tags: [[pink]] 
- Just as species develop biological adaptations over generations, you need to be open to new ideas and paradigms, adjusting your thinking and behavior as necessary. Similarly, when organizations face disruption, they must find new ways to operate if they want to thrive. Professor Leon Megginson, paraphrasing Darwin, put it like this in a 1963 speech to the Southwestern Social Science Association: “It is not the most intellectual of the species that survives; it is not the strongest that survives; but ([Location 1841](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1841))
    - Tags: [[pink]] 
- the species that survives is the one that is able best to adapt and adjust to the changing environment in which it finds itself.” That is, you need to change color like the peppered moth did. ([Location 1845](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1845))
    - Tags: [[pink]] 
- Inertia is a physical object’s resistance to changing its current state of motion. The meme above illustrates this concept in practice. As a metaphor, inertia can describe any resistance to a change in direction. In Chapter 1, we explained how you tend to have significant inertia in your beliefs because of confirmation bias and related models. This adherence to your beliefs can hamper your adaptability. By questioning your assumptions, you can adapt to new ways of thinking and overcome this personal inertia. Inertia can increase the longer you hold on to your beliefs. If you’re like most people, many of your core political, religious, and social beliefs can be traced to the family and geographic culture in which you were raised as a child. Have you reevaluated these views recently? If not, you are likely clinging to many beliefs that are in conflict with other beliefs you came to hold later, or that you never properly questioned. The more inertia you have, the more resistant you will be to changing these beliefs, and the less likely you will be to adapt your thinking when you need to. ([Location 1866](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1866))
    - Tags: [[pink]] 
- Inertia in beliefs and behaviors allows entrenched ideas and organizations to persist for long periods of time. The Lindy effect is the name of this phenomenon. It was popularized by Nassim Taleb in his book Antifragile, which we mentioned in Chapter 1. Taleb explains: If a book has been in print for forty years, I can expect it to be in print for another forty years. But, and that is the main difference, if it survives another decade, then it will be expected to be in print another fifty years. This, simply, as a rule, tells you why things that have been around for a long time are not “aging” like persons, but “aging” in reverse. Every year that passes without extinction doubles the additional life expectancy. This is an indicator of some robustness. The robustness of an item is proportional to its life! The Lindy effect applies to technologies, ideas, organizations, and other nonperishable things. Assuming that the thing in question is not falling out of favor, the longer it endures, the longer you can expect it to further endure. The Lindy effect explains the continued relevance of Shakespeare and the Beatles. Since they show no signs of falling out of favor, the Lindy effect tells us we can expect Shakespeare plays to be performed for at least another four hundred years, and Beatles songs to be heard for at least another fifty. ([Location 1907](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1907))
    - Tags: [[pink]] 
- In an organizational context, establishing such beliefs and norms is the act of creating culture, which we will explore more fully in Chapter 8. Here, though, consider the relation of culture to inertia embodied in this saying: Culture eats strategy for breakfast. It is a warning that if you embark on a strategy that is in opposition to your organization’s culture, which has much more inertia than even its strategy, it is very unlikely to succeed. ([Location 1953](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1953))
    - Tags: [[pink]] 
- The good news is that if you can establish inertia and momentum in an adaptable culture, or in any context really, it can have staying power. A mental model that captures this process well is the flywheel, a rotating physical disk that is used to store energy. Flywheels are still used in many industrial applications, though a more relatable example to get the concept is a children’s merry-go-round. It takes a lot of effort to get a merry-go-round to start spinning, but once it is spinning, it takes little effort to keep it spinning. ([Location 1970](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1970))
    - Tags: [[pink]] 
- In his book Good to Great, Jim Collins relates many similar examples, using the flywheel metaphor to summarize how companies systematically and incrementally go from good to great. The flywheel image captures the overall feel of what it was like inside the companies as they went from good to great. No matter how dramatic the end result, the good-to-great transformations never happened in one fell swoop. There was no single defining action, no grand program, no one killer innovation, no solitary lucky break, no wrenching revolution. Good to great comes about by a cumulative process—step by step, action by action, decision, turn by turn of the flywheel—that adds up to sustained and spectacular results. ([Location 1982](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1982))
    - Tags: [[pink]] 
- First, from biology, there is homeostasis, which describes a situation in which an organism constantly regulates itself around a specific target, such as body temperature. When you get too cold, you shiver to warm up; when it’s too hot, you sweat to cool off. In both cases, your body is trying to revert to its normal temperature. That’s helpful, but the same effect also prevents change from the status quo when you want it to occur. ([Location 1998](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=1998))
    - Tags: [[pink]] 
- On a more mundane scale, within organizations or communities, people often naturally resist change, regularly responding with the mantra “If it ain’t broke, don’t fix it,” “Don’t rock the boat,” or “That’s how we do things around here.” This reaction can be rationalized and justified because change can be disruptive, even if the proposed end state has attractive benefits. ([Location 2004](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2004))
    - Tags: [[pink]] 
- Unfortunately, as a result of these types of homeostatic reactions, we stay in suboptimal arrangements for too long, from toxic personal relationships ([Location 2011](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2011))
    - Tags: [[pink]] 
- to poor organizational processes, all the way up to ineffective government policies. When you fight homeostasis—in yourself or in others—look out for the underlying mechanisms that are working against your efforts to make changes. A good example would be exercising more to lose weight only to have your increased exercise lead to an increased appetite. Anticipating this reaction, some people eat protein after working out to mitigate the homeostatic effect, because certain types of slow-digesting proteins help you feel full for longer. ([Location 2012](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2012))
    - Tags: [[pink]] 
- Critical mass as a super model applies to any system in which an accumulation can reach a threshold amount that causes a major change in the system. The point at which the system starts changing dramatically, rapidly gaining momentum, is often referred to as a tipping point. For example, a party needs to reach a critical mass of people before it feels like a party, and the arrival of the final person needed for the party to reach the critical number tips the party into high gear. Sometimes this point is also referred to as an inflection point, where the growth curve bends, or inflects. However, note that mathematically the inflection point actually refers to a different point on the curve, when it changes from concave to convex, or vice versa. ([Location 2079](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2079))
    - Tags: [[pink]] 
- Reaching a critical mass is a common proximate cause (see Chapter 1) of a tipping point. But the root cause of why a tipping point has been reached is often found in network effects, where the value of a network grows with each addition to it (the effect). Think of a social network—each person who joins makes the service more enticing because there are then more people to reach. ([Location 2117](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2117))
    - Tags: [[pink]] 
- The concept of a network is wider, however, encompassing any system where things (often referred to as nodes) can interact. For example, you need enough uranium atoms (“nodes”) in the “network” of a nuclear bomb such that when one decays, it can rapidly interact with another, instead of dissipating harmlessly. To use another example from everyday life, the telephone isn’t a useful device if there is no one else to call. But as each person gets a phone, the number of possible connections grows proportionally to the square of the number of phones (nodes). Two phones can make only one connection, five can make ten, and twelve can make sixty-six. Network Effects ([Location 2123](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2123))
    - Tags: [[pink]] 
- This relationship, known as Metcalfe’s law, is named after Robert Metcalfe, the co-inventor of the networking technology Ethernet. It describes the nonlinear growth in network value when nodes are connected to one another. His law oversimplifies reality since it assumes that every node (or telephone in this case) has the same value to the network and that every node may want to contact every other, but nevertheless it serves as a decent model. Having a million telephones on the phone network is much more than twice as valuable as having five hundred thousand. And knowing that everyone is connected is extremely valuable, which explains why Facebook has such a strong network effect. ([Location 2133](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2133))
    - Tags: [[pink]] 
- Many global systems, including the economy and weather, are known as chaotic systems. ([Location 2173](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2173))
    - Tags: [[pink]] 
- Mathematician Edward Lorenz is famous for studying such chaotic systems, pioneering a branch of mathematics called chaos theory. He introduced a metaphor known as the butterfly effect to explain the concept that chaotic systems are extremely sensitive to small perturbations or changes in initial conditions. He illustrated this concept by saying that the path of a tornado could be affected by a butterfly flapping its wings weeks before, sending air particles on a slightly different path than they would have otherwise traveled, which then gets amplified over time and ultimately results in a different path for the tornado. This metaphor has been popularized in many forms of entertainment, including by Jeff Goldblum’s character in the 1993 movie Jurassic Park and in the 2004 movie The Butterfly Effect, starring Ashton Kutcher. THE BUTTERFLY EFFECT The fact that you are surrounded by chaotic systems is a key reason why adaptability is so important to your success. While it is a good idea to plan ahead, you cannot accurately predict the circumstances you will face. No one plans to lose their spouse at a young age, or to graduate from college during an economic downturn. You must continuously adapt to what life throws at you. Unlike an air particle, though, you have free will and can actively navigate the world. This means you have the potential to increase the probability of a successful outcome for yourself. You can at least attempt to turn lemons into lemonade by using these chaotic systems to your advantage. For example, some studies show that businesses started during a recession actually do better over time, and research by the ([Location 2177](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2177))
    - Tags: [[pink]] 
- Kauffman Foundation, summarized in “The Economic Future Just Happened” in 2009, found that the majority of Fortune 500 companies were… ([Location 2191](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2191))
    - Tags: [[pink]] 
- One way to more systematically take advantage of the butterfly effect is using the super model of luck surface area, coined by entrepreneur Jason Roberts. You may recall from geometry that the surface area of an object is how much area the surface of an object covers. In the same way that it is a lot easier to catch a fish if you cast a wide net, your personal luck surface area will increase as you interact with more people in more diverse situations. If you want greater luck surface area, you need to relax your rules for how you engage with the world. For example, you might put yourself in more unfamiliar situations: instead of spending the bulk of your time in your house or office, you might socialize more or take a class. As a result, you will make your own luck by meeting more people and finding more opportunities. Thinking of the butterfly effect, you are increasing your chances of influencing a tornado, such as forming a new partnership that ultimately blossoms into a large, positive outcome. You obviously have to be judicious about which events to attend, or you will constantly be running to different places without getting any focused work done. However, saying no to everything also has a negative consequence—it reduces your luck surface area too much. A happy medium has you attending occasional events that expose you to people who can help you advance your goals. Say no often so you can say yes when you might make some new meaningful connections. Your luck surface area relates to the natural concept of entropy, which measures the amount of disorder in a system. In a clean room where there is a rule for where everything goes—socks in the sock drawer, shirts on hangers, etc.—… ([Location 2195](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2195))
    - Tags: [[pink]] 
- In this context, increasing your luck surface area means increasing your personal maximum entropy, by increasing the possible number of situations you put yourself in. Your life will be a bit less orderly, but disorder in moderation can be a good thing. Of course, as we have seen so far, too much of a good thing can also be a bad thing. Too much entropy is just chaos. We refer to our kids as entropy machines because they create disorder very quickly. They don’t follow rules for where their belongings go in their rooms, so the maximum possible entropy for their rooms is very high. Almost anything can go almost anywhere and ultimately their rooms can get pretty close to this maximum, resulting in a big mess. As entropy increases, things… ([Location 2211](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2211))
    - Tags: [[pink]] 
- In a closed system, like our kids’ rooms, entropy doesn’t just decrease on its own. Russian playwright Anton Chekhov put it like this: “Only entropy comes easy.” If our kids don’t make an effort to clean up, the room just gets messier and messier. The natural increase of entropy over time in a closed system is known as the second law of thermodynamics. Thermodynamics is the study of heat. If you consider our universe as the biggest closed system, this law leads to a plausible end state of our universe as a homogenous gas, evenly distributed everywhere, commonly known as the heat death of the universe. On a more practical level, the second law serves as a reminder that orderliness needs to be maintained, lest it be slowly chipped away by disorder. This natural progression is based on the reality that most orderliness doesn’t happen naturally. Broken eggs don’t spontaneously mend themselves. In boiling water, an ice cube melts and never re-forms as ice. If you take a puzzle apart and shake up the pieces, it isn’t going to miraculously put itself back together again. You must continually put energy back into systems to maintain their desired… ([Location 2218](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2218))
    - Tags: [[pink]] 
- to spend this limited resource wisely, such as by using the Eisenhower Decision Matrix. Seen through the lens of entropy, your time, if left unmanaged, will start to go to random, largely reactionary activities. You will get pulled into the chaotic systems that surround you. Instead, you need to manage your time so that it is in a state of lower entropy. When you are able to make time for important activities, you are more easily able to adapt to your changing environment because you have the ability to allocate time to a particular important activity when needed. To apply the Eisenhower Decision Matrix usefully, though, you need to assess properly what is an important activity and what is not. Given the butterfly effect and the fact that you must interact with chaotic systems like the economy, making these determinations can be challenging. This is especially true when deciding how and when to pursue new ideas, where an unexpected encounter can reveal new and important information. To make these determinations, you must therefore seek to understand and simplify chaotic systems like the economy so that you can successfully navigate them. All… ([Location 2228](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2228))
    - Tags: [[pink]] 
- Low-cost events High-cost events High-impact events Attend Maybe attend Low-impact events Maybe attend Ignore You can use this 2 × 2 matrix to help you categorize events as either high or low impact and high or low cost (time, money, etc.). You want to attend high-impact, low-cost events, and ignore low-impact, high-cost events. The other two quadrants are more nuanced. If there is a high-impact, high-cost event, such as a conference far from where you live, it may be worth going to depending on the specifics of the event and your particular situation: do you have the time and money to go? Similarly, if there is a low-impact event down the hall that will take only an hour of your time, it might be worth attending because the cost is so low. These 2 × 2 matrices draw on a concept from physics called polarity, which describes a feature that has only two possible values. A magnet has a north and south pole. An electric charge can be positive or negative. Polarity is useful because it helps you categorize things into one of two states:… ([Location 2241](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2241))
    - Tags: [[pink]] 
- While 2 × 2 matrices can be illuminating, they can also be misleading because most things don’t fall squarely into binary or even discrete states. Instead they fall along a continuum. For example, if you’re considering ways to make extra money across a set of possible activities, you don’t just want to know if you will make any money with each; you want to know how much, and how difficult it will be to generate new income from each activity. Winning the lottery will be significantly different from finding money on the ground or getting a part-time job. One simple way visually to introduce this type of complexity is through a scatter plot on top of a 2 × 2 matrix, which visualizes the relative values of what you are analyzing. While polarity can be useful, when making comparisons you… ([Location 2255](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2255))
    - Tags: [[pink]] 
- and white. Practically, whenever you are presented with a decision with two options,… ([Location 2263](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2263))
    - Tags: [[pink]] 
- If you study the biographies of successful people, you will notice a pattern: luck plays a significant role in success. However, if you look deeper, you will notice that most also had a broad luck surface area. Yes, they were in the right place at the right time, but they made the effort to be in a right place. If it wasn’t that particular place and time, there might have been another. Maybe it wouldn’t have resulted in the same degree of success, but they probably would have still been successful. ([Location 2289](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2289))
    - Tags: [[pink]] 
- Adopt an experimental mindset, looking for opportunities to run experiments and apply the scientific method wherever possible. Respect inertia: create or join healthy flywheels; avoid strategy taxes and trying to enact change in high-inertia situations unless you have a tactical advantage such as discovery of a catalyst and a lot of potential energy. ([Location 2302](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2302))
    - Tags: [[pink]] 
- When enacting change, think deeply about how to reach critical mass and how you will navigate the technology adoption life cycle. Use forcing functions to grease the wheels for change. Actively cultivate your luck surface area and put in work needed to not be subsumed by entropy. When faced with what appears to be a zero-sum or black-and-white situation, look for additional options and ultimately for a win-win. ([Location 2305](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2305))
    - Tags: [[pink]] 
- You may have heard anecdotes about people who happened to get cold and flu symptoms around the time that they got the flu vaccine and blame their illness on the vaccine. Just because two events happened in succession, or are correlated, doesn’t mean that the first actually caused the second. Statisticians use the phrase correlation does not imply causation to describe this fallacy. ([Location 2370](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2370))
    - Tags: [[pink]] 
- What is often overlooked when this fallacy arises is a confounding factor, a third, possibly non-obvious factor that influences both the assumed cause and the observed effect, confounding the ability to draw a correct conclusion. ([Location 2374](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2374))
    - Tags: [[pink]] 
- If you set out to collect or evaluate scientific evidence based on an experiment, the first step is to define or understand its hypothesis, the proposed explanation for the effect being studied (e.g., drinking Snapple can reduce the length of the common cold). Defining a hypothesis up front helps to avoid the Texas sharpshooter fallacy. This model is named after a joke about a person who comes upon a barn with targets drawn on the side and bullet holes in the middle of each target. He is amazed at the shooter’s accuracy, only to find that the targets were drawn around the bullet holes after the shots were fired. A similar concept is the moving target, where the goal of an experiment is changed to support a desired outcome after seeing the results. One method to consider, often referred to as the gold standard in experimental design, is the randomized controlled experiment, where participants are randomly assigned to two groups, and then results from the experimental group (who receive a treatment) are compared with the results from the control group (who do not). This setup isn’t limited to medicine; it can be used in fields such as advertising and product development. (We will walk through a detailed example in a later section.) A popular version of this experimental design is A/B testing, where user behavior is compared between version A (the experimental group) and version B (the control group) of a site or product, which may differ in page flow, wording, imagery, colors, etc. Such experiments must be carefully designed to isolate the one factor you are studying. The simplest way to do this is to change just one thing between the two groups. ([Location 2391](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2391))
    - Tags: [[pink]] 
- Interestingly, just the act of receiving something that you expect to have a positive effect can actually create one, called the placebo effect. ([Location 2416](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2416))
    - Tags: [[pink]] 
- Similarly, anticipation of side effects can also result in real negative effects, even with fake treatments, a phenomenon known as the nocebo effect. ([Location 2422](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2422))
    - Tags: [[pink]] 
- However, when the concept that researchers are interested in studying isn’t clearly observable or measurable, they must use a proxy endpoint (also called a surrogate endpoint or marker), a measure expected to be closely correlated to the endpoint they would measure if they could. A proxy essentially means a stand-in for something else. Other uses of this mental model include the proxy vote (e.g., absentee ballot) and proxy war (e.g., current conflicts in Yemen and Syria are a proxy war between Iran and Saudi Arabia). ([Location 2426](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2426))
    - Tags: [[pink]] 
- While there is no one objective measure of the quality of a university, every year U.S. News and World Report tries to rank schools against one another using a proxy metric that is a composite of objective measures, such as graduation rates and admission data, along with more subjective measures, such as academic reputation. Other examples of common proxy metrics include the body mass index (BMI), used to measure obesity, and IQ, used to measure intelligence. Proxy metrics are more prone to criticism because they are indirect measures, and all three of these examples have been criticized significantly. ([Location 2430](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2430))
    - Tags: [[pink]] 
- In the last section, we mentioned a few things to watch out for when reviewing or conducting experiments, such as observer-expectancy bias and confounding factors. There are a few more of these subtle concepts to be wary of. First, sometimes it is not ethical or practical to randomly assign people to different experimental groups. For example, if researchers wanted to study the effect of smoking during pregnancy, it wouldn’t be right to make nonsmoking pregnant women start smoking. The smokers in the study would therefore be those who selected to continue smoking, which can introduce a bias called selection bias. ([Location 2444](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2444))
    - Tags: [[pink]] 
- With selection bias, there is no guarantee that the study has isolated smoking to be the only difference between these groups. So if there is a difference detected at the end of the study, it cannot be easily determined how much smoking contributed to this difference. For instance, women who choose to continue smoking during their pregnancy against the advice of doctors may similarly make other medically questionable choices, which could drive adverse outcomes. Selection bias can also occur when a sample is selected that is not representative of the broader population of interest, as with online reviews. If the group studied isn’t representative, then the results may not be applicable overall. ([Location 2448](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2448))
    - Tags: [[pink]] 
- Another type of selection bias, common to surveys, is nonresponse bias, which occurs when a subset of people don’t participate in an experiment after they are selected for it, e.g., they fail to respond to the survey. If the reason for not responding is related to the topic of the survey, the results will end up biased. For instance, let’s suppose your company wants to understand whether it has a problem with employee motivation. Like many companies, you might choose to study this potential problem via an employee engagement survey. Employees missing the survey due to a scheduled vacation would be random and not likely to introduce bias, but employees not filling it out due to apathy would be nonrandom and would likely bias the results. That’s because the latter group is made up of disengaged employees, and by not participating, their disengagement is not being captured. Surveys like this also do not usually account for the opinions of former employees, which can create another bias in the results called survivorship bias. Unhappy employees may have chosen to leave the company, but you cannot capture their opinions when you survey only current employees. Results are therefore biased based on measuring just the population that survived, in this case the employees remaining at the company. Do these biases invalidate this survey methodology? Not necessarily. Almost every methodology has drawbacks, and bias of one form or another is often unavoidable. You should just be aware of all the potential issues in a study and consider them when drawing conclusions. For example, knowing about the survivorship bias in remaining employees, you could examine the data from exit interviews to see whether motivation issues were mentioned by departing employees. You could even try to survey them too. A few other examples can further illustrate how subtle survivorship bias can be. In World War II, naval researchers conducted a study of damaged aircraft that returned from missions, so that they could make suggestions as to how to bolster aircraft defenses for future missions. Looking at where these planes had been hit, they concluded that areas where they had taken the most damage should receive extra armor. However, statistician Abraham Wald noted that the study sampled only planes that had survived missions, and not the many planes that had been shot down. He therefore theorized the opposite conclusion, which turned out to be correct: that the areas with holes represented areas where aircraft could be shot and still return safely, whereas the areas without holes probably contained areas that, if hit, would cause the planes to go down. ([Location 2459](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2459))
    - Tags: [[pink]] 
- When you critically evaluate a study (or conduct one yourself), you need to ask yourself: Who is missing from the sample population? What could be making this sample population nonrandom relative to the underlying population? For example, if you want to grow your company’s customer base, you shouldn’t just sample existing customers; that sample doesn’t account for the probably much larger ([Location 2485](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2485))
    - Tags: [[pink]] 
- population of potential customers. This much larger potential customer base may behave very differently from your existing customer base (as is the case with early adopters versus the early majority, which we described in Chapter 4). One more type of bias that can be inadvertently introduced is response bias. While nonresponse bias is introduced when certain types of people do not respond, for those who do respond, various cognitive biases can cause them to deviate from accurate or truthful responses. For example, in the employee engagement survey, people may lie (by omission or otherwise) for fear of reprisal. ([Location 2487](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2487))
    - Tags: [[pink]] 
- In general, survey results can be influenced by response bias in a number of ways, including the following: ([Location 2493](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2493))
    - Tags: [[pink]] 
- How questions are worded, e.g., leading or loaded questions The order of questions, where earlier questions can influence later ones Poor or inaccurate memory of respondents Difficulty representing feelings in a number, such as one-to-ten ratings Respondents reporting things that reflect well on themselves It’s worth trying to account for all of these subtle biases (selection bias, nonresponse bias, response bias, survivorship bias), because after you do so, you can be even more sure of your conclusions. ([Location 2494](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2494))
    - Tags: [[pink]] 
- When you interpret data, you should watch out for a basic mistake that causes all sorts of trouble: overstating results from a sample that is too small. Even in a well-run experiment (like a political poll), you cannot expect to get a good estimate based on a small sample. This fallacy is sometimes referred to as the law of small numbers, and this section explores it in more detail. The name is derived from a valid statistical concept called the law of large numbers, which states that the larger the sample, the closer your average result is expected to be to the true average. ([Location 2500](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2500))
    - Tags: [[pink]] 
- First, consider the gambler’s fallacy, named after roulette players who believe that a streak of reds or blacks from a roulette wheel is more likely to end than to continue with the next spin. Suppose you see ten blacks in a row. Those who fall victim to this fallacy expect the next spin to have a higher chance of coming up red, when in fact the underlying probability of each spin hasn’t changed. For this fallacy to be true, there would have to be some kind of corrective force in the roulette wheel that is bringing the results closer to parity. That’s simply not the case. ([Location 2512](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2512))
    - Tags: [[pink]] 
- The improbable should not be confused with the impossible. If enough chances are taken, even rare events are expected to happen. Some people do win the lottery and some people do get struck by lightning. A one-in-a-million event happens quite frequently on a planet with seven billion people. ([Location 2534](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2534))
    - Tags: [[pink]] 
- Knowing the gambler’s fallacy, you shouldn’t always expect short-term results to match long-term expectations. The inverse is also true: you shouldn’t base long-term expectations on a small set of short-term results. ([Location 2539](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2539))
    - Tags: [[pink]] 
- But in most cases, the true cause is purely mathematical, explained through a model called regression to the mean. Mean is just another word for average, and regression to the mean explains why extreme events are usually followed by something more typical, regressing closer to the expected mean. ([Location 2544](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2544))
    - Tags: [[pink]] 
- That’s why a range of summary statistics and graphs are often used on a case-by-case basis ([Location 2566](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2566))
    - Tags: [[pink]] 
- to summarize data. The mean (average or expected value) measures central tendency, or where the values tend to be centered. Two other popular summary statistics that measure central tendency are the median (middle value that splits the data into two halves) and the mode (the most frequent result). These statistics help describe what a “typical” number might look like for a given set of data. For body temperature, though, just reporting the central tendency, such as the mean, can at times be too simplistic. This brings us to the second common set of summary statistics, those that measure dispersion, or how far the data is spread out. The simplest dispersion statistics report ranges. For body temperature, that could be specifying the range of values considered normal, e.g., minimum to maximum reported values from healthy people, as in the graph below (called a histogram). ([Location 2566](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2566))
    - Tags: [[pink]] 
- The graph on the previous page depicts the frequencies of 130 different body temperatures derived from a study of healthy adults. A histogram like this one is a simple way that you can summarize data visually: group the values into buckets, count how many data points are in each bucket, and make a vertical bar graph of the buckets. Before reporting a range, you might first look for outliers, those data points that don’t seem to fit with the rest of the data. These are the data points set apart in the histogram, such as the one at 100.8°F. Perhaps a sick person sneaked into the dataset. As a result, you might report a normal temperature range of 96.3°F to 100.0°F. Of course, with more data, you could produce a more accurate… ([Location 2575](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2575))
    - Tags: [[pink]] 
- is 98.25°F, the median is 98.3°F, and the mode is 98°F. In other scenarios, though, these three summary statistics may be quite different. To illustrate this, consider another histogram, below, showing the distribution of U.S. household income in 2016. This dataset also has one peak, at $20,000–$24,999, but it is asymmetric, skewing to the right. (All incomes above $200,000 are grouped into one bar; had this not been the case, the graph would have a long tail stretching much farther to the right.) Unlike for the body temperatures, the median income of $59,039 is very different from the mean income of… ([Location 2581](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2581))
    - Tags: [[pink]] 
- Also, a minimum–maximum range is less informative here. A better summary of the dispersion in this case might be an interquartile range specifying the 25th percentile to the 75th percentile of the data, which captures the middle 50 percent of incomes, from $27,300 to $102,350. The most common statistical measures of dispersion, though, are the variance and the standard deviation (the latter usually denoted by the Greek letter σ, sigma). They are both measures of how far… ([Location 2589](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2589))
    - Tags: [[pink]] 
- Because the standard deviation is just the square root of the variance, if you know one, then you can easily calculate the other. Higher values for each indicate that it is more common to see data points further from the mean, as shown in the targets below. ([Location 2609](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2609))
    - Tags: [[pink]] 
- measurements (e.g., heights, blood pressure, standardized tests). Histograms of these types of datasets have similar bell-curve shapes with a cluster of values in the middle, close to the mean, and fewer and fewer results as you go further away from the mean. When a set of data has this type of shape, it is often suggested that it comes from a normal distribution. The normal distribution is a special type of probability distribution, a mathematical function that describes how the probabilities for all possible outcomes of a random phenomenon are distributed. For example, if you take a random person’s temperature, getting any particular temperature has a certain probability, with the mean of 98.2°F being the most probable and values further away being less and less probable. Given that a probability distribution describes all the possible outcomes, all probabilities in a given distribution add up to 100 percent (or 1). To understand this better, let’s consider another example. As mentioned above, people’s heights also roughly follow a normal distribution. Below is a graphical representation of the distribution of men’s and women’s heights based on data from the U.S. Centers for Disease Control and Prevention. The distributions both have the typical bell-curve shape, even though the men’s and women’s heights have different means. Normal Distribution ([Location 2619](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2619))
    - Tags: [[pink]] 
- In normal distributions like these (and as we saw with the body temperatures), approximately 68 percent of all values should fall within one standard deviation of the mean, about 95 percent within two, and nearly all (99.7 percent) within three. In this manner, a normal distribution can be uniquely described by just its mean and standard deviation. Because so many phenomena can be described by the normal distribution, knowing these facts is particularly useful. Normal Distribution Standard Deviations ([Location 2630](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2630))
    - Tags: [[pink]] 
- So, if you stopped a random woman on the street, you could use these facts to form a likely guess for her height. A guess of around five feet four inches (162 centimeters) would be best, as that’s the mean. Additionally, you could, with about two-to-one odds, guess that she will have a height between five feet one inch and five feet seven inches. That’s because the standard deviation of women’s heights is slightly less than three inches, so about two-thirds of women’s heights will fall within that range (within one standard deviation of the mean). By contrast, women shorter than four feet ten inches or taller than five feet ten inches make up less than about 5 percent of all women (outside two standard deviations from the mean). ([Location 2636](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2636))
    - Tags: [[pink]] 
- There are many other common probability distributions besides the normal distribution that are useful across a variety of circumstances. A few are depicted in the figure on the previous page. We called this section “The Bell Curve,” however, because the normal distribution is especially useful due to one of the handiest results in all of statistics, called the central limit theorem. This theorem states that when numbers are drawn from the same distribution and then are averaged, this resulting average approximately follows a normal distribution. This is the case even if the numbers originally came from a completely different distribution. ([Location 2652](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2652))
    - Tags: [[pink]] 
- To appreciate what this theorem means and why it is so useful, consider the familiar opinion poll that determines an approval rating, such as for the U.S. Congress. Each person is asked whether they approve of Congress or not. That means the individual data points are each just a yes or a no. This type of data looks nothing like a normal distribution, as each data point can take only one of two possible values. Binary data like this is often analyzed using a different probability distribution, called the Bernoulli distribution, which represents the result of a single yes/no-type experiment or question, such as from a survey or poll. This distribution is useful in a wide variety of situations, such as analyzing advertising campaigns (whether someone purchased or not), clinical trials (responded to treatment or not), and A/B testing (clicked or not). The estimated approval rating is just an average of all of the different individual answers (1 for approval and 0 otherwise). For example, if 1,000 people were polled and 240 approved, then the approval rating would be 24.0 percent. The central limit theorem tells us that this statistical average (sample mean) is approximately normally distributed (assuming enough people participate in the survey). The figure on the next page illustrates how this works visually with the Bernoulli distribution and two others that also initially look nothing like the normal distribution. The middle column shows how the distribution of the sample mean from a Bernoulli distribution, made up of a series of ones and zeros, ends up looking like a bell curve. The first row depicts a distribution with a 75 percent ([Location 2657](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2657))
    - Tags: [[pink]] 
- chance of disapproval (the spike at 0 on the left) and a 25 percent chance of approval (the spike at 1 on the right). This 25 percent chance is based on the approval rating across the whole country, if you polled everyone. Each person in a poll comes from this population distribution. When you take a poll, you get only an estimate of the overall approval rating (like the 24 percent approval rating estimate mentioned earlier). When you do that, you are taking a sample from the entire population (e.g., asking one thousand people) and averaging the results to calculate the estimate. This sample mean has a distribution itself, called the sample distribution, which describes the chances of getting each possible approval rating from the sample. You can think of this distribution as the result of plotting the different approval ratings (sample means) obtained from many, many polls. Central Limit Theorem   Uniform distribution Bernouli distribution Exponential distribution Population distribution… ([Location 2668](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2668))
    - Tags: [[pink]] 
- The second row shows a plot of this sample distribution for an approval rating based on polling two randomly selected people. This plot looks different from the original distribution, but still nothing like a normal distribution, as it can have only three outcomes: two approvals (the spike at 1), two disapprovals (the spike at 0), or one approval and one disapproval (the spike at 0.5). If you base the polls on asking five people, the sample distribution starts to look a bit more like a bell shape with six possible outcomes (third row). With thirty people (thirty-one outcomes, depicted in the fourth row), it starts to look a lot like the characteristic bell-curve shape of the normal distribution. As you ask more and more people, the sample distribution becomes more and more like a normal distribution, with a mean of 25 percent, the true approval rating from the population distribution. Just as in the case of body temperatures or heights, while this mean is the most likely value obtained by the poll, values close to it are also likely, such as 24 percent. Values further and further away are less and less likely, with probabilities following the normal distribution. ([Location 2695](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2695))
    - Tags: [[pink]] 
- How much less likely, exactly? It depends on how many people you ask. The more people you ask, the tighter the distribution. To convey this information, polls like this usually report a margin of error. An article describing the poll results might include something like “Congress has a 24 percent approval rating with a margin of error of ±3 percent.” The “±3 percent” is the margin of error, but where this margin of error comes from or what it really means is rarely explained. With knowledge of the above mental models, you now can know! The margin of error is really a type of confidence interval, an estimated range of numbers that you think may include the true value of the parameter you are studying, e.g., the approval rating. This range has a corresponding confidence level, which quantifies the level of confidence you have that the true value of parameter is in the range you estimated. For example, a confidence level of 95 percent tells you that if you ran the poll many times and calculated many confidence intervals (one for each poll), on average 95 percent of them would include the true approval rating (i.e., 25 percent). Most media reports don’t mention the confidence level used to calculate their margin of error, but it is usually safe to assume they used 95 percent. Research publications, by contrast, are usually more explicit in stating what confidence levels they used to represent the uncertainty in their estimates (again typically, though not always, 95 percent). For the approval-rating scenario, the range is calculated using the fact that the central limit theorem tells us that ([Location 2703](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2703))
    - Tags: [[pink]] 
- the sample mean is approximately normally distributed, so we should expect 95 percent of possible values to be found within two standard deviations of the true mean (i.e., the real approval rating). The part that hasn’t been explained yet is that the standard deviation of this distribution, also called the standard error, is not the same as the sample standard deviation calculation from earlier. However, these two values are directly related. In particular, the standard error is the same as the standard deviation for the sample, divided by the square root of the sample size. This means that if you want to reduce the margin of error by a factor of two, you need to increase the sample size by a factor of four. For a yes/no poll like the approval rating, a margin of error of 10 percent is achieved with just 96 people, 5 percent at 384 people, 3 percent at 1,067 people, and 2 percent at 2,401. Since the margin of error is an expression of how confident the pollsters are in their estimate, it makes sense that it is directly related to the size of the sample group. The illustration on the next page shows how confidence intervals work for repeated experiments. It depicts one hundred 95 percent confidence intervals for the probability of flipping… ([Location 2715](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2715))
    - Tags: [[pink]] 
- Error bars are not always confidence intervals; they could be derived from other types of error calculations too. On an error bar, the dot in the middle is the parameter estimate, in this case the sample mean, and the lines at the end indicate the top and bottom of the range, in this case the confidence interval. The error bars in the plot vary due to what was seen in the different experiments, but they each span a range of about twenty percentage points, which corresponds to the ±10 percent mentioned above (with a sample size of one hundred flips). Given the 95 percent confidence level, you would expect ninety-five of these confidence intervals… ([Location 2728](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2728))
    - Tags: [[pink]] 
- Confidence intervals like these are often used as estimates of reasonable values for a parameter, such as the probability of getting heads. However, as you just saw, the true value of the parameter (in this case 50 percent) is sometimes outside a given confidence interval. The lesson here is, you should know that a confidence interval is not the definitive range for all possible values, and the true value is not guaranteed to be included in the interval. One thing that really bothers us is when statistics are reported in the media without error bars or confidence intervals. Always remember to look for them when reading reports and to include them in your own work. Without an error estimate,… ([Location 2734](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2734))
    - Tags: [[pink]] 
- As you saw in the last section, the average woman’s height is five feet four inches. If you had to guess the height of a random stranger, but you didn’t know for a fact that they were a woman, five feet four inches wouldn’t be a great guess because the average man is closer to five feet nine inches, and so something in the middle would be better. But if you had the additional information that the person was a… ([Location 2741](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2741))
    - Tags: [[pink]] 
- This is an example of a model called conditional probability, the probability of one thing happening under the condition that something else also happened. Conditional probability allows us to better estimate probabilities by using this additional information. Conditional probabilities are common in everyday life. For example, home insurance rates are tailored to the differing conditional probabilities of insurance claims (e.g., premiums are higher in coastal Florida, where hurricane damage is more likely, relative to where we live in Pennsylvania). Similarly, genetic testing can tell you if you are at higher risk for certain diseases; women with abnormal BRCA1 or BRCA2 genes have up to an 80 percent risk of developing breast cancer by age ninety. Conditional probability is denoted with a | symbol. For example, the probability (P) that you will get breast cancer by age ninety given that you are a woman with a BRCA mutation would be denoted as P(breast cancer by ninety | woman with BRCA mutation). Some people find conditional probabilities confusing. They mix up the probability that an event A will happen given a condition that event B happened—P(A|B)—with the probability that an event B will happen given the condition that event A happened—P(B|A).… ([Location 2745](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2745))
    - Tags: [[pink]] 
- other people develop breast cancer who do not have these mutations. Let’s walk through a longer example to see this fallacy in action. Suppose the police pull someone over at random at a drunk-driving checkpoint and administer a Breathalyzer test that indicates they are drunk. Further, suppose the test is wrong on average 5 percent of the time, saying that a sober person is drunk. What is the probability that this person is wrongly accused of drunk driving? Your first inclination might be to say 5 percent. However, you have been given the probability that the test says someone is drunk given they are sober, or P(Test=drunk | Person=sober) = 5 percent. But what you have been asked for is the probability that the person is sober given that the test says they are drunk, or P(Person=sober | Test=drunk). These are not the same probabilities! What you haven’t considered is how the results depend on the base rate of the percentage of drunk drivers. Consider the scenario where everyone makes the right decision, and no one ever drives drunk. In this case the probability that a person is sober is 100 percent, regardless of what the Breathalyzer test results say. When a probability calculation fails to account for the base rate (like the base… ([Location 2757](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2757))
    - Tags: [[pink]] 
- be wrong (the tests will be wrong 5 percent of the time), the police will most likely go through a lot of wrong tests before they find a person who was actually drunk-driving. In fact, if the police stop a thousand people, they would on average conduct nearly fifty wrong tests along their way to finding one actual drunk driver. So there is approximately only a 2 percent chance that a failed Breathalyzer in this scenario indicates that the person is actually drunk. Alternatively, this can be stated as a 98 percent chance that the person is sober. That’s way, way more than 5 percent! So, P(A|B) does not equal P(B|A), but how are they related? There is a very useful result in probability called Bayes’ theorem, which tells us the relationship between these two conditional probabilities. On the next… ([Location 2768](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2768))
    - Tags: [[pink]] 
- Now that you know about Bayes’ theorem, you should also know that there are two schools of thought in statistics, based on different ways to think about probability: Frequentist and Bayesian. Most studies you hear about in the news are based on frequentist statistics, which relies on and requires many observations of an event before it can make reliable statistical determinations. Frequentists view probability as fundamentally tied to the frequency of events. By observing the frequency of results over a large sample (e.g., asking a large number of people if they approve of Congress), frequentists estimate an unknown quantity. If there are very few data points, however, they can’t say much of anything, since the confidence intervals they can calculate will be extremely large. In their view, probability without observations makes no sense. Bayesians, by contrast, allow probabilistic judgments about any situation, regardless of whether any observations have yet occurred. To do this, Bayesians begin by bringing related evidence to statistical determinations. For example, picking a penny up off the street, you’d probably initially estimate a fifty-fifty chance that it would come up heads if you flipped it, even if you’d never observed a flip of that particular coin before. In Bayesian statistics, you can bring such knowledge of base rates to a problem. In frequentist statistics, you cannot. Many people find this Bayesian way of looking at probability more intuitive because it is similar to how your beliefs naturally evolve. In everyday life, you aren’t starting from scratch every time, as you would in frequentist statistics. For instance, on policy issues, your starting point is what you currently know on that topic—what Bayesians call a prior—and then when you get new data, you (hopefully) update your prior based on the new information. The same is true for relationships, with your starting point being your previous experiences with that person; a strong prior would be a lifelong relationship, whereas a weak prior would be just a first impression. You saw in the last section that frequentist statistics produce confidence intervals. These statistics tell you that if you ran an experiment many times (e.g., the one-hundred-coin-flips example we presented), the confidence intervals calculated should contain the parameter you are studying (e.g., 50 percent… ([Location 2779](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2779))
    - Tags: [[pink]] 
- whereas pragmatists (like us) use whichever methodology works best for the situation. And more commonly, remember not to confuse a conditional probability with its inverse: P(A|B) is not equal to P(B|A). You now know that these probabilities are related… ([Location 2810](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2810))
    - Tags: [[pink]] 
- RIGHT OR WRONG? So far you have learned that you shouldn’t base your decisions on anecdotes and that small samples cannot reliably tell you what will happen in larger populations. You might be wondering, then: How much data is enough data to be sure of my conclusions? Deciding the sample size, the total number of data points collected, is a balancing act. On one side, the more information you collect, the better your estimates will be, and the more sure you can be of your conclusions. On the other side is the fact that gathering more data takes more time and more money, and potentially puts more participants at risk. So, how do you know what the right sample size is? That’s what we cover in this section. Even with the best experimental design, sometimes you get a fluke result that leads you to draw the wrong conclusions. A higher sample size will give you more confidence that a positive result is not just a fluke and will also give you a greater chance of detecting a positive result. Consider a typical polling situation, such as measuring public support for an upcoming referendum, e.g., marijuana legalization. Suppose that the referendum ultimately fails, but the pollsters had randomly selected as their respondents people who were more in favor of it when compared with the whole population. This situation could result in a false positive: falsely giving a… ([Location 2813](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2813))
    - Tags: [[pink]] 
- These error models occur well beyond statistics, in any system where judgments are made. Your email spam filter is a good example. Recently our spam filters flagged an email with photos of our new niece as spam (false positive). And actual spam messages still occasionally make it through our spam filters (false negatives). ([Location 2838](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2838))
    - Tags: [[pink]] 
- In statistics, a false positive is also known as a type I error and a false negative is also called a type II error. When designing an experiment, scientists get to decide on the probability of each type of error they are willing to tolerate. The most common false positive rate chosen is 5 percent. (This rate is also denoted by the Greek letter α, alpha, which is equal to 100 minus the confidence level. This is why you typically see people say a confidence level of 95 percent.) That means that, on average, if your hypothesis is false, one in twenty experiments (5 percent) will get a false positive result. ([Location 2844](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2844))
    - Tags: [[pink]] 
- Regardless of the sample size of your experiment, you can always choose the false positive error rate. It doesn’t have to be 5 percent; you could choose 1 percent or even 0.1 percent. The catch is that, for a given sample size, when you do set such a low false positive rate, you increase your false negative error rate, possibly failing to detect a real result. This is where the sample size selection comes in. Once you set your false positive rate, you then determine what sample size you need in order to detect a real result with a high enough probability. This value, called the power of the experiment, is typically selected to be an 80 to 90 percent chance of detection, with a corresponding false negative error rate of 10 to 20 percent. (This rate is also denoted by the Greek letter β, beta, which is equal to 100 minus the power.) Researchers say their study is powered at 80 percent. ([Location 2849](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2849))
    - Tags: [[pink]] 
- Let’s consider an example to illustrate how all these models work together. Suppose a company wants to prove that its new sleep meditation app is working. Their background research shows that half of the time, the average person falls asleep within ten minutes. The app developers think that their app can improve this rate, helping more people fall asleep in less than ten minutes. The developers plan a study in a sleep lab to test their theory. The test group will use their app and the control group will just go to sleep without it. (A real study might have a slightly more complicated design, but this simple design will let us better explain the statistical models.) The statistical setup behind most experiments (including this one) starts with a hypothesis that there is no difference between the groups, called the null hypothesis. If the developers collect sufficient evidence to reject this hypothesis, then they will conclude that their app really does help people fall asleep faster. That is, the app developers plan to observe both groups and then calculate the percentage of people who fall asleep within ten minutes for each group. If they see enough of a difference between the two percentages, they will conclude that the results are not compatible with the null hypothesis, which would mean their app is likely really working. The developers also need to specify an alternative hypothesis, which describes the smallest meaningful change they think could occur between the two groups, e.g., 15 percent more people will fall asleep within ten minutes. This is the real result they want their study to confirm and have an 80 percent chance to detect (corresponding to a false negative rate of 20 percent). This alternative hypothesis is needed to determine the sample size. The smaller the difference in the alternative hypothesis, the more people will be needed to detect it. With the experimental setup described, a sample size of 268 participants is required. All of these models come together visually in the figure on the next page. First, look at the bell curves. (Due to the central limit theorem, we can assume that our differences will be approximately normally distributed.) The curve on the left is for the results under the null hypothesis: that there is no real difference between the two groups. That’s why this left bell curve is centered on 0 percent. Even so, some of the time they’d measure a higher or lower difference than zero due to random chance, with larger differences being less likely. That is, due to the underlying variability, even if the app has no real effect, they might still measure differences between the two groups because of the variable times it takes for people to fall asleep. ([Location 2868](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2868))
    - Tags: [[pink]] 
- The other bell curve (on the right) represents the alternative hypothesis that the app developers hope to be true: that there is a 15 percent increase in the percentage of people who fall asleep within ten minutes using the app as compared with people not using the app. Again, even if this hypothesis were true, due to variability, some of the time they’d still measure less than a 15 percent increase, and some of the time more than a 15 percent increase. That’s why the right bell curve is centered on 15 percent. Statistical Significance ([Location 2888](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2888))
    - Tags: [[pink]] 
- The dotted line represents the threshold for statistical significance. All values larger than this threshold (to the right) would result in rejection of the null hypothesis because differences this large are very unlikely to have occurred if the null hypothesis were true. In fact, they would occur with less than a 5 percent chance—the false positive rate initially set by the developers. The final measure commonly used to declare whether a result is statistically significant is called the p-value, which is formally defined as the probability of obtaining a result equal to or more extreme than what was observed, assuming the null hypothesis was true. Essentially, if the p-value is smaller than the selected false positive rate (5 percent), then you would say that the result is statistically significant. P-values are commonly used in study reports to communicate such significance. For example, a p-value of 0.01 would mean that a difference equal to or larger than the one observed would happen only 1 percent of the time if the app had no effect. This value corresponds to a value on the figure in the extreme tail of the left bell curve and close to the middle of the right bell curve. This placement indicates that the result is more consistent with the alternative hypothesis, that the app has an effect of 15 percent. Now, notice how these two curves overlap, showing that some differences between the two groups are consistent with both hypotheses (under both bell curves simultaneously). These gray areas show where the two types of error can occur. The light gray area is the false positive region and the dark gray area is the false negative region. A false positive would occur when a large difference is measured between the two groups (like one with a p-value of 0.01), but in reality, the app does nothing. This could happen if the no-app group randomly had trouble falling asleep and the app group randomly had an easy time. Alternatively, a false negative would occur when the app really does help people fall asleep faster, but the… ([Location 2894](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2894))
    - Tags: [[pink]] 
- If you want to reduce one of the error rates without increasing the other, you need to increase the sample size. When that happens, each of the bell curves becomes narrower (see the figure below, again as compared to the original). Statistical Significance Alpha: 5%   Beta: 12%   Sample size: 344 Increasing the sample size and narrowing the bell curves decreases the overlap between the two curves, shrinking the total gray area in the process. This is of course attractive because there is less chance of making… ([Location 2915](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2915))
    - Tags: [[pink]] 
- sample size (time, money, risk to participants, etc.). The table on the next page illustrates how sample size varies for different limits on the error rates for the sleep app study. You will see that if error rates are decreased, the sample size must be increased. The sample size values in the following table are all dependent on the selected alternative hypothesis of a 15 percent difference. The sample sizes would all further increase if the developers wanted to detect a smaller difference and would all decrease if they wanted to detect only a larger difference. Researchers often feel pressure to use a smaller sample size in order to save time and money, which can make choosing a larger difference for the alternative hypothesis appealing. But such a choice comes at a high… ([Location 2922](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2922))
    - Tags: [[pink]] 
- However, if the true difference the app makes is really only 15 percent, with this smaller sample size they will be able to detect this smaller difference only 32 percent of the time! That’s down from 80 percent originally and means that two-thirds of the time they’d get a false negative, failing to detect the 15 percent difference. As a result, ideally any experiment should be designed to detect the smallest meaningful difference. One final note on p-values and statistical significance: Most statisticians caution against overreliance on p-values in interpreting the results of a study. Failing to find a significant result (a sufficiently small p-value) is not the same as having confidence that there is no effect. The absence of evidence is not the evidence of absence. Similarly, even though the study may have achieved a low p-value, it might not be a replicable result, which we will explore in the final section. Statistical significance should not be confused with scientific, human, or economic significance. Even the most minuscule effects can be detected as statistically significant if the sample size is large enough. For example, with enough people in the sleep study, you could potentially detect a 1 percent difference between the two groups, but is that meaningful to any customers? No. Alternatively, more emphasis could be placed on the difference measured in a study along with its corresponding confidence interval. For the app study, while the customers want to know that they have better chances of falling asleep with the app than without, they ([Location 2955](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2955))
    - Tags: [[pink]] 
- also want to know how much better. The developers might even want to increase the sample size in order to be able to guarantee a certain margin of error in their estimates. Further, the American Statistical Association stressed in The American Statistician in 2016 that “scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.” Focusing too much on the p-value encourages black-and-white thinking and compresses the wealth of information that comes out of a study into just one number. Such a singular focus can make you overlook possible suboptimal choices in a study’s design (e.g., sample size) or biases that could have crept in (e.g., selection bias). ([Location 2966](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2966))
    - Tags: [[pink]] 
- Replication efforts are an attempt to distinguish between false positive and true positive results. Consider the chances of replication in each of these ([Location 2976](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2976))
    - Tags: [[pink]] 
- two groups. A false positive is expected to replicate—that is, a second false positive is expected to occur in a repetition of the study—only 5 percent of the time. On the other hand, a true positive is expected to replicate 80 to 90 percent of the time, depending on the power of the replication study. For the sake of argument, let’s assume this is 80 percent as we did in the last section. Using those numbers, a replication rate of 50 percent requires about 60 percent of the studies to have been true positives and 40 percent of them to have been false positives. To see this, consider 100 studies: If 60 were true positives, we would expect 48 of those to replicate (80 percent of 60). Of the remaining 40 false positives, 2 would replicate (5 percent of 40) for a total of 50. The replication rate would then be 50 per 100 studies, or 50 percent. Replication Crisis Re-test 100 Studies So, under this scenario, about a fourth of the failed replications (12 of 50) are explained by a lack of power in the replication efforts. These are real results that would likely be replicated successfully either if an additional replication study were done or if the original replication study had a higher sample size. The rest of the results that failed to replicate should have never been positive results in the first place. Many of these original studies probably underestimated their type I error rate, increasing their chances of being a false positive. That’s because when a study is designed for a 5 percent chance of a false positive, that chance applies only to one statistical test, but very rarely is only one statistical test conducted. The act of running additional tests to look for statistically significant results has many names, including data dredging, fishing, and p-hacking (trying to hack your data looking for small enough p-values). Often this is done with the best of intentions, as seeing data from an experiment can be illuminating, spurring a researcher to form new hypotheses. The temptation to test these additional hypotheses is strong, since the data needed to analyze them has already been collected. The trouble comes in, though, when a researcher overstates results that arise from these additional tests. The XKCD cartoon on this page illustrates how data dredging can play out: when no statistically significant relationship was found between jelly beans and acne, the scientists proceeded to dredge through twenty-one subgroups until one with a sufficiently low p-value was found, resulting in the headline “Green Jelly Beans Linked to Acne!” ([Location 2977](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=2977))
    - Tags: [[pink]] 
- Nevertheless, as a result of the replication crisis and the reasons that underlie it, you should be skeptical of any isolated study, especially when you don’t know how the data was gathered and analyzed. More broadly, when you interpret a claim, it is important to evaluate critically any data that backs up that claim: Is it from an isolated study or is there a body of research behind the claim? If so, how were the studies designed? Have all biases been accounted for in the designs and analyses? And so on. ([Location 3028](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3028))
    - Tags: [[pink]] 
- Whenever we are looking at the validity of a claim, we first look to see whether a thorough systematic review has been conducted, and if so, we start there. After all, systematic reviews and meta-analyses are commonly used by ([Location 3046](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3046))
    - Tags: [[pink]] 
- policy makers in decision making, e.g., in developing medical guidelines. If one thing is clear from this chapter, it’s probably that designing good experiments is tough! We hope you’ve also gathered that probability and statistics are useful tools for better understanding problems that involve uncertainty. However, as this section should also make clear, statistics is not a magical cure for uncertainty. As statistician Andrew Gelman suggested in The American Statistician in 2016, we must “move toward a greater acceptance of uncertainty and embracing of variation.” ([Location 3047](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3047))
    - Tags: [[pink]] 
- Avoid succumbing to the gambler’s fallacy or the base rate fallacy. Anecdotal evidence and correlations you see in data are good hypothesis generators, but correlation does not imply causation—you still need to rely on well-designed experiments to draw strong conclusions. Look for tried-and-true experimental designs, such as randomized controlled experiments or A/B testing, that show statistical significance. The normal distribution is particularly useful in experimental analysis due to the central limit theorem. Recall that in a normal distribution, about 68 percent of values fall within one standard deviation, and 95 percent within two. Any isolated experiment can result in a false positive or a false negative and can also be biased by myriad factors, most commonly selection bias, response bias, and survivorship bias. Replication increases confidence in results, so start by looking for a systematic review and/or meta-analysis when researching an area. Always keep in mind that when dealing with uncertainty, the values you see reported or calculate yourself are uncertain themselves, and that you should seek out and report values with error bars! ([Location 3061](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3061))
    - Tags: [[pink]] 
- IF YOU COULD KNOW HOW your decisions would turn out, decision making would be so easy! It is hard because you have to make decisions with imperfect information. ([Location 3073](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3073))
    - Tags: [[pink]] 
- How do you make sense of it all? The go-to framework for most people in situations like this is the pro-con list, where you list all the positive things that could happen if the decision was made (the pros), weighing them against the negative things that could happen (the cons). ([Location 3081](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3081))
    - Tags: [[pink]] 
- While useful in some simple cases, this basic pro-con methodology has significant shortcomings. First, the list presumes there are only two options, when as you just saw there are usually many more. Second, it presents all pros and cons as if they had equal weight. Third, a pro-con list treats each item independently, whereas these factors are often interrelated. A fourth problem is that since the pros are often more obvious than the cons, this disparity can lead to a grass-is-greener mentality, causing you mentally to accentuate the positives (e.g., greener grass) and overlook the negatives. ([Location 3085](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3085))
    - Tags: [[pink]] 
- This anecdote is meant to illustrate that it is inherently difficult to create a complete pro-con list when your experience is limited. Other mental models in this chapter will help you approach situations like these with more objectivity and skepticism, so you can uncover the complete picture faster and make sense of what to do about it. You’ve probably heard the phrase If all you have is a hammer, everything looks like a nail. This phrase is called Maslow’s hammer and is derived from this longer passage by psychologist Abraham Maslow in his 1966 book The Psychology of Science: I remember seeing an elaborate and complicated automatic washing machine for automobiles that did a beautiful job of washing them. But it could do only that, and everything else that got into its clutches was treated as if it were an automobile to be washed. I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. The hammer of decision-making models is the pro-con list; useful in some instances, but not the optimal tool for every decision. Luckily, there are other decision-making models to help you efficiently discover and evaluate your options and their consequences across a variety of situations. As some decisions are complex and consequential, they demand more complicated mental models. In simpler cases, applying these sophisticated models would be overkill. It is best, however, to be aware of the range of mental models available so that you can pick the right tool for any situation. ([Location 3098](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3098))
    - Tags: [[pink]] 
- One simple approach to improving the pro-con list is to add some numbers to it. Go through each of your pros and cons and put a score of −10 to 10 next to it, indicating how much that item is worth to you relative to the others (negatives for cons and positives for pros). When considering a new job, perhaps location is much more important to you than a salary adjustment? If so, location would get a higher score. Scoring in this way helps you overcome some of the pro-con list deficiencies. Now each item isn’t treated equally anymore. You can also group multiple items together into one score if they are interrelated. And you can now more easily compare multiple options: simply add up all the pros and cons for each option (e.g., job offers) and see which one comes out on top. This method is a simple type of cost-benefit analysis, a natural extension of the pro-con list that works well as a drop-in replacement in many situations. This powerful mental model helps you more systematically and quantitatively analyze the benefits (pros) and costs (cons) across an array of options. ([Location 3110](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3110))
    - Tags: [[pink]] 
- The first change when you get more sophisticated is that instead of putting relative scores next to each item (e.g., −10 to 10), you start by putting explicit dollar values next to them (e.g., −$100, +$5,000, etc.). Now when you add up the costs and benefits, you will end up with an estimate of that option’s worth to you in dollars. ([Location 3122](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3122))
    - Tags: [[pink]] 
- So far, you’ve moved from scoring to dollar values. Next, you graduate to a spreadsheet! Instead of a column of costs and a column of benefits, now you want to arrange the costs and benefits on a timeline. Give each item its own row, and each column in the timeline will now list the cost or benefit created by that item in a given year. So, the first column holds all the costs and benefits you expect this year (in year 0), the next column in year 1, then year 2, and so on. The row for a $2,000-per-month mortgage payment would look like −$24,000, −$24,000, −$24,000, for as many years as the life of the mortgage. The reason it is important to lay out the costs and benefits over time in this manner (in addition to increased clarity) is that benefits you get today are worth more than those same benefits later. There are three reasons for this that are important to appreciate, so please excuse the tangent; back to the cost-benefit analysis in a minute. First, if you receive money (or another benefit) today, you can use it immediately. This opens up opportunities for you that you wouldn’t otherwise have. For instance, you could invest those funds right now and be receiving a return on that money via a different investment, or you could use the funds for additional education, investing in yourself. (See opportunity cost of capital in Chapter 3.) Second, most economies have some level of inflation, which describes how, over time, prices tend to increase, or inflate. As a result, your money will have less purchasing power in the future than it does today. When we were younger, the standard price for a slice of pizza was one dollar; now a slice will run you upward of three dollars! That’s inflation. Because of inflation, if you get one hundred dollars ten years from now, you won’t be able to buy as much as you could if you had the same amount of money today. Consequently, you don’t want to regard an amount of money in ten years as the equivalent amount of money available today. Third, the future is uncertain, and so there is risk that your predicted benefits and costs will change. For instance, benefits that depend on currencies, stock markets, and interest rates will fluctuate in value, and the further you go into the future, the harder they are to predict. Now back to cost-benefit analysis. As you recall, you have a spreadsheet that lays out current and future costs and benefits across time. To account for the differences in value between current and future benefits, you use a mental model we introduced back in Chapter 3: the discount rate. ([Location 3145](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3145))
    - Tags: [[pink]] 
- A central challenge with cost-benefit analysis is that this end result is sensitive to the chosen discount rate. One way to show this sensitivity is through a sensitivity analysis, which is a useful method to analyze how sensitive a model is to its input parameters. Using the $50,000 bond example, let’s run a sensitivity analysis on the discount rate. To do so, you just vary the discount rate and calculate the net benefit for each variation. ([Location 3205](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3205))
    - Tags: [[pink]] 
- Notice how a seemingly small difference in the discount rate can represent a huge difference in the net benefit. That is, the net benefit is very sensitive to the discount rate. While the net benefit is positive at a 6 percent discount rate, it is three times more positive at 4 percent, and it is negative at 8 percent. That’s because at higher discount rates, the future benefit is discounted more. Eventually, it is discounted so much that the net benefit drops into negative territory. Running a sensitivity analysis like this can give you an idea of a range of net benefits you can expect under reasonable discount rates. You should similarly run a sensitivity analysis on any input parameter about which you are uncertain so that you can tell how much it influences the outcome. ([Location 3222](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3222))
    - Tags: [[pink]] 
- Governments typically use rates close to their interest rates, which normally move with inflation rates. Large corporations use sophisticated methods that account for their rates of borrowing money and the return on investment seen from previous projects, together resulting in a rate that is usually significantly higher than government interest rates. New businesses, which are highly speculative, should be using much higher discount rates still, since it costs them a lot to borrow money and they are often in a race against time before they run out of money or get eaten by competitors. Thus, the range of acceptable rates can vary widely, from close to the inflation rate all the way up to 50 percent or higher in an extremely high-risk/high-reward situation. ([Location 3237](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3237))
    - Tags: [[pink]] 
- One final issue with cost-benefit analysis to keep in mind is the trickiness of comparing two options that have different time horizons. To illustrate this trap, let’s compare our theoretical bond investment from earlier to another bond investment. Our bond investment from before cost $50,000 and returned $100,000 in ten years, which at a 6 percent discount rate resulted in a net benefit in today’s dollars of $5,839. Our new investment will also be a $50,000 bond investment, though instead of returning $100,000 in ten years, it pays back $75,000 in just six years. The cost today (year 0) for this second bond is again −$50,000. Using the same 6 percent discount rate, the $75,000 benefit six years from now discounted back to today’s dollars would be worth $52,872, for a net benefit of $2,872 ($52,872 − $50,000). This net benefit is less than the net benefit of the first bond investment opportunity of $5,839 and so it seems the first bond is a better investment. However, if you purchased the second bond, your $75,000 would be freed up after six years, leaving you four more years to invest that money in another way. If you were able to invest that money in a new investment at a high enough rate, this second bond is potentially more attractive in the end. When making a comparison, you therefore must consider what could happen over the same time frame. ([Location 3258](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3258))
    - Tags: [[pink]] 
- In other words, cost-benefit analysis is only as good as the numbers you put into it. In computer science, there is a model describing this phenomenon: garbage in, garbage out. If your estimates of costs and benefits are highly inaccurate, your timelines don’t line up, or your discount rate is poorly reasoned (garbage in), then your net result will be similarly flawed (garbage out). On the other hand, if you take great care to make accurate estimates and perform relevant sensitivity analyses, then cost-benefit analysis can be a first-rate model for framing a decision-making process, and is in most cases a desirable replacement for a pro-con list. Next time you make a pro-con list, at least consider the scoring method to turn it into a simple cost-benefit analysis. ([Location 3268](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3268))
    - Tags: [[pink]] 
- Luckily, there is another straightforward mental model you can use to make sense of all these potential outcomes: the decision tree. It’s a diagram that looks like a tree (drawn on its side), and helps you analyze decisions with uncertain outcomes. The branches (often denoted by squares) are decision points and the leaves represent different possible outcomes (often using open circles to denote chance points). A decision tree that represents this pool situation could look like the figure below. ([Location 3289](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3289))
    - Tags: [[pink]] 
- The first square represents your choice between the two contractors, and then the open circles further branch out to the different possible outcomes for each of those choices. The leaves with the closed circles list the resulting costs for each outcome, and their probabilities are listed on each line. (This is a simple probability distribution [see Chapter 5], which describes how all the probabilities are distributed across the possible outcomes. Each group of probabilities sums to 100 percent, representing all the possible outcomes for that choice.) You can now use your probability estimates to get an expected value for each contractor, by multiplying through each potential outcome’s probability with its cost, and then summing them all up. This resulting summed value is what you would expect to pay on average for each contractor, given all the potential outcomes. ([Location 3294](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3294))
    - Tags: [[pink]] 
- As applied to decision tree analysis, a conservative approach would be to increase your probability estimates of low-probability but highly impactful scenarios like the bankruptcy one. This revision would account for the fact that the scenario might represent a black swan event, and that you might therefore be wrong about its probability. One reason that the probability of black swan events may be miscalculated relates to the normal distribution (see Chapter 5), which is the bell-curve-shaped probability distribution that explains the frequency of many natural phenomena (e.g., people’s heights). In a normal distribution, rare events occur on the tails of the distribution (e.g., really tall or short people), far from the middle of the bell curve. Black swan events, though, often come from fat-tailed distributions, which literally have fatter tails, meaning that events way out from the middle have a much higher probability when compared with a normal distribution. Fat-Tailed Distribution ([Location 3365](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3365))
    - Tags: [[pink]] 
- There are many naturally occurring fat-tailed distributions as well, and sometimes people just incorrectly assume they are dealing with a normal distribution when in fact they are dealing with a distribution with a fatter tail, and that means that events in the tail occur with higher probability. In practice, these are distributions where some of the biggest outliers happen more often than you would expect from a normal distribution, such as occurs with insurance payouts, or in the U.S. income distribution (see the histogram in Chapter 5). Another reason why you might miscalculate the probability of a black swan event is that you misunderstand the reasons for its occurrence. This can happen when you think a situation should come from one distribution, but multiple are really involved. For example, there are genetic reasons (e.g., dwarfism and Marfan syndrome) why there might be many more shorter or taller people than you would expect from just a regular normal distribution, which doesn’t account for these rarer genetic variations. A third reason is that you may underestimate the possibility and impact of cascading failures (see Chapter 4). As you recall, in a cascading-failure scenario, parts of the system are correlated: if one part falters, the next part falters, and so on. The 2007/2008 financial crisis is an example, where the failure of mortgage-backed securities cascaded all the way to the banks and associated insurance companies. ([Location 3374](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3374))
    - Tags: [[pink]] 
- To better determine the outcome probabilities in highly complex systems like banking or climate, you may first have to take a step back and try to make sense of the whole system before you can even try to create a decision tree or cost-benefit analysis for a particular subset or situation. Systems thinking describes this act, when you attempt to ([Location 3389](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3389))
    - Tags: [[pink]] 
- think about the entire system at once. By thinking about the overall system, you are more likely to understand and account for subtle interactions between components that could otherwise lead to unintended consequences from your decisions. For example, when thinking about making an investment, you might start to appreciate how seemingly unrelated parts of the economy might affect its outcome. Some systems are fairly simple and you can picture the whole system in your head. Others are so complex that it is too challenging simultaneously to hold all the interlocking pieces in your head. One solution is literally to diagram the system visually. Drawing diagrams can help you get a better sense of complex systems and how the parts of the system interact with one another. Techniques for how to effectively diagram complex systems are beyond the scope of this book, but know that there are many techniques that you can learn, including causal loop diagrams (which showcase feedback loops in a system) and stock and flow diagrams (which showcase how things accumulate and flow in a system). Gabriel’s master’s thesis involved diagraming the email spam system. The picture on the next page is one of his causal loop diagrams—you aren’t meant to understand this diagram; it’s just an example of what these things can end up looking like. Just know now that it was really helpful in gaining a much better understanding of this complex system. ([Location 3391](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3391))
    - Tags: [[pink]] 
- A related mental model that also arises in dynamic systems and simulations is hysteresis, which describes how a system’s current state can be dependent on its history. Hysteresis is also a naturally occurring phenomenon, with examples across most scientific disciplines. In physics, when you magnetize a material in one direction, such as by holding a magnet to another piece of metal, the metal does not fully demagnetize after you remove the magnet. In biology, the T cells that help power your immune system, once activated, thereafter require a lower threshold to reactivate. Hysteresis describes how both the metal and the T cells partially remember their states, such that what happened previously can impact what will happen next. Again, this may already seem like a familiar concept, because it is similar to the mental model of path dependence (see Chapter 2), which more generally describes how choices have consequences in terms of limiting what you can do in the future. Hysteresis is one type of path dependence, as applied to systems. ([Location 3418](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3418))
    - Tags: [[pink]] 
- A particular type of simulation that can be especially useful in this way is a Monte Carlo simulation. ([Location 3433](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3433))
    - Tags: [[pink]] 
- A Monte Carlo simulation is actually many simulations run independently, with random initial conditions or other uses of random numbers within the simulation itself. By running a simulation of a system many times, you ([Location 3442](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3442))
    - Tags: [[pink]] 
- can begin to understand how probable different outcomes really are. Think of it as a dynamic sensitivity analysis. ([Location 3443](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3443))
    - Tags: [[pink]] 
- In 1955, psychologists Joseph Luft and Harrington Ingham originated the concept of unknown unknowns, which was made popular by former U.S. Secretary of Defense Donald Rumsfeld at a news briefing on February 12, 2002, with this exchange: Jim Miklaszewski: In regard to Iraq, weapons of mass destruction, and terrorists, is there any evidence to indicate that Iraq has attempted to or is willing to supply terrorists with weapons of mass destruction? Because there are reports that there is no evidence of a direct link between Baghdad and some of these terrorist organizations. Rumsfeld: Reports that say that something hasn’t happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don’t know we don’t know. And if one looks throughout the history of our country and other free countries, it is the latter category that tend to be the difficult ones. ([Location 3459](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3459))
    - Tags: [[pink]] 
- The context and evasiveness of the exchange aside, the underlying model is useful in decision making. When faced with a decision, you can use a handy 2 × 2 matrix (see Chapter 4) as a starting point to envision these four categories of things you know and don’t know. Knowns & Unknowns   Known Unknown Known What you know you know What you know you don’t know Unkown What you don’t know you know What you don’t know you don’t know This model is particularly effective when thinking more systematically about risks, such as risks to a project’s success. Each category deserves its own attention and process: ([Location 3468](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3468))
    - Tags: [[pink]] 
- As you can see, you enumerate items in each of the four categories, and then work to make them all known knowns. This model is about giving yourself more complete knowledge of a situation. It’s similar to systems thinking, from the last section, in that you are attempting to get a full picture of the system so you can make better decisions. ([Location 3493](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3493))
    - Tags: [[pink]] 
- A related model that can help you uncover unknown unknowns is scenario analysis (also known as scenario planning), which is a method for thinking about possible futures more deeply. It gets its name because it involves analyzing different scenarios that might unfold. That sounds simple enough, but it is deceptively complicated in practice. That’s because thinking up possible future scenarios is a really challenging exercise, and thinking through their likelihoods and consequences is even more so. ([Location 3501](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3501))
    - Tags: [[pink]] 
- Another technique for thinking more broadly about possible future scenarios is the thought experiment, literally an experiment that occurs just in your thoughts, i.e., not in the physical world. The most famous thought experiment is probably “Schrödinger’s cat,” named after Austrian physicist Erwin Schrödinger, who thought it up in 1935 to explore the implications of different interpretations of the physics of quantum mechanics. From his 1935 paper “The Present Situation in Quantum Mechanics”: A cat is penned up in a steel chamber, along with the following device (which must be secured against direct interference by the cat): in a Geiger counter, there is a tiny bit of radioactive substance, so small, that perhaps in the course of the hour one of the atoms decays, but also, with equal probability, perhaps none; if it happens, the counter tube discharges and through a relay releases a hammer that shatters a small flask of hydrocyanic acid. If one has left this entire system to itself for an hour, one would say that the cat still lives if meanwhile no atom has decayed. The first atomic decay would have poisoned it. So, you have a cat in a box, and if a radioactive atom decayed in the last hour, it would have killed the cat. This thought experiment poses some seemingly unanswerable questions: Until you observe the cat by opening the box, is it alive or dead, or in an in-between state, as certain interpretations of quantum mechanics would suggest? And what exactly happens when you open the box? Schrödinger’s Cat Thought Experiment Answers to this thought experiment are beyond the scope of this book and were argued over for decades after it was posed. Therein lies the power of the thought experiment. Thought experiments are particularly useful in scenario analysis. Posing questions that start with “What would ([Location 3514](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3514))
    - Tags: [[pink]] 
- happen if . . .” is a good practice in this way: What would happen if life expectancy jumped forty years? What would happen if a well-funded competitor copied our product? What would happen if I switched careers? These types of what-if questions can also be applied to the past, in what is called counterfactual thinking, which means thinking about the past by imagining that the past was different, counter to the facts of what actually occurred. You’ve probably seen this model in books and movies about scenarios such as what would have happened if Germany had won World War II (e.g., Philip K. Dick’s The Man in the High Castle). Examples from your own life can help you improve your decision making when you think through the possible consequences of your past decisions. What if I had taken that job? What if I had gone to that other school? What if I hadn’t done that side project? When reconsidering your past decisions, though, it is important not only to think of the positive consequences that might have occurred if you had made a different life choice. The butterfly effect (see Chapter 4) reminds us that one small change can have ripple effects, so when considering a counterfactual scenario, it is important to remember that if you change one thing, it is unlikely that everything else would stay the same. Posing what-if questions can nevertheless help you think more creatively, coming up with scenarios that diverge from your intuition. More generally, this technique is one of many associated with lateral thinking, a type of thinking that helps you move laterally from one idea to another, as opposed to critical thinking, which is more about judging an idea in front of you. Lateral thinking is thinking outside the box. ([Location 3531](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3531))
    - Tags: [[pink]] 
- Another way, easily enabled by the internet, is to crowdsource ideas, where you seek (source) ideas quite literally from anyone who would like to participate (the crowd). Crowdsourcing has been effective across a wide array of situations, from soliciting tips in journalism, to garnering contributions to Wikipedia, to solving the real-world problems of companies and governments. For example, Netflix held a contest in 2009 in which crowdsourced researchers beat Netflix’s own recommendation algorithms. Crowdsourcing can help you get a sense of what a wide array of people think about a topic, which can inform your future decision making, updating your prior beliefs (see Bayesian statistics in Chapter 5). It can also help you uncover unknown unknowns and unknown knowns as you get feedback from people with previous experiences you might not have had. In James Surowiecki’s book The Wisdom of Crowds, he examines situations where input from crowds can be particularly effective. It opens with a story about how the crowd at a county fair in 1906, attended by statistician Francis Galton, correctly guessed the weight of an ox. Almost eight hundred people participated, each individually guessing, and the average weight guessed was 1,197 pounds—exactly the weight of the ox, to the pound! While you cannot expect similar results in all situations, Surowiecki explains the key conditions in which you can expect good results from crowdsourcing: Diversity of opinion: Crowdsourcing works well when it draws on different people’s private information based on their individual knowledge and experiences. Independence: People need to be able to express their opinions without influence from others, avoiding groupthink. ([Location 3569](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3569))
    - Tags: [[pink]] 
- Aggregation: The entity doing the crowdsourcing needs to be able to combine the diverse opinions in such a way as to arrive at a collective decision. If you can design a system with these properties, then you can draw on the collective intelligence of the crowd. This allows you to glean the useful bits of information that might be hidden among a group of diverse participants. In the ox example, a butcher may notice something different than a farmer would and different yet than a vet would. All this knowledge was captured in the collective weight guessed. A more modern example of making use of collective intelligence would be an audience poll as done on the television show Who Wants to Be a Millionaire? In general, drawing on collective intelligence makes sense when the group’s collective pool of knowledge is greater than what you could otherwise get access to; this helps you arrive at a more intelligent decision than you would arrive at on your own. “The crowd” can help systematically think through various scenarios, get new data and ideas, or simply help improve existing ideas. One direct application of crowdsourcing to scenario analysis is the use of a prediction market, which is like a stock market for predictions. In a simple formulation of this concept, the price of each stock can range between $0 and $1 and represents the market’s current probability of an event taking place, such as whether a certain candidate will be elected. For example, a price of $0.59 would represent a 59 percent probability that the candidate would be elected. ([Location 3583](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3583))
    - Tags: [[pink]] 
- Another project, called the Good Judgment Project, crowdsources predictions for world events. Its co-creator, Philip E. Tetlock, studied thousands of participants and discovered superforecasters, people who make excellent forecasts, repeatedly. He found that these superforecasters consistently beat the world’s leading intelligence services in their predictions of world events, even though they lack classified intelligence that these services have access to! In a book entitled Superforecasting, Tetlock examines characteristics that lead superforecasters to make such accurate predictions. As it happens, these are good characteristics to cultivate in general: Intelligence: Brainpower is crucial, especially the ability to enter a new domain and get up to speed quickly. Domain expertise: While you can learn about a particular domain on the fly, the more you learn about it, the more it helps. Practice: Good forecasting is apparently a skill you can hone and get better at over time. Working in teams: Groups of people can outperform individuals as long as they avoid groupthink. Open-mindedness: People who are willing to challenge their beliefs tend to make better predictions. Training in past probabilities: People who looked at probabilities of similar situations in the past were better able to assess the current probability, avoiding the base rate fallacy (see Chapter 5). Taking time: The more time people took to make the prediction, the better they did. Revising predictions: Forecasters who continually revised their predictions based on new information successfully avoided confirmation bias (see Chapter 1). ([Location 3607](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3607))
    - Tags: [[pink]] 
- When tempted to use a pro-con list, consider upgrading to a cost-benefit analysis or decision tree as appropriate. When making any quantitative assessment, run a sensitivity analysis across inputs to uncover key drivers and appreciate where you may need to seek greater accuracy in your assumptions. Pay close attention to any discount rate used. Beware of black swan events and unknown unknowns. Use systems thinking and scenario analysis to more systematically uncover them and assess their impact. For really complex systems or decision spaces, consider simulations to help you better assess what may happen under different scenarios. Watch out for blind spots that arise from groupthink. Consider divergent and lateral thinking techniques when working with groups, including seeking more diverse points of view. Strive to understand the global optimum in any system and look for decisions that move you closer to it. ([Location 3640](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3640))
    - Tags: [[pink]] 
- IN ADVERSARIAL SITUATIONS, nearly every one of your choices directly or indirectly affects other people, and these effects can play a large role in how a conflict turns out. In the words of English poet John Donne, “No man is an island.” ([Location 3651](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3651))
    - Tags: [[pink]] 
- The phrase keeping up with the Joneses describes this phenomenon and comes from the name of a comic strip that followed the McGinis family, who were fixated on matching the lifestyle of their neighbors, the Joneses. ([Location 3673](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3673))
    - Tags: [[pink]] 
- As an individual, avoiding an arms race means not getting sucked into keeping up with the Joneses. You want to use your income on things that make you fulfilled (such as on family vacations or on classes that interest you), rather than on unfulfilling status symbols. As an organization, avoiding an arms race means differentiating yourself from the competition instead of pursuing a one-upmanship strategy on features or deals, which can eat away at your profit margins. By focusing on your unique value proposition, you can devote more resources to improving and communicating it rather than to keeping up with your competition. ([Location 3680](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3680))
    - Tags: [[pink]] 
- Game theory is the study of strategy and decision making in adversarial situations, and it provides several foundational mental models to help you think critically about conflict. Game in this context refers to a simplified version of a conflict, in which players engage in an artificial scenario with well-defined rules and quantifiable outcomes, much like a board game. In most familiar games—chess, poker, baseball, Monopoly, etc.—there are usually winners and losers. However, game theorists recognize that in real-life conflicts there isn’t always a clear winner or a clear loser. In fact, sometimes everyone playing the game can win and other times everyone can lose. The most famous “game” from game theory is called the prisoner’s dilemma. It can be used to illustrate useful game-theory concepts and can also be adapted to many life situations, including the arms race. Here’s the setup: Suppose two criminals are captured and put in jail, each in their own cell with no way to communicate. The prosecutor doesn’t have enough evidence to convict either one for a major crime but does have enough to convict both for minor infractions. However, if the prosecutor could get one of the prisoners to turn on their co-conspirator, the other one could be put away for the major crime. So the prosecutor offers each prisoner the same deal: the first one who betrays their partner walks free now, and anyone who stays silent goes to prison. In game theory, diagrams can help you study your options. One example is called a payoff matrix, showing the payoffs for possible player choices in matrix form (see 2 × 2 matrix in Chapter 4). From the prisoner’s perspective, the payoff matrix looks like this: Prisoner’s Dilemma Payoff Matrix: Sentences Received   B remains silent B betrays A A remains silent 1 year, 1 year 10 years, 0 years A betrays B 0 years, 10 years 5 years, 5 years Here’s where it gets interesting. The simplest formulation of this game assumes that the consequences for the players are only the prison sentences listed, i.e., there is no consideration of real-time negotiation or future retribution. If, as a player, you are acting independently and rationally, the dominant strategy given this formulation and payoff matrix is always to betray your partner: No matter what they do, you’re better off betraying, and that’s the only way to get off free. If your co-conspirator remains silent, you go from one to zero years by betraying them, and if they betray you too, you go from ten to five years. The rub is that if your co-conspirator follows the same strategy, you both go away for much longer than if you both just remained silent (five years versus one year). Hence the dilemma: do you risk their betrayal, or can you trust their solidarity and emerge with a small sentence? The dual betrayal with its dual five-year sentences is known as the Nash equilibrium of this game, named after ([Location 3689](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3689))
    - Tags: [[pink]] 
- mathematician John Nash, one of the pioneers of game theory and the subject of the biopic A Beautiful Mind. The Nash equilibrium is a set of player choices for which a change of strategy by any one player would worsen their outcome. In this case, the Nash equilibrium is the strategy of dual betrayals, because if either player instead chose to remain silent, that player would get a longer sentence. To both get a shorter sentence, they’d have to act cooperatively, coordinating their strategies. That coordinated strategy is unstable (i.e., not an equilibrium) because either player could then betray the other to better their outcome. In any game you play, you want to know whether there is a Nash equilibrium, as that is the most likely outcome unless something is done to change the parameters of the game. For example, the Nash equilibrium for an arms race is choosing a high arms strategy where both parties continue to arm themselves. Here’s an example of a payoff matrix for this scenario: Arms Race Payoff Matrix: Economic Outcomes   B disarms B arms A disarms win, win lose big, win big A arms win big, lose big lose, lose As you can see, the arms race directly parallels the prisoner’s dilemma. Both A and B arming (the lose-lose ([Location 3718](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3718))
    - Tags: [[pink]] 
- situation) is the Nash equilibrium, because if either party switched to disarming, they’d be worse off, enabling an even poorer outcome, such as an invasion they couldn’t defend against (denoted as “lose big”). The best outcome again results from being cooperative, with both parties agreeing to disarm (the win-win situation), thus opening up the opportunity to spend those resources more productively. That’s the arms race equivalent of remaining silent, but it is also an unstable situation, since either side could then better their situation by arming again (and potentially invading the other side, leading to a “win big” outcome). In both scenarios, a superior outcome is much more likely if everyone involved does not consider the situation as just one turn of the game but, rather, if both sides can continually take turns, running the same game over and over—called an iterated or repeated game. When we mentioned earlier the possibility of future retribution, this is what we were talking about. What if you have to play the game with the same people again and again? In an iterated game of prisoner’s dilemma, cooperating in a tit-for-tat approach usually results in better long-term outcomes than constant betrayal. You can start out… ([Location 3735](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3735))
    - Tags: [[pink]] 
- Similarly, cooperation pays off in most long-term life situations where reputation matters. If you are known as a betrayer, people will not want to be your friend or do business with you. On the other hand, if people can trust you based on your repeated good behavior, they will want to make you their ally and collaborate with you. In any case, analyzing conflicts from a game-theory perspective is a sound approach to help you understand how your situation is likely to play out. You can write out the payoff matrix and use a decision tree (see Chapter 6) to… ([Location 3746](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3746))
    - Tags: [[pink]] 
- First, consider six powerful yet subtle influence models that psychologist Robert Cialdini presents in his book Influence… ([Location 3754](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3754))
    - Tags: [[pink]] 
- The mental model this study illustrates is called reciprocity, whereby you tend to feel an obligation to return (or reciprocate) a favor,… ([Location 3758](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3758))
    - Tags: [[pink]] 
- The second model that Cialdini describes is commitment—if you agree (or commit) to something, however small, you are more likely to continue to agree later. That’s because not being consistent causes psychological discomfort, called cognitive dissonance (see Chapter 1). ([Location 3772](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3772))
    - Tags: [[pink]] 
- Salespeople will also try to find common ground through a model Cialdini calls liking. Quite simply, you ([Location 3777](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3777))
    - Tags: [[pink]] 
- are more prone to take advice from people you like, and you tend to like people who share characteristics with you. ([Location 3778](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3778))
    - Tags: [[pink]] 
- A fourth influence model is known as social proof, drawing on social cues as proof that you are making a good decision. You are more likely to do things that you see other people doing, because of your instinct to want to be part of the group (see in-group favoritism in Chapter 4). Think of fashion and food trends or “trending” stories and memes online. ([Location 3790](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3790))
    - Tags: [[pink]] 
- Scarcity is another influence model, this one describing how you become more interested in opportunities the less available they are, triggering your fear of missing out (FOMO). So-called “limited-time offers” and “once-in-a-lifetime opportunities” prey on this fear. These are easy to spot online, such as the travel site that says there are “only 3 rooms left at this price,” or the retailer reporting “only 5 left in stock.” Scarcity signals also often imply social proof, e.g., this shirt is going to run out because it is so popular. ([Location 3808](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3808))
    - Tags: [[pink]] 
- Cialdini’s sixth major influence model is authority, which describes how you are inclined to follow perceived authority figures. ([Location 3814](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3814))
    - Tags: [[pink]] 
- Authority also explains why simple changes in wardrobe and accessories can increase the likelihood of getting you to do something. For instance, lab coats were worn in the Milgram experiments to convey authority. ([Location 3825](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3825))
    - Tags: [[pink]] 
- Cialdini’s influence models can be used in many situations, including in adversarial ones where you are trying to persuade others to make certain choices. If you want a crash course on the use of these mental models in real-life just go to a casino, where all of them are used simultaneously to ultimately take your money. Casinos give away a lot of free stuff (reciprocity); they get you to first buy chips with cash (commitment); they try to personalize your experience to your interests (liking); they show you examples of other people who won big (social proof); they constantly present you with offers preying on your fear of missing out (scarcity); and dealers will even give you suboptimal advice (authority). Beware. There is a reason why the house always wins! ([Location 3834](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3834))
    - Tags: [[pink]] 
- In conflicts, you may similarly get the outcome you want by winning people over to your point of view. Thomas Paine did this masterfully by building allies when a conflict was unavoidable. Sometimes you may use framing in this way to prevent a direct fight altogether. There are some more subtle aspects of framing to consider, captured in a few mental models that we explore in the rest of this section. Let’s think about a more mundane situation than the American Revolution: getting a babysitter. While mid-career professionals are unlikely to take up babysitting for extra cash, they are likely to babysit for free when a friend is in a pinch. The first scenario is framed from a market perspective (“Would you babysit my kids for fifteen dollars an hour?”) and the second is framed from a social perspective (“Can you please do me a favor?”). The difference in the way this situation is framed can be thought of as social norms versus market norms and draws on the concept of reciprocity from the previous section. When you consider something from a market perspective (like babysitting for money), you consider it in the context of your own financial situation and its impact on you in an impersonal way (“I can earn sixty dollars, but it may not be worth my time”). In contrast, when you consider something from the social perspective (like doing your friend a favor), you consider it in the context of whether it is the right thing to do (“My friend needs my help for four hours, so I am going to help her”). In his book Predictably Irrational, economist Dan Ariely offers another illustrative example, of an Israeli daycare center trying to address the problem of parents showing up late to pick up their kids. As the problem became prevalent, the daycare instituted a fine for showing up late. In spite of the fine, this policy actually resulted in more late pickups. Ariely explains: Before the fine was introduced, the teachers and parents had a social contract, with social norms about being late. Thus, if parents were late—as they occasionally were—they felt guilty about it—and their guilt compelled them to be more prompt in picking up their kids in the future. . . . But once the fine was imposed, the day care center had inadvertently replaced the social norms with market norms. Now that the parents were paying for their tardiness, they interpreted the situation in terms of market norms. In other words, since they were being fined, they could decide for themselves whether to be late or not, and they frequently chose to be late. Needless to say, this was not what the day care center intended. But the real story only started here. The most interesting part occurred a few weeks later, when the day care center removed the fine. Now the center was back to the social norm. Would the parents also return to the social norm? Would their guilt return as well? Not at all. Once the fine was removed, the behavior of the parents didn’t change. They continued to pick up their kids… ([Location 3867](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3867))
    - Tags: [[pink]] 
- You must be careful not to inadvertently replace social norms with market norms, because you may end up eliminating benefits that are hard to bring back (see irreversible decisions in Chapter 2). Once social norms are undermined, the damage has been done and they are no longer norms. So take pause when you’re thinking about introducing monetary incentives into a situation where social norms are the standard. Social Norms vs. Market Norms Social Norms No money involved No instant payback Community situations Market Norms Money involved Transactional Business situations ([Location 3891](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3891))
    - Tags: [[pink]] 
- As Martin Luther King Jr. put it in a May 8, 1967, interview with NBC News: “It’s all right to tell a man to lift himself by his own bootstraps, but it is a cruel jest to say to a bootless man that he ought to lift himself by his own bootstraps.” ([Location 3924](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3924))
    - Tags: [[pink]] 
- When you are in a conflict, you should consider how its framing is shaping the perception of it by you and others. Take the prisoner’s dilemma. The prosecutors have chosen to frame the situation competitively because, for them, the Nash equilibrium with both criminals getting five years is actually the preferred outcome. However, if the criminals can instead frame the situation cooperatively—stick together at all costs—they can vastly improve their outcome. ([Location 3951](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3951))
    - Tags: [[pink]] 
- You’re probably familiar with the mythical tale of the Trojan horse, a large wooden horse made by the Greeks to win a war against the Trojans. The Greeks couldn’t get into the city of Troy, and so they pretended to sail away, leaving behind this supposed parting gift. What the Trojans didn’t know is that the Greeks also left a small force of soldiers inside the horse. The Trojans brought the horse into the city, and under the cover of night, the Greek soldiers exited the horse and proceeded to destroy Troy and win the war. A Trojan horse can refer to anything that persuades you to lower your defenses by seeming harmless or even attractive, like a gift. It often takes the form of a bait and switch, such as a malicious computer program that poses as an innocuous and enticing download (the bait), but instead does something nefarious, like spying on you (the switch). ([Location 3962](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=3962))
    - Tags: [[pink]] 
- Do the ends justify the means? Only you can decide where the line is for you. ([Location 4001](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4001))
    - Tags: [[pink]] 
- THE ONLY WINNING MOVE IS NOT TO PLAY Considering a conflict through a game-theory lens helps you identify what you have to gain and what you have to lose. We have just looked at models that increase your chances of a good outcome through influencing other players. Now we will consider the same problem from the inverse (see inverse thinking in Chapter 1) and explore models that decrease your chances of a bad outcome. Often this means finding a way to avoid the conflict altogether. ([Location 4002](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4002))
    - Tags: [[pink]] 
- The reason that there is no winner in Global Thermonuclear War is that both sides have amassed enough weapons to destroy the other side and so any nuclear conflict would quickly escalate to mutually assured destruction (MAD). As a result, neither side has any incentive to use its weapons offensively or to disarm completely, leading to a stable, albeit tense, peace. Mutually assured destruction isn’t just a military model. A parallel in business is when companies amass large patent portfolios, but generally don’t use them on one another for fear of escalating lawsuits that could potentially destabilize all the companies involved. Occasionally you see these suits and countersuits, such as the ones between Apple and Qualcomm (over chip patents), Oracle and Google (over Java patents), and Uber and Google (over autonomous vehicle patents), but these companies often have so many patents (sometimes tens of thousands each) that there could be literally hundreds of suits like these if not for MAD. There are countless possible destructive outcomes to a conflict besides this arguably most extreme outcome of MAD. Engaging in any direct conflict is dangerous, though, because conflicts are unpredictable and often cause collateral damage (see Chapter 2). For example, drawn-out divorce battles can be harmful to the children. That’s why it makes sense to consider conflict prevention measures like mediation, or, more generally, diplomacy (see win-win in Chapter 4 for some related mental models). ([Location 4021](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4021))
    - Tags: [[pink]] 
- A related tactic is flypaper theory, which calls for you to deliberately attract enemies to one location where they are more vulnerable, like attracting flies to flypaper, usually also directing them far away from your valuable assets. A former commander of U.S. ground forces in Iraq, General Ricardo Sánchez, described the benefits of this strategy in a 2003 CNN interview with regard to preventing terrorism on U.S. soil: “This is what I would call a terrorist magnet, where America, being present here in Iraq, creates a target of opportunity. . . . But this is exactly where we want to fight them. . . . This will prevent the American people from having to go through their attacks back in the United States.” In a computing context, this is known as a honeypot, which is used to attract and trap malicious actors for study, in the same way honey lures bears. A honeypot may be a special set of servers set up to look like the core servers where valuable data is stored, but which instead are isolated servers set up specifically to entrap hackers. A sting operation by police where they lure criminals into a place to arrest them could be called an offline honeypot. ([Location 4080](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4080))
    - Tags: [[pink]] 
- Finally, as Joshua said in WarGames, sometimes “the only winning move is not to play.” An increasingly common example is conflict with the online troll, someone whose whole game is to irritate people and bait them into arguments they can’t win. As a result, the best move is usually not to engage with them (don’t feed the trolls; don’t stoop to their level; rise above the fray), though, as in any situation, you have to assess it on a case-by-case basis, and where reporting mechanisms exist, you should consider them too. Any parent will similarly tell you that you need to pick your battles. ([Location 4137](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4137))
    - Tags: [[pink]] 
- Because a war of attrition is a long-term strategy, it can counterintuitively make sense to lose a battle intentionally, or even many battles, to win the eventual war. The winner of such a battle gets a hollow victory, sometimes referred to as an empty victory or Pyrrhic victory. The latter is named after King Pyrrhus of Epirus, Greece, whose army suffered irreplaceable casualties in defeating the Romans at the Battle of Heraclea, and then ultimately lost the war. In sports and gaming, this scenario is known as a sacrifice play. Examples include bunts and sacrifice flies in baseball and intentionally giving up a piece to get better board position in chess. ([Location 4165](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4165))
    - Tags: [[pink]] 
- One adage to keep in mind when you find yourself in a guerrilla warfare situation is that generals always fight the last war, meaning that armies by default use strategies, tactics, and technology that worked for them in the past, or in their last war. The problem is that what was most useful for the last war may not be best for the next one, as the British experienced during the American Revolution. The most effective strategies, tactics, and especially technologies change over time. If your opponent is using outdated tactics and you are using more modern, useful ones, then you can come out the victor even with a much smaller force. Essentially, you use your tactical advantage to change the game without their realizing it; they think they are still winning a war of attrition. ([Location 4187](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4187))
    - Tags: [[pink]] 
- Decisive battles have been won on the back of superior technology like this many times in military history. Don’t bring a knife to a gunfight. This concept is far-reaching, describing any situation where circumstances have changed significantly, leaving the status quo unequipped to deal with new threats. ([Location 4200](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4200))
    - Tags: [[pink]] 
- Employing guerrilla warfare is an example of punching above your weight. In boxing, competitors are grouped by weight, because large differences in weight, all other things being equal, make a fight unfair. This takes us back to the physics models we discussed in Chapter 4 (see inertia). Heavier boxers pack more powerful punches and are generally harder to knock over. A boxer who punches above their weight intentionally fights in a heavier class, taking on larger competitors on purpose. As a mental model, punching above your weight occurs any time you try to perform at a higher level than is expected of you, even outside a competitive context. Examples include joining a group made up of more accomplished members or writing an op-ed on a subject on which you are not yet a recognized expert. On the macro scale, whole countries punch above their weight when they engage in prominent roles on the world stage, such as Ireland serving as a tax haven for major corporations. Given the inherent disadvantage of being the smaller player, you should engage in this type of fight only when you can deploy guerrilla tactics that you believe will tilt the game in your favor. If that’s the case, though, you may actively seek out these types of conflicts because punching above your weight can have many benefits. These include the obvious benefit of increasing your chances to reach your goals faster, but also potential exposure to large audiences and opportunities to absorb knowledge from world-class experts. However, following the metaphor can also get you punched hard in the face, so it is inherently risky. It’s like when a new TV show gets marketed to the mainstream but does not retain sufficient viewership and gets quickly canceled—not ready for prime time. When deploying such tactics, you will want to reevaluate your odds as the game goes on, to make sure you are on the right track. Are your odds improving? Are you effectively changing the game to be in your favor? ([Location 4211](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4211))
    - Tags: [[pink]] 
- If a solid exit strategy isn’t forthcoming, one tactic is to throw a Hail Mary pass, a last-ditch, long-shot final effort for a successful outcome. ([Location 4246](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4246))
    - Tags: [[pink]] 
- Spanish explorer Hernán Cortés made a counterintuitive Hail Mary pass by actually eliminating his expedition’s default exit strategy. In 1519, Cortés started a war with the Aztecs that led to the destruction of their empire. However, he had only six hundred men, whereas the Aztecs controlled most of modern-day Mexico. The odds were obviously thought to be heavily against the Spanish, and many of Cortés’s soldiers were reasonably wary of his plans. To secure their motivation, Cortés sank his ships to make sure they had no option but to succeed or die. Without the escape hatch of going back to Spain on the boats, the soldiers’ best option was to fight with Cortés. Translation errors led some to believe he burned the boats, but now we know he just had them damaged to the point of sinking. Nonetheless, burn the boats lives on as a mental model for crossing the point of no return. (Sometimes people also say crossing the Rubicon, referencing Julius Caesar’s crossing of the Rubicon River with his troops in 49 B.C., deliberately breaking Roman law, making armed conflict inevitable and ultimately leading to him becoming dictator of Rome.) ([Location 4250](https://readwise.io/to_kindle?action=open&asin=B07P8J83WR&location=4250))
    - Tags: [[pink]]

